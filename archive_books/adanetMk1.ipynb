{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import adanet\n",
    "from adanet.examples import simple_dnn\n",
    "\n",
    "# The random seed to use.\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "#tensorboard --logdir /media/eigenstir/1TBSecondary/tbgraphs --host localhost --port 7888 &\n",
    "\n",
    "LOG_DIR = '/media/eigenstir/1TBSecondary/tbgraphs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "Load and label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I mistakenly kept myself awake late last night...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Saw this movie recently and had higher hopes. ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a film that had a lot to live down to ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Rock 'n' Roll High School\" will probably have...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Even if you know absolutely nothing about Irel...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence sentiment  polarity\n",
       "0  I mistakenly kept myself awake late last night...         1         0\n",
       "1  Saw this movie recently and had higher hopes. ...         2         0\n",
       "2  This is a film that had a lot to live down to ...         3         0\n",
       "3  \"Rock 'n' Roll High School\" will probably have...        10         1\n",
       "4  Even if you know absolutely nothing about Irel...        10         1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_directory_data(directory):\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    for file_path in os.listdir(directory):\n",
    "        with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "def download_and_load_datasets(force_download=False):\n",
    "    dataset = tf.keras.utils.get_file(\n",
    "    fname=\"aclImdb.tar.gz\",\n",
    "    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
    "    extract=True\n",
    "    )\n",
    "    train_df = load_dataset(os.path.join(os.path.dirname(dataset),\n",
    "                                      \"aclImdb\", \"train\"))\n",
    "    test_df = load_dataset(os.path.join(os.path.dirname(dataset),\n",
    "                                      \"aclImdb\", \"test\"))\n",
    "    return train_df, test_df\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "train_df, test_df = download_and_load_datasets()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supply the data in TF \n",
    "Use input functions that wrap the DataFrame in input functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_KEY = \"sentence\"\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "  train_df, train_df[\"polarity\"], num_epochs=None, shuffle=True)\n",
    "\n",
    "predict_train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "  train_df, train_df[\"polarity\"], shuffle=False)\n",
    "\n",
    "predict_test_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "  test_df, test_df[\"polarity\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish baselines\n",
    "Test how a simple model performs on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "\n",
    "loss_reduction = tf.losses.Reduction.SUM_OVER_BATCH_SIZE\n",
    "\n",
    "head = tf.contrib.estimator.binary_classification_head(\n",
    "  loss_reduction=loss_reduction)\n",
    "\n",
    "hub_columns=hub.text_embedding_column(\n",
    "    key=FEATURES_KEY, \n",
    "    module_spec=\"https://tfhub.dev/google/nnlm-en-dim128/1\")\n",
    "\n",
    "def make_config(experiment_name):\n",
    "  # Estimator configuration.\n",
    "  return tf.estimator.RunConfig(\n",
    "    save_checkpoints_steps=1000,\n",
    "    save_summary_steps=1000,\n",
    "    tf_random_seed=RANDOM_SEED,\n",
    "    model_dir=os.path.join(LOG_DIR, experiment_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/media/eigenstir/1TBSecondary/tbgraphs/linear', '_tf_random_seed': 42, '_save_summary_steps': 1000, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3825974860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0408 21:46:28.434419 139880324773696 estimator.py:201] Using config: {'_model_dir': '/media/eigenstir/1TBSecondary/tbgraphs/linear', '_tf_random_seed': 42, '_save_summary_steps': 1000, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3825974860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0408 21:46:28.440585 139880324773696 estimator_training.py:185] Not using Distribute Coordinator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0408 21:46:28.444280 139880324773696 training.py:610] Running training and evaluation locally (non-distributed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1000 or save_checkpoints_secs None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0408 21:46:28.447603 139880324773696 training.py:698] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1000 or save_checkpoints_secs None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skipping training since max_steps has already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0408 21:46:28.470821 139880324773696 estimator.py:351] Skipping training since max_steps has already saved.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8e2320a79ce5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                                         steps=None))\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"average_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "#@title Parameters\n",
    "LEARNING_RATE = 0.001 #@param {type:\"number\"}\n",
    "TRAIN_STEPS = 5000 #@param {type:\"integer\"}\n",
    "\n",
    "estimator = tf.estimator.LinearClassifier(\n",
    "  feature_columns=[hub_columns],\n",
    "  n_classes=NUM_CLASSES,\n",
    "  optimizer=tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE),\n",
    "  loss_reduction=loss_reduction,\n",
    "  config=make_config(\"linear\"))\n",
    "\n",
    "results, _ = tf.estimator.train_and_evaluate(estimator, \n",
    "                                             train_spec=tf.estimator.TrainSpec(\n",
    "                                                        input_fn=train_input_fn,\n",
    "                                                        max_steps=TRAIN_STEPS),\n",
    "                                              eval_spec=tf.estimator.EvalSpec(\n",
    "                                                        input_fn=predict_test_input_fn,\n",
    "                                                        steps=None))\n",
    "\n",
    "print(\"Accuracy: \", results[\"accuracy\"])\n",
    "print(\"Loss: \", results[\"average_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple DNN AdaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.003\n",
    "TRAIN_STEPS = 5000\n",
    "ADANET_ITERATIONS = 2\n",
    "\n",
    "estimator = adanet.Estimator(\n",
    "    head=head, #using the linear classifier from earlier\n",
    "    \n",
    "    #define a generator which defines a space of subnetworks to train as candidates. \n",
    "    subnetwork_generator = simple_dnn.Generator(\n",
    "    feature_columns=[hub_columns],\n",
    "    optimizer=tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE),\n",
    "    seed=RANDOM_SEED),\n",
    "    \n",
    "    #number of train steps per iteration\n",
    "    max_iteration_steps=TRAIN_STEPS//ADANET_ITERATIONS,\n",
    "    \n",
    "    #evaluator to compute the overall AdaNet loss (train loss + complexity regularisation) to\n",
    "    #to select the best candidate for the final model\n",
    "    evaluator=adanet.Evaluator(\n",
    "    input_fn=predict_train_input_fn,\n",
    "    steps=1000),\n",
    "    \n",
    "    #config for estimators\n",
    "    config=make_config(\"simple_dnn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, _ = tf.estimator.train_and_evaluate(\n",
    "                estimator,\n",
    "                train_spec=tf.estimator.TrainSpec(\n",
    "                    input_fn=train_input_fn,\n",
    "                    max_steps=TRAIN_STEPS),\n",
    "                eval_spec=tf.estimator.EvalSpec(\n",
    "                    input_fn=predict_test_input_fn,\n",
    "                    steps=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", results[\"accuracy\"])\n",
    "print(\"Loss:\", results[\"average_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a slightly improvement - ~78% to 80%. This is because our simple_dnn.Generator will search over fully connected NNs that have more power than a simple linear model.\n",
    "\n",
    "The above only generates subnetworks that take embedding results from one module. We can add diversity to the search space by building subnetworks that take different embeddings which might improve performance. To do that, we need to define a custom:\n",
    "1. adanet.subnetwork.Build\n",
    "1. adanet.subnetwork.Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define an AdaNet model with TF Hub text embedding modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetworkBuilder(adanet.subnetwork.Builder):\n",
    "  \"\"\"Builds a simple subnetwork with text embedding module.\"\"\"\n",
    "\n",
    "  def __init__(self, learning_rate, max_iteration_steps, seed,\n",
    "               module_name, module):\n",
    "    \"\"\"Initializes a `SimpleNetworkBuilder`.\n",
    "\n",
    "    Args:\n",
    "      learning_rate: The float learning rate to use.\n",
    "      max_iteration_steps: The number of steps per iteration.\n",
    "      seed: The random seed.\n",
    "\n",
    "    Returns:\n",
    "      An instance of `SimpleNetworkBuilder`.\n",
    "    \"\"\"\n",
    "    self._learning_rate = learning_rate\n",
    "    self._max_iteration_steps = max_iteration_steps\n",
    "    self._seed = seed\n",
    "    self._module_name = module_name\n",
    "    self._module = module\n",
    "\n",
    "  def build_subnetwork(self,\n",
    "                       features,\n",
    "                       logits_dimension,\n",
    "                       training,\n",
    "                       iteration_step,\n",
    "                       summary,\n",
    "                       previous_ensemble=None):\n",
    "    \"\"\"See `adanet.subnetwork.Builder`.\"\"\"\n",
    "    sentence = features[\"sentence\"]\n",
    "    # Load module and apply text embedding, setting trainable=True.\n",
    "    m = hub.Module(self._module, trainable=True)\n",
    "    x = m(sentence)\n",
    "    kernel_initializer = tf.keras.initializers.he_normal(seed=self._seed)\n",
    "\n",
    "    # The `Head` passed to adanet.Estimator will apply the softmax activation.\n",
    "    logits = tf.layers.dense(\n",
    "        x, units=1, activation=None, kernel_initializer=kernel_initializer)\n",
    "\n",
    "    # Use a constant complexity measure, since all subnetworks have the same\n",
    "    # architecture and hyperparameters.\n",
    "    complexity = tf.constant(1)\n",
    "\n",
    "    return adanet.Subnetwork(\n",
    "        last_layer=x,\n",
    "        logits=logits,\n",
    "        complexity=complexity,\n",
    "        persisted_tensors={})\n",
    "\n",
    "  def build_subnetwork_train_op(self, \n",
    "                                subnetwork, \n",
    "                                loss, \n",
    "                                var_list, \n",
    "                                labels, \n",
    "                                iteration_step,\n",
    "                                summary, \n",
    "                                previous_ensemble=None):\n",
    "    \"\"\"See `adanet.subnetwork.Builder`.\"\"\"\n",
    "\n",
    "    learning_rate = tf.train.cosine_decay(\n",
    "        learning_rate=self._learning_rate,\n",
    "        global_step=iteration_step,\n",
    "        decay_steps=self._max_iteration_steps)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, .9)\n",
    "    # NOTE: The `adanet.Estimator` increments the global step.\n",
    "    return optimizer.minimize(loss=loss, var_list=var_list)\n",
    "\n",
    "  def build_mixture_weights_train_op(self, loss, var_list, logits, labels,\n",
    "                                     iteration_step, summary):\n",
    "    \"\"\"See `adanet.subnetwork.Builder`.\"\"\"\n",
    "    return tf.no_op(\"mixture_weights_train_op\")\n",
    "\n",
    "  @property\n",
    "  def name(self):\n",
    "    \"\"\"See `adanet.subnetwork.Builder`.\"\"\"\n",
    "    return self._module_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_subnetwork_train_op(self,\n",
    "                             subnetwork, \n",
    "                             loss,\n",
    "                             var_list,\n",
    "                             labels,\n",
    "                             iteration_step, \n",
    "                             summary,\n",
    "                             previous_ensemble=None):\n",
    "    learning_rate = tf.train.cosine_decay(\n",
    "        learning_rate=self._learning_rate,\n",
    "        global_step=iteration_step,\n",
    "        decay_steps=self._max_iteration_steps)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, .9)\n",
    "    return optimizer.minimize(loss=loss, var_list=var_list)\n",
    "\n",
    "def build_mixture_weights_train_op(self, loss, var_list, logits, labels,\n",
    "                                  iteration_step, summary):\n",
    "    return tf.no_op(\"mixture_weights_train_op\")\n",
    "\n",
    "@property\n",
    "def name(self):\n",
    "    return self._module_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adanet.subnetwork.Generator defines a search space of candidate SimpleNetworkBuilder to consider including the final network. It can craete one or more at each iteration with different parameters, and the AdaNet algorithm will select the candidate that best improves the overall networks adanet_loss on the training set. \n",
    "\n",
    "The below will loop through the text embedding modules listed in MODULES and give it a different random seed at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODULES = [\n",
    "    \"https://tfhub.dev/google/nnlm-en-dim50/1\",\n",
    "    \"https://tfhub.dev/google/nnlm-en-dim128/1\",\n",
    "    \"https://tfhub.dev/google/universal-sentence-encoder/1\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetworkGenerator(adanet.subnetwork.Generator):\n",
    "    def __init__(self, learning_rate, max_iteration_steps, seed=None):\n",
    "        #initializes a generator that builds Simple Network\n",
    "        self._seed = seed\n",
    "        self._dnn_builder_fn = functools.partial(\n",
    "            SimpleNetworkBuilder,\n",
    "            learning_rate=learning_rate,\n",
    "            max_iteration_steps=max_iteration_steps)\n",
    "    \n",
    "    def generate_candidates(self, previous_ensemble, \n",
    "                            iteration_number, previous_ensemble_reports, \n",
    "                            all_reports):\n",
    "        module_index = iteration_number % len(MODULES)\n",
    "        module_name = MODULES[module_index].split(\"/\")[-2]\n",
    "        \n",
    "        print(\"generating candidate: %s \" %module_name)\n",
    "        \n",
    "        seed = self._seed\n",
    "        #change the seed according to the iteration \n",
    "        if seed is not None:\n",
    "            seed += iteration_number\n",
    "            return [self._dnn_builder_fn(seed=seed,\n",
    "                                        module_name=module_name,\n",
    "                                        module=MODULES[module_index])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass these to the AdaNet estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.05\n",
    "TRAIN_STEPS = 7500\n",
    "ADANET_ITERATIONS = 3\n",
    "\n",
    "max_iteration_steps = TRAIN_STEPS // ADANET_ITERATIONS\n",
    "estimator = adanet.Estimator(head=head,\n",
    "                            subnetwork_generator=SimpleNetworkGenerator(\n",
    "                                learning_rate=LEARNING_RATE,\n",
    "                                max_iteration_steps=max_iteration_steps,\n",
    "                                seed=RANDOM_SEED),\n",
    "                            max_iteration_steps=max_iteration_steps,\n",
    "                            evaluator=adanet.Evaluator(input_fn=train_input_fn, \n",
    "                                                       steps=10),\n",
    "                            report_materializer=None,\n",
    "                            adanet_loss_decay=0.99,\n",
    "                            config=make_config(\"tfhub\"))\n",
    "\n",
    "results, _ = tf.estimator.train_and_evaluate(estimator,\n",
    "                        train_spec=tf.estimator.TrainSpec(input_fn=train_input_fn,\n",
    "                                                            max_steps=TRAIN_STEPS),\n",
    "                        eval_spec=tf.estimator.EvalSpec(input_fn=predict_test_input_fn,\n",
    "                                                       steps=None))\n",
    "\n",
    "print(\"Accuracy:\", results[\"accuracy\"])\n",
    "print(\"Loss:\", results[\"average_loss\"])\n",
    "\n",
    "def ensemble_architecture(result):\n",
    "    architecture=result[\"architecture/adanet/ensembles\"]\n",
    "    summary_proto = tf.summary.Summary.FromString(architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "  test_df.iloc[:10], test_df[\"polarity\"].iloc[:10], shuffle=False)\n",
    "\n",
    "predictions = estimator.predict(input_fn=predict_input_fn)\n",
    "\n",
    "for i, val in enumerate(predictions):\n",
    "    predicted_class = val['class_ids'][0]\n",
    "    prediction_confidence = val['probabilities'][predicted_class] * 100\n",
    "    \n",
    "    print('Actual text: ' + test_df[\"sentence\"][i])\n",
    "    print('Predicted class: %s, confidence: %s%%' \n",
    "          % (predicted_class, round(prediction_confidence, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    \"\"\"Serving input_fn that builds features from placeholders\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.estimator.export.ServingInputReceiver\n",
    "    \"\"\"\n",
    "    number = tf.placeholder(dtype=tf.float32, shape=[None, 1], name='number')\n",
    "    receiver_tensors = {'number': number}\n",
    "    features = tf.tile(number, multiples=[1, 2])\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n",
    "\n",
    "estimator.export_saved_model('saved_model', serving_input_receiver_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload & predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = 'saved_model'\n",
    "subdirs = [x for x in Path(export_dir).iterdir()\n",
    "           if x.is_dir() and 'temp' not in str(x)]\n",
    "latest = str(sorted(subdirs)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import predictor\n",
    "\n",
    "predict_fn = predictor.from_saved_model(latest)\n",
    "for nb in my_service():\n",
    "    pred = predict_fn({'number': [[nb]]})['output']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
