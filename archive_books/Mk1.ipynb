{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILES = [\n",
    "    'embeddings/crawl-300d-2M.vec',\n",
    "    'embeddings/glove.840B.300d.txt'\n",
    "]\n",
    "\n",
    "NUM_MODELS = 2\n",
    "BATCH_SIZE = 128\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functiosn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
    "\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embedding_matrix, model_name):\n",
    "    words = Input(shape=(max_length,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='tanh')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='sigmoid')(hidden)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=result)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    model.name = model_name\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_directory_data(directory):\n",
    "    data={}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    for file_path in os.listdir(directory):\n",
    "        with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(directory + \"/pos\")\n",
    "    neg_df = load_directory_data(directory + \"/neg\")\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_dataset(\"data/aclImdb/train/\")\n",
    "test_df = load_dataset(\"data/aclImdb/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df[\"sentence\"]\n",
    "y_train = train_df[\"polarity\"]\n",
    "\n",
    "x_test = test_df[\"sentence\"]\n",
    "y_test = test_df[\"polarity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import to_categorical\n",
    "\n",
    "# y_train = to_categorical(train_df[\"sentiment\"])\n",
    "# y_test = to_categorical(test_df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'1', '10', '2', '3', '4', '7', '8', '9'}, '1', 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_df['sentiment'].values), train_df[\"sentiment\"][1], y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I have never seen a comedy that was this much of a chore to sit thru...not one laugh in it. Ok, maybe one little chuckle for the Michael Clarke Duncan bit as the big, black, bald gay virgin. But the rest of it was shockingly un-funny. On top of being void of any laughs the \"skits\" go on forever! Steer clear of this one if you value your time and money. DREADFUL!!! The worst!!!',\n",
       " 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1], y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_obj = text.Tokenizer()\n",
    "total_reviews = x_train + x_test\n",
    "tokenizer_obj.fit_on_texts(total_reviews)\n",
    "\n",
    "max_length = max([len(s.split()) for s in total_reviews])\n",
    "vocab_size = len(tokenizer_obj.word_index) + 1\n",
    "\n",
    "X_train_tokens = tokenizer_obj.texts_to_sequences(x_train)\n",
    "X_test_tokens = tokenizer_obj.texts_to_sequences(x_test)\n",
    "\n",
    "X_train_pad = sequence.pad_sequences(X_train_tokens, maxlen=max_length, padding=\"post\")\n",
    "X_test_pad = sequence.pad_sequences(X_test_tokens, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix = np.concatenate(\n",
    "#     [build_matrix(tokenizer_obj.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n",
    "\n",
    "# np.savetxt('embedding_concat.txt', embedding_matrix , fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.loadtxt('embedding_concat.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.callbacks import Callback\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "class SGDRScheduler(Callback):\n",
    "    '''Cosine annealing learning rate scheduler with periodic restarts.\n",
    "    # Usage\n",
    "        ```python\n",
    "            schedule = SGDRScheduler(min_lr=1e-5,\n",
    "                                     max_lr=1e-2,\n",
    "                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
    "                                     lr_decay=0.9,\n",
    "                                     cycle_length=5,\n",
    "                                     mult_factor=1.5)\n",
    "            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n",
    "        ```\n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        lr_decay: Reduce the max_lr after the completion of each cycle.\n",
    "                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n",
    "        cycle_length: Initial number of epochs in a cycle.\n",
    "        mult_factor: Scale epochs_to_restart after each full cycle completion.\n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: http://arxiv.org/abs/1608.03983\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=10,\n",
    "                 mult_factor=2):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
    "LOG_DIR = '/media/eigenstir/1TBSecondary/tbgraphs'\n",
    "\n",
    "tbCallBack = TensorBoard(log_dir=LOG_DIR, histogram_freq=0, write_graph=True, write_images=True)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7, min_delta=0.0001)\n",
    "mc = ModelCheckpoint('best_model_cudnnlstm.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "ls_sched = SGDRScheduler(1e-2, 1e-1, np.ceil(25000/BATCH_SIZE))\n",
    "# ls_sched = LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3173)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 3173, 600)    75367800    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 3173, 600)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 3173, 256)    747520      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 3173, 256)    395264      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 256)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 512)          0           concatenate_1[0][0]              \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 512)          0           add_1[0][0]                      \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            513         add_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 77,036,409\n",
      "Trainable params: 1,668,609\n",
      "Non-trainable params: 75,367,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(embedding_matrix, model_name=\"cudnnlstm\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 234s 9ms/step - loss: 0.6827 - acc: 0.5540 - val_loss: 0.6527 - val_acc: 0.6247\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.62472, saving model to best_model_cudnnlstm.h5\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 244s 10ms/step - loss: 0.6298 - acc: 0.6430 - val_loss: 0.5850 - val_acc: 0.6926\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.62472 to 0.69256, saving model to best_model_cudnnlstm.h5\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 244s 10ms/step - loss: 0.5883 - acc: 0.6840 - val_loss: 0.5823 - val_acc: 0.6786\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.69256\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 246s 10ms/step - loss: 0.5577 - acc: 0.7094 - val_loss: 0.5420 - val_acc: 0.7268\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.69256 to 0.72680, saving model to best_model_cudnnlstm.h5\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 246s 10ms/step - loss: 0.5344 - acc: 0.7284 - val_loss: 0.5269 - val_acc: 0.7378\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.72680 to 0.73776, saving model to best_model_cudnnlstm.h5\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 248s 10ms/step - loss: 0.5062 - acc: 0.7487 - val_loss: 0.6475 - val_acc: 0.6374\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.73776\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 245s 10ms/step - loss: 0.4865 - acc: 0.7610 - val_loss: 0.5115 - val_acc: 0.7481\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.73776 to 0.74812, saving model to best_model_cudnnlstm.h5\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 246s 10ms/step - loss: 0.4625 - acc: 0.7775 - val_loss: 0.5124 - val_acc: 0.7516\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.74812 to 0.75156, saving model to best_model_cudnnlstm.h5\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 245s 10ms/step - loss: 0.4410 - acc: 0.7937 - val_loss: 0.5163 - val_acc: 0.7498\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.75156\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 249s 10ms/step - loss: 0.4137 - acc: 0.8076 - val_loss: 0.5273 - val_acc: 0.7444\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.75156\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 264s 11ms/step - loss: 0.3979 - acc: 0.8216 - val_loss: 0.5289 - val_acc: 0.7447\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.75156\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 261s 10ms/step - loss: 0.3769 - acc: 0.8269 - val_loss: 0.5471 - val_acc: 0.7504\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.75156\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 261s 10ms/step - loss: 0.3625 - acc: 0.8337 - val_loss: 0.5717 - val_acc: 0.7231\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.75156\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 262s 10ms/step - loss: 0.3561 - acc: 0.8406 - val_loss: 0.5609 - val_acc: 0.7442\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.75156\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f64a0458eb8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_pad,\n",
    "            y_train,\n",
    "            validation_data=(X_test_pad, y_test),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=20,\n",
    "            callbacks=[tbCallBack, es, mc]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(embedding_matrix, 'test')\n",
    "model.load_weights('best_model_cudnnlstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"When I saw this movie i expected it to be a cheesy American movie done on the cheap with appalling actors. I was really surprised to find that i was totally wrong. The movie centres around Bartely or B who has been rejected from all of his colleges- the actor who play B is very natural and makes his character seem real- and decides to create a pretend school so his parents stop harassing him. However loads of people see his fake website and join. Feeling their sorrows B can't turn them away much to the chagrin of his best mate. The college is the ideal place with you learning what you want or doing nothing. The school faces opposition from the proper college which ends up closing it down. The film ends on a high and i recommend you watching it. Its does have it flaws but it is a feel good cheerful film with a few unpredictable twists.\",\n",
       " 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 2\n",
    "test_text = test_df[\"sentence\"][idx]\n",
    "test_df[\"sentence\"][idx], test_df[\"polarity\"][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = tokenizer_obj.texts_to_sequences(test_text)\n",
    "test_text = sequence.pad_sequences(test_text, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62270343], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing texts: Uber\n",
    "\n",
    "Neg:\n",
    "\n",
    "1. #uber's customer service department is worst than comcast.\n",
    "1. #Uber ...get your act right! # frustrated#waiting for competition\n",
    "1. .@Uber charges more if they think you're willing to pay more. Uber thinks you're willing to pay more when your battery is about to die. And Uber knows when your battery is about to die.\n",
    "\n",
    "Pos:\n",
    "1. Ride-hailing industry expected to grow eightfold to $285 billion by 2030\n",
    "1. Uber is awesome!\n",
    "1. Best service ever with Uber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "imdb = tf.keras.datasets.imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Times Singapore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bs4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bac35a298376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bs4'"
     ]
    }
   ],
   "source": [
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "date_sentiments = {}\n",
    "\n",
    "for i in range(1,11):\n",
    "    page = urlopen('https://www.businesstimes.com.sg/search/facebook?page='+str(i)).read()\n",
    "    soup = BeautifulSoup(page, features=\"html.parser\")\n",
    "    posts = soup.findAll(\"div\", {\"class\": \"media-body\"})\n",
    "    for post in posts:\n",
    "        time.sleep(1)\n",
    "        url = post.a['href']\n",
    "        date = post.time.text\n",
    "        print(date, url)\n",
    "        try:\n",
    "            link_page = urlopen(url).read()\n",
    "        except:\n",
    "            url = url[:-2]\n",
    "            link_page = urlopen(url).read()\n",
    "        link_soup = BeautifulSoup(link_page)\n",
    "        sentences = link_soup.findAll(\"p\")\n",
    "        passage = \"\"\n",
    "        for sentence in sentences:\n",
    "            passage += sentence.text\n",
    "        sentiment = sia.polarity_scores(passage)['compound']\n",
    "        date_sentiments.setdefault(date, []).append(sentiment)\n",
    "\n",
    "date_sentiment = {}\n",
    "\n",
    "for k,v in date_sentiments.items():\n",
    "    date_sentiment[datetime.strptime(k, '%d %b %Y').date() + timedelta(days=1)] = round(sum(v)/float(len(v)),3)\n",
    "\n",
    "earliest_date = min(date_sentiment.keys())\n",
    "\n",
    "print(date_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtrader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backtrader as bt\n",
    "import backtrader.indicators as btind\n",
    "import datetime\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "class Sentiment(bt.Indicator):\n",
    "    lines = ('sentiment',)\n",
    "    plotinfo = dict(\n",
    "        plotymargin=0.15,\n",
    "        plothlines=[0],\n",
    "        plotyticks=[1.0, 0, -1.0])\n",
    "    \n",
    "    def next(self):\n",
    "        self.date = self.data.datetime\n",
    "        date = bt.num2date(self.date[0]).date()\n",
    "        prev_sentiment = self.sentiment\n",
    "        if date in date_sentiment:\n",
    "            self.sentiment = date_sentiment[date]\n",
    "        self.lines.sentiment[0] = self.sentiment\n",
    "\n",
    "\n",
    "class SentimentStrat(bt.Strategy):\n",
    "    params = (\n",
    "        ('period', 15),\n",
    "        ('printlog', True),\n",
    "    )\n",
    "\n",
    "    def log(self, txt, dt=None, doprint=False):\n",
    "        ''' Logging function for this strategy'''\n",
    "        if self.params.printlog or doprint:\n",
    "            dt = dt or self.datas[0].datetime.date(0)\n",
    "            print('%s, %s' % (dt.isoformat(), txt))\n",
    "\n",
    "    def __init__(self):\n",
    "        # Keep a reference to the \"close\" line in the data[0] dataseries\n",
    "        self.dataclose = self.datas[0].close\n",
    "        # Keep track of pending orders\n",
    "        self.order = None\n",
    "        self.buyprice = None\n",
    "        self.buycomm = None\n",
    "        self.sma = bt.indicators.SimpleMovingAverage(\n",
    "            self.datas[0], period=self.params.period)\n",
    "        self.date = self.data.datetime\n",
    "        self.sentiment = None\n",
    "        Sentiment(self.data)\n",
    "        \n",
    "    def notify_order(self, order):\n",
    "        if order.status in [order.Submitted, order.Accepted]:\n",
    "            # Buy/Sell order submitted/accepted to/by broker - Nothing to do\n",
    "            return\n",
    "        \n",
    "        # Check if an order has been completed\n",
    "        # Attention: broker could reject order if not enough cash\n",
    "        if order.status in [order.Completed]:\n",
    "            if order.isbuy():\n",
    "                self.log(\n",
    "                    'BUY EXECUTED, Price: %.2f, Cost: %.2f, Comm %.2f' %\n",
    "                    (order.executed.price,\n",
    "                     order.executed.value,\n",
    "                     order.executed.comm))\n",
    "                self.buyprice = order.executed.price\n",
    "                self.buycomm = order.executed.comm\n",
    "            else:  # Sell\n",
    "                self.log('SELL EXECUTED, Price: %.2f, Cost: %.2f, Comm %.2f' %\n",
    "                         (order.executed.price,\n",
    "                          order.executed.value,\n",
    "                          order.executed.comm))\n",
    "                \n",
    "            self.bar_executed = len(self)     \n",
    "            \n",
    "        elif order.status in [order.Canceled, order.Margin, order.Rejected]:\n",
    "            self.log('Order Canceled/Margin/Rejected')\n",
    "            \n",
    "        # Write down: no pending order\n",
    "        self.order = None\n",
    "        \n",
    "    def notify_trade(self, trade):\n",
    "        if not trade.isclosed:\n",
    "            return\n",
    "\n",
    "        self.log('OPERATION PROFIT, GROSS %.2f, NET %.2f' %\n",
    "                 (trade.pnl, trade.pnlcomm))\n",
    "    \n",
    "    ### Main Strat ###\n",
    "    def next(self):\n",
    "        # log closing price of the series from the reference\n",
    "        self.log('Close, %.2f' % self.dataclose[0])\n",
    "        \n",
    "        date = bt.num2date(self.date[0]).date()\n",
    "        prev_sentiment = self.sentiment\n",
    "        if date in date_sentiment:\n",
    "            self.sentiment = date_sentiment[date]\n",
    "        \n",
    "        # Check if an order is pending. if yes, we cannot send a 2nd one\n",
    "        if self.order:\n",
    "            return\n",
    "        print(self.sentiment)\n",
    "        # If not in the market and previous sentiment not none\n",
    "        if not self.position and prev_sentiment:\n",
    "            # buy if current close more than sma AND sentiment increased by >= 0.5\n",
    "            if self.dataclose[0] > self.sma[0] and self.sentiment - prev_sentiment >= 0.5:\n",
    "                self.log('BUY CREATE, %.2f' % self.dataclose[0])\n",
    "                self.order = self.buy()\n",
    "                \n",
    "        # Already in the market and previous sentiment not none\n",
    "        elif prev_sentiment:\n",
    "            # sell if current close less than sma AND sentiment decreased by >= 0.5\n",
    "            if self.dataclose[0] < self.sma[0] and self.sentiment - prev_sentiment <= -0.5:\n",
    "                self.log('SELL CREATE, %.2f' % self.dataclose[0])\n",
    "                self.order = self.sell()\n",
    "\n",
    "    def stop(self):\n",
    "        self.log('(MA Period %2d) Ending Value %.2f' %\n",
    "                 (self.params.period, self.broker.getvalue()), doprint=True)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cerebro = bt.Cerebro()\n",
    "    \n",
    "    # Strategy\n",
    "    cerebro.addstrategy(SentimentStrat)\n",
    "\n",
    "    # Data Feed\n",
    "    data = bt.feeds.YahooFinanceData(\n",
    "        dataname = 'FB',\n",
    "        fromdate = earliest_date,\n",
    "        todate = datetime.datetime(2018,11,25),\n",
    "        reverse = False\n",
    "    )\n",
    "    \n",
    "    cerebro.adddata(data)\n",
    "\n",
    "    cerebro.broker.setcash(100000.0)\n",
    "    cerebro.addsizer(bt.sizers.FixedSize, stake=10)\n",
    "    cerebro.broker.setcommission(commission=0.001)\n",
    "    print('Starting Portfolio Value: %.2f' % cerebro.broker.getvalue())\n",
    "    cerebro.run()\n",
    "    print('Final Portfolio Value: %.2f' % cerebro.broker.getvalue())\n",
    "    \n",
    "    cerebro.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "# from keras import backend as K\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import graph_io\n",
    "from tensorflow.python.saved_model import signature_constants\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.set_learning_phase(0)\n",
    "\n",
    "model = build_model(embedding_matrix, model_name = str(model_idx))\n",
    "config = model.get_config()\n",
    "weights = model.get_weights()\n",
    "new_model = Model.from_config(config)\n",
    "new_model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model import utils\n",
    "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
    "from tensorflow.python.saved_model.signature_def_utils_impl import build_signature_def, predict_signature_def\n",
    "from tensorflow.contrib.session_bundle import exporter\n",
    "\n",
    "export_path = 'exported_model'\n",
    "builder = saved_model_builder.SavedModelBuilder(export_path)\n",
    "\n",
    "signature = predict_signature_def(inputs={'text': new_model.input},\n",
    "                                  outputs={'sentiment': new_model.output})\n",
    "\n",
    "with keras.get_session() as sess:\n",
    "    builder.add_meta_graph_and_variables(sess=sess,\n",
    "                                         tags=[tag_constants.SERVING],\n",
    "                                         signature_def_map={'predict': signature})\n",
    "    builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another type of loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-15-c5479d416f9e>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-c5479d416f9e>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.loadtxt('embedding_concat.txt', dtype=int)\n",
    "words = Input(shape=(max_length,))\n",
    "x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "x = SpatialDropout1D(0.3)(x)\n",
    "x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "result = Dense(11, activation='sigmoid')(hidden)\n",
    "    \n",
    "model = Model(inputs=words, outputs=result)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=0, write_graph=True, write_images=True)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "ls_sched = LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_pad, y_train, batch_size=64, epochs=10, \n",
    "          validation_data=(X_test_pad, y_test), callbacks=[tbCallBack, es, mc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
