{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 500\n",
    "LSTM_UNITS = 256\n",
    "DENSE_UNITS = LSTM_UNITS * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdbEr.txt', 'train', 'imdb.vocab', 'test', 'README']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data/aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_directory_data(directory):\n",
    "    data={}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    for file_path in os.listdir(directory):\n",
    "        with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(directory + \"/pos\")\n",
    "    neg_df = load_directory_data(directory + \"/neg\")\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_dataset(\"data/aclImdb/train/\")\n",
    "test_df = load_dataset(\"data/aclImdb/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STAR RATING: ***** Saturday Night **** Friday ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Original Claymation Rudolph: Pretty good. Orig...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I know that in this episode there's other stuf...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Warner Bros effort starring Errol Fl...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oliver Hardy awakens with a hangover and soon ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence sentiment  polarity\n",
       "0  STAR RATING: ***** Saturday Night **** Friday ...         4         0\n",
       "1  Original Claymation Rudolph: Pretty good. Orig...         2         0\n",
       "2  I know that in this episode there's other stuf...         4         0\n",
       "3  Excellent Warner Bros effort starring Errol Fl...         9         1\n",
       "4  Oliver Hardy awakens with a hangover and soon ...         4         0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df['sentence']\n",
    "y_train = train_df['polarity']\n",
    "x_test = test_df['sentence']\n",
    "y_test = test_df['polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    STAR RATING: ***** Saturday Night **** Friday ...\n",
       " 1    Original Claymation Rudolph: Pretty good. Orig...\n",
       " 2    I know that in this episode there's other stuf...\n",
       " Name: sentence, dtype: object, 0    0\n",
       " 1    0\n",
       " 2    0\n",
       " Name: polarity, dtype: int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:3], y_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize & Pad Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "import pickle\n",
    "\n",
    "tokenizer = text.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.fit_on_texts(x_train + x_test)\n",
    "\n",
    "# # saving\n",
    "# with open('tokenizer_imdb.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading\n",
    "with open('tokenizer_imdb.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(s.split()) for s in (x_train + x_test)])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=max_length, padding=\"post\")\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset shapes: ((64, 2722), (64,)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shuffle(BATCH_SIZE)\n",
    "test_dataset.shuffle(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 2722]), TensorShape([64]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_inp_batch, example_targ_batch = next(iter(train_dataset))\n",
    "example_inp_batch.shape, example_targ_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate, Flatten, Embedding\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, Input, LSTM\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM, CuDNNGRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0413 13:39:43.010889 139752145893184 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f19081a6470>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
      "W0413 13:39:43.012671 139752145893184 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f19686c0d68>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    }
   ],
   "source": [
    "class MyLSTM(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length)\n",
    "        self.dropout1 = SpatialDropout1D(0.3)\n",
    "        self.lstm1 = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True, dropout=0.2))\n",
    "        self.gmp = GlobalMaxPooling1D()\n",
    "        self.dense1 = Dense(100, activation='relu')\n",
    "        self.dropout2 = Dropout(0.2)\n",
    "        self.denseOut = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.gmp(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.denseOut(x)\n",
    "        return x\n",
    "    \n",
    "model = MyLSTM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    return tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.expand_dims(real, axis=1), \n",
    "                                                              pred, from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(inp)\n",
    "        loss += loss_function(targ, preds)\n",
    "    \n",
    "    trn_acc_metric.update_state(targ, preds)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def val_step(inp, targ):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(inp)\n",
    "        loss += loss_function(targ, preds)\n",
    "    \n",
    "    val_acc_metric.update_state(targ, preds)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Trn Loss 0.7792 Val Loss 0.7260 Trn_Acc 0.34 Val_Acc 0.52\n",
      "Epoch 1 Batch 100 Trn Loss 0.6929 Val Loss 0.6929 Trn_Acc 0.50 Val_Acc 0.50\n",
      "Epoch 1 Batch 200 Trn Loss 0.6903 Val Loss 0.6848 Trn_Acc 0.51 Val_Acc 0.51\n",
      "Epoch 1 Batch 300 Trn Loss 0.6931 Val Loss 0.6850 Trn_Acc 0.52 Val_Acc 0.52\n",
      "Epoch 1 Trn Loss 0.6853\n",
      "Epoch 1 Val Loss 0.6862\n",
      "Time taken for 1 epoch 280.2880861759186 sec\n",
      "\n",
      "Epoch 2 Batch 0 Trn Loss 0.6560 Val Loss 0.6218 Trn_Acc 0.53 Val_Acc 0.53\n",
      "Epoch 2 Batch 100 Trn Loss 0.5418 Val Loss 0.5735 Trn_Acc 0.58 Val_Acc 0.58\n",
      "Epoch 2 Batch 200 Trn Loss 0.6008 Val Loss 0.5454 Trn_Acc 0.62 Val_Acc 0.62\n",
      "Epoch 2 Batch 300 Trn Loss 0.5873 Val Loss 0.5588 Trn_Acc 0.65 Val_Acc 0.65\n",
      "Epoch 2 Trn Loss 0.5831\n",
      "Epoch 2 Val Loss 0.5860\n",
      "Time taken for 1 epoch 266.32435941696167 sec\n",
      "\n",
      "Epoch 3 Batch 0 Trn Loss 0.6039 Val Loss 0.5560 Trn_Acc 0.68 Val_Acc 0.67\n",
      "Epoch 3 Batch 100 Trn Loss 0.4971 Val Loss 0.5517 Trn_Acc 0.70 Val_Acc 0.69\n",
      "Epoch 3 Batch 200 Trn Loss 0.5562 Val Loss 0.5349 Trn_Acc 0.72 Val_Acc 0.71\n",
      "Epoch 3 Batch 300 Trn Loss 0.5532 Val Loss 0.5342 Trn_Acc 0.74 Val_Acc 0.73\n",
      "Epoch 3 Trn Loss 0.5443\n",
      "Epoch 3 Val Loss 0.5666\n",
      "Time taken for 1 epoch 275.8470792770386 sec\n",
      "\n",
      "Epoch 4 Batch 0 Trn Loss 0.5572 Val Loss 0.5738 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 100 Trn Loss 0.5044 Val Loss 0.5520 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 4 Batch 200 Trn Loss 0.5420 Val Loss 0.5422 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 4 Batch 300 Trn Loss 0.5380 Val Loss 0.5423 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 4 Trn Loss 0.5304\n",
      "Epoch 4 Val Loss 0.5654\n",
      "Time taken for 1 epoch 277.65251564979553 sec\n",
      "\n",
      "Epoch 5 Batch 0 Trn Loss 0.5704 Val Loss 0.5704 Trn_Acc 0.80 Val_Acc 0.77\n",
      "Epoch 5 Batch 100 Trn Loss 0.5148 Val Loss 0.5541 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 5 Batch 200 Trn Loss 0.5306 Val Loss 0.5214 Trn_Acc 0.82 Val_Acc 0.78\n",
      "Epoch 5 Batch 300 Trn Loss 0.5496 Val Loss 0.5666 Trn_Acc 0.83 Val_Acc 0.78\n",
      "Epoch 5 Trn Loss 0.5240\n",
      "Epoch 5 Val Loss 0.5660\n",
      "Time taken for 1 epoch 276.74147725105286 sec\n",
      "\n",
      "Epoch 6 Batch 0 Trn Loss 0.5507 Val Loss 0.5322 Trn_Acc 0.83 Val_Acc 0.79\n",
      "Epoch 6 Batch 100 Trn Loss 0.4978 Val Loss 0.5521 Trn_Acc 0.84 Val_Acc 0.79\n",
      "Epoch 6 Batch 200 Trn Loss 0.5248 Val Loss 0.5388 Trn_Acc 0.84 Val_Acc 0.80\n",
      "Epoch 6 Batch 300 Trn Loss 0.5268 Val Loss 0.5660 Trn_Acc 0.85 Val_Acc 0.80\n",
      "Epoch 6 Trn Loss 0.5221\n",
      "Epoch 6 Val Loss 0.5674\n",
      "Time taken for 1 epoch 275.83209204673767 sec\n",
      "\n",
      "Epoch 7 Batch 0 Trn Loss 0.5506 Val Loss 0.5421 Trn_Acc 0.85 Val_Acc 0.80\n",
      "Epoch 7 Batch 100 Trn Loss 0.5015 Val Loss 0.5583 Trn_Acc 0.86 Val_Acc 0.80\n",
      "Epoch 7 Batch 200 Trn Loss 0.5311 Val Loss 0.5179 Trn_Acc 0.86 Val_Acc 0.81\n",
      "Epoch 7 Batch 300 Trn Loss 0.5200 Val Loss 0.5572 Trn_Acc 0.87 Val_Acc 0.81\n",
      "Epoch 7 Trn Loss 0.5195\n",
      "Epoch 7 Val Loss 0.5662\n",
      "Time taken for 1 epoch 274.03437995910645 sec\n",
      "\n",
      "Epoch 8 Batch 0 Trn Loss 0.5488 Val Loss 0.5426 Trn_Acc 0.87 Val_Acc 0.81\n",
      "Epoch 8 Batch 100 Trn Loss 0.4915 Val Loss 0.5502 Trn_Acc 0.87 Val_Acc 0.81\n",
      "Epoch 8 Batch 200 Trn Loss 0.5257 Val Loss 0.5244 Trn_Acc 0.88 Val_Acc 0.82\n",
      "Epoch 8 Batch 300 Trn Loss 0.5091 Val Loss 0.5435 Trn_Acc 0.88 Val_Acc 0.82\n",
      "Epoch 8 Trn Loss 0.5156\n",
      "Epoch 8 Val Loss 0.5624\n",
      "Time taken for 1 epoch 274.26110005378723 sec\n",
      "\n",
      "Epoch 9 Batch 0 Trn Loss 0.5448 Val Loss 0.5534 Trn_Acc 0.88 Val_Acc 0.82\n",
      "Epoch 9 Batch 100 Trn Loss 0.4973 Val Loss 0.5573 Trn_Acc 0.89 Val_Acc 0.82\n",
      "Epoch 9 Batch 200 Trn Loss 0.5334 Val Loss 0.5196 Trn_Acc 0.89 Val_Acc 0.82\n",
      "Epoch 9 Batch 300 Trn Loss 0.5092 Val Loss 0.5520 Trn_Acc 0.89 Val_Acc 0.82\n",
      "Epoch 9 Trn Loss 0.5147\n",
      "Epoch 9 Val Loss 0.5637\n",
      "Time taken for 1 epoch 274.28718733787537 sec\n",
      "\n",
      "Epoch 10 Batch 0 Trn Loss 0.5448 Val Loss 0.5610 Trn_Acc 0.89 Val_Acc 0.82\n",
      "Epoch 10 Batch 100 Trn Loss 0.4963 Val Loss 0.5626 Trn_Acc 0.90 Val_Acc 0.83\n",
      "Epoch 10 Batch 200 Trn Loss 0.5290 Val Loss 0.5183 Trn_Acc 0.90 Val_Acc 0.83\n",
      "Epoch 10 Batch 300 Trn Loss 0.5207 Val Loss 0.5418 Trn_Acc 0.90 Val_Acc 0.83\n",
      "Epoch 10 Trn Loss 0.5142\n",
      "Epoch 10 Val Loss 0.5641\n",
      "Time taken for 1 epoch 274.01141238212585 sec\n",
      "\n",
      "Epoch 11 Batch 0 Trn Loss 0.5565 Val Loss 0.5707 Trn_Acc 0.90 Val_Acc 0.83\n",
      "Epoch 11 Batch 100 Trn Loss 0.4913 Val Loss 0.5599 Trn_Acc 0.90 Val_Acc 0.83\n",
      "Epoch 11 Batch 200 Trn Loss 0.5248 Val Loss 0.5154 Trn_Acc 0.91 Val_Acc 0.83\n",
      "Epoch 11 Batch 300 Trn Loss 0.5095 Val Loss 0.5375 Trn_Acc 0.91 Val_Acc 0.83\n",
      "Epoch 11 Trn Loss 0.5140\n",
      "Epoch 11 Val Loss 0.5652\n",
      "Time taken for 1 epoch 271.0165786743164 sec\n",
      "\n",
      "Epoch 12 Batch 0 Trn Loss 0.5515 Val Loss 0.5789 Trn_Acc 0.91 Val_Acc 0.83\n",
      "Epoch 12 Batch 100 Trn Loss 0.4913 Val Loss 0.5823 Trn_Acc 0.91 Val_Acc 0.83\n",
      "Epoch 12 Batch 200 Trn Loss 0.5248 Val Loss 0.5099 Trn_Acc 0.91 Val_Acc 0.83\n",
      "Epoch 12 Batch 300 Trn Loss 0.5091 Val Loss 0.5302 Trn_Acc 0.91 Val_Acc 0.84\n",
      "Epoch 12 Trn Loss 0.5120\n",
      "Epoch 12 Val Loss 0.5636\n",
      "Time taken for 1 epoch 271.7169997692108 sec\n",
      "\n",
      "Epoch 13 Batch 0 Trn Loss 0.5448 Val Loss 0.5738 Trn_Acc 0.91 Val_Acc 0.84\n",
      "Epoch 13 Batch 100 Trn Loss 0.4913 Val Loss 0.5816 Trn_Acc 0.92 Val_Acc 0.84\n",
      "Epoch 13 Batch 200 Trn Loss 0.5248 Val Loss 0.5255 Trn_Acc 0.92 Val_Acc 0.84\n",
      "Epoch 13 Batch 300 Trn Loss 0.5091 Val Loss 0.5434 Trn_Acc 0.92 Val_Acc 0.84\n",
      "Epoch 13 Trn Loss 0.5121\n",
      "Epoch 13 Val Loss 0.5661\n",
      "Time taken for 1 epoch 269.559855222702 sec\n",
      "\n",
      "Epoch 14 Batch 0 Trn Loss 0.5448 Val Loss 0.5839 Trn_Acc 0.92 Val_Acc 0.84\n",
      "Epoch 14 Batch 100 Trn Loss 0.4913 Val Loss 0.5477 Trn_Acc 0.92 Val_Acc 0.84\n",
      "Epoch 14 Batch 200 Trn Loss 0.5248 Val Loss 0.5243 Trn_Acc 0.92 Val_Acc 0.84\n",
      "Epoch 14 Batch 300 Trn Loss 0.5091 Val Loss 0.5278 Trn_Acc 0.92 Val_Acc 0.84\n",
      "Epoch 14 Trn Loss 0.5120\n",
      "Epoch 14 Val Loss 0.5652\n",
      "Time taken for 1 epoch 272.5597982406616 sec\n",
      "\n",
      "Epoch 15 Batch 0 Trn Loss 0.5449 Val Loss 0.5423 Trn_Acc 0.92 Val_Acc 0.84\n",
      "Epoch 15 Batch 100 Trn Loss 0.4917 Val Loss 0.5801 Trn_Acc 0.92 Val_Acc 0.84\n",
      "Epoch 15 Batch 200 Trn Loss 0.5248 Val Loss 0.5289 Trn_Acc 0.93 Val_Acc 0.84\n",
      "Epoch 15 Batch 300 Trn Loss 0.5091 Val Loss 0.5338 Trn_Acc 0.93 Val_Acc 0.84\n",
      "Epoch 15 Trn Loss 0.5113\n",
      "Epoch 15 Val Loss 0.5658\n",
      "Time taken for 1 epoch 276.5142593383789 sec\n",
      "\n",
      "Epoch 16 Batch 0 Trn Loss 0.5448 Val Loss 0.5918 Trn_Acc 0.93 Val_Acc 0.84\n",
      "Epoch 16 Batch 100 Trn Loss 0.4913 Val Loss 0.5730 Trn_Acc 0.93 Val_Acc 0.84\n",
      "Epoch 16 Batch 200 Trn Loss 0.5248 Val Loss 0.5416 Trn_Acc 0.93 Val_Acc 0.84\n",
      "Epoch 16 Batch 300 Trn Loss 0.5091 Val Loss 0.5326 Trn_Acc 0.93 Val_Acc 0.84\n",
      "Epoch 16 Trn Loss 0.5105\n",
      "Epoch 16 Val Loss 0.5645\n",
      "Time taken for 1 epoch 278.9483766555786 sec\n",
      "\n",
      "Epoch 17 Batch 0 Trn Loss 0.5459 Val Loss 0.5835 Trn_Acc 0.93 Val_Acc 0.84\n",
      "Epoch 17 Batch 100 Trn Loss 0.4913 Val Loss 0.5807 Trn_Acc 0.93 Val_Acc 0.84\n",
      "Epoch 17 Batch 200 Trn Loss 0.5250 Val Loss 0.5318 Trn_Acc 0.93 Val_Acc 0.85\n",
      "Epoch 17 Batch 300 Trn Loss 0.5091 Val Loss 0.5251 Trn_Acc 0.93 Val_Acc 0.85\n",
      "Epoch 17 Trn Loss 0.5113\n",
      "Epoch 17 Val Loss 0.5665\n",
      "Time taken for 1 epoch 275.5495011806488 sec\n",
      "\n",
      "Epoch 18 Batch 0 Trn Loss 0.5448 Val Loss 0.5909 Trn_Acc 0.93 Val_Acc 0.85\n",
      "Epoch 18 Batch 100 Trn Loss 0.4919 Val Loss 0.5851 Trn_Acc 0.94 Val_Acc 0.85\n",
      "Epoch 18 Batch 200 Trn Loss 0.5309 Val Loss 0.5310 Trn_Acc 0.94 Val_Acc 0.85\n",
      "Epoch 18 Batch 300 Trn Loss 0.5091 Val Loss 0.5188 Trn_Acc 0.94 Val_Acc 0.85\n",
      "Epoch 18 Trn Loss 0.5112\n",
      "Epoch 18 Val Loss 0.5674\n",
      "Time taken for 1 epoch 276.8686499595642 sec\n",
      "\n",
      "Epoch 19 Batch 0 Trn Loss 0.5448 Val Loss 0.5684 Trn_Acc 0.94 Val_Acc 0.85\n",
      "Epoch 19 Batch 100 Trn Loss 0.4913 Val Loss 0.5877 Trn_Acc 0.94 Val_Acc 0.85\n",
      "Epoch 19 Batch 200 Trn Loss 0.5248 Val Loss 0.5382 Trn_Acc 0.94 Val_Acc 0.85\n",
      "Epoch 19 Batch 300 Trn Loss 0.5091 Val Loss 0.5290 Trn_Acc 0.94 Val_Acc 0.85\n",
      "Epoch 19 Trn Loss 0.5107\n",
      "Epoch 19 Val Loss 0.5649\n",
      "Time taken for 1 epoch 276.3130886554718 sec\n",
      "\n",
      "Epoch 20 Batch 0 Trn Loss 0.5448 Val Loss 0.5758 Trn_Acc 0.94 Val_Acc 0.85\n",
      "Epoch 20 Batch 100 Trn Loss 0.4913 Val Loss 0.5732 Trn_Acc 0.94 Val_Acc 0.85\n",
      "Epoch 20 Batch 200 Trn Loss 0.5248 Val Loss 0.5193 Trn_Acc 0.94 Val_Acc 0.85\n",
      "Epoch 20 Batch 300 Trn Loss 0.5091 Val Loss 0.5334 Trn_Acc 0.94 Val_Acc 0.85\n",
      "Epoch 20 Trn Loss 0.5101\n",
      "Epoch 20 Val Loss 0.5646\n",
      "Time taken for 1 epoch 277.2159357070923 sec\n",
      "\n",
      "Epoch 21 Batch 0 Trn Loss 0.5448 Val Loss 0.5945 Trn_Acc 0.94 Val_Acc 0.85\n",
      "Epoch 21 Batch 100 Trn Loss 0.4913 Val Loss 0.5954 Trn_Acc 0.94 Val_Acc 0.85\n",
      "Epoch 21 Batch 200 Trn Loss 0.5273 Val Loss 0.5305 Trn_Acc 0.94 Val_Acc 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Batch 300 Trn Loss 0.5091 Val Loss 0.5344 Trn_Acc 0.94 Val_Acc 0.85\n",
      "Epoch 21 Trn Loss 0.5098\n",
      "Epoch 21 Val Loss 0.5656\n",
      "Time taken for 1 epoch 276.44204783439636 sec\n",
      "\n",
      "Epoch 22 Batch 0 Trn Loss 0.5448 Val Loss 0.5820 Trn_Acc 0.94 Val_Acc 0.85\n",
      "Epoch 22 Batch 100 Trn Loss 0.4854 Val Loss 0.5989 Trn_Acc 0.94 Val_Acc 0.85\n",
      "Epoch 22 Batch 200 Trn Loss 0.5525 Val Loss 0.5417 Trn_Acc 0.94 Val_Acc 0.85\n",
      "Epoch 22 Batch 300 Trn Loss 0.5091 Val Loss 0.5317 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 22 Trn Loss 0.5102\n",
      "Epoch 22 Val Loss 0.5658\n",
      "Time taken for 1 epoch 276.44273495674133 sec\n",
      "\n",
      "Epoch 23 Batch 0 Trn Loss 0.5448 Val Loss 0.5682 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 23 Batch 100 Trn Loss 0.4854 Val Loss 0.5766 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 23 Batch 200 Trn Loss 0.5249 Val Loss 0.5258 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 23 Batch 300 Trn Loss 0.5089 Val Loss 0.5279 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 23 Trn Loss 0.5099\n",
      "Epoch 23 Val Loss 0.5639\n",
      "Time taken for 1 epoch 276.7256543636322 sec\n",
      "\n",
      "Epoch 24 Batch 0 Trn Loss 0.5448 Val Loss 0.5743 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 24 Batch 100 Trn Loss 0.4854 Val Loss 0.5846 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 24 Batch 200 Trn Loss 0.5248 Val Loss 0.5395 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 24 Batch 300 Trn Loss 0.5032 Val Loss 0.5304 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 24 Trn Loss 0.5094\n",
      "Epoch 24 Val Loss 0.5642\n",
      "Time taken for 1 epoch 278.31481075286865 sec\n",
      "\n",
      "Epoch 25 Batch 0 Trn Loss 0.5448 Val Loss 0.5709 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 25 Batch 100 Trn Loss 0.4854 Val Loss 0.5880 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 25 Batch 200 Trn Loss 0.5340 Val Loss 0.5356 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 25 Batch 300 Trn Loss 0.5032 Val Loss 0.5307 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 25 Trn Loss 0.5095\n",
      "Epoch 25 Val Loss 0.5648\n",
      "Time taken for 1 epoch 276.2140326499939 sec\n",
      "\n",
      "Epoch 26 Batch 0 Trn Loss 0.5448 Val Loss 0.5644 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 26 Batch 100 Trn Loss 0.4857 Val Loss 0.5703 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 26 Batch 200 Trn Loss 0.5248 Val Loss 0.5281 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 26 Batch 300 Trn Loss 0.5032 Val Loss 0.5298 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 26 Trn Loss 0.5091\n",
      "Epoch 26 Val Loss 0.5639\n",
      "Time taken for 1 epoch 277.35066866874695 sec\n",
      "\n",
      "Epoch 27 Batch 0 Trn Loss 0.5448 Val Loss 0.5620 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 27 Batch 100 Trn Loss 0.4856 Val Loss 0.5822 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 27 Batch 200 Trn Loss 0.5248 Val Loss 0.5314 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 27 Batch 300 Trn Loss 0.5091 Val Loss 0.5286 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 27 Trn Loss 0.5096\n",
      "Epoch 27 Val Loss 0.5653\n",
      "Time taken for 1 epoch 276.2260811328888 sec\n",
      "\n",
      "Epoch 28 Batch 0 Trn Loss 0.5448 Val Loss 0.5820 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 28 Batch 100 Trn Loss 0.4854 Val Loss 0.5713 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 28 Batch 200 Trn Loss 0.5249 Val Loss 0.5423 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 28 Batch 300 Trn Loss 0.5032 Val Loss 0.5255 Trn_Acc 0.95 Val_Acc 0.85\n",
      "Epoch 28 Trn Loss 0.5090\n",
      "Epoch 28 Val Loss 0.5627\n",
      "Time taken for 1 epoch 277.06017661094666 sec\n",
      "\n",
      "Epoch 29 Batch 0 Trn Loss 0.5448 Val Loss 0.5852 Trn_Acc 0.95 Val_Acc 0.86\n",
      "Epoch 29 Batch 100 Trn Loss 0.4854 Val Loss 0.5708 Trn_Acc 0.95 Val_Acc 0.86\n",
      "Epoch 29 Batch 200 Trn Loss 0.5248 Val Loss 0.5173 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 29 Batch 300 Trn Loss 0.5032 Val Loss 0.5188 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 29 Trn Loss 0.5088\n",
      "Epoch 29 Val Loss 0.5635\n",
      "Time taken for 1 epoch 276.73002767562866 sec\n",
      "\n",
      "Epoch 30 Batch 0 Trn Loss 0.5448 Val Loss 0.5688 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 30 Batch 100 Trn Loss 0.4854 Val Loss 0.5729 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 30 Batch 200 Trn Loss 0.5422 Val Loss 0.5710 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 30 Batch 300 Trn Loss 0.5032 Val Loss 0.5220 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 30 Trn Loss 0.5094\n",
      "Epoch 30 Val Loss 0.5644\n",
      "Time taken for 1 epoch 277.3689937591553 sec\n",
      "\n",
      "Epoch 31 Batch 0 Trn Loss 0.5448 Val Loss 0.5764 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 31 Batch 100 Trn Loss 0.4854 Val Loss 0.5778 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 31 Batch 200 Trn Loss 0.5248 Val Loss 0.5292 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 31 Batch 300 Trn Loss 0.5032 Val Loss 0.5386 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 31 Trn Loss 0.5091\n",
      "Epoch 31 Val Loss 0.5661\n",
      "Time taken for 1 epoch 276.72232460975647 sec\n",
      "\n",
      "Epoch 32 Batch 0 Trn Loss 0.5448 Val Loss 0.5832 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 32 Batch 100 Trn Loss 0.4854 Val Loss 0.5737 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 32 Batch 200 Trn Loss 0.5248 Val Loss 0.5234 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 32 Batch 300 Trn Loss 0.4973 Val Loss 0.5352 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 32 Trn Loss 0.5090\n",
      "Epoch 32 Val Loss 0.5649\n",
      "Time taken for 1 epoch 277.4986050128937 sec\n",
      "\n",
      "Epoch 33 Batch 0 Trn Loss 0.5448 Val Loss 0.5765 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 33 Batch 100 Trn Loss 0.4913 Val Loss 0.5691 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 33 Batch 200 Trn Loss 0.5248 Val Loss 0.5275 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 33 Batch 300 Trn Loss 0.4973 Val Loss 0.5195 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 33 Trn Loss 0.5091\n",
      "Epoch 33 Val Loss 0.5646\n",
      "Time taken for 1 epoch 259.63753485679626 sec\n",
      "\n",
      "Epoch 34 Batch 0 Trn Loss 0.5448 Val Loss 0.5810 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 34 Batch 100 Trn Loss 0.4854 Val Loss 0.5781 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 34 Batch 200 Trn Loss 0.5248 Val Loss 0.5188 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 34 Batch 300 Trn Loss 0.4973 Val Loss 0.5255 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 34 Trn Loss 0.5094\n",
      "Epoch 34 Val Loss 0.5668\n",
      "Time taken for 1 epoch 277.7797677516937 sec\n",
      "\n",
      "Epoch 35 Batch 0 Trn Loss 0.5448 Val Loss 0.5727 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 35 Batch 100 Trn Loss 0.4854 Val Loss 0.5736 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 35 Batch 200 Trn Loss 0.5299 Val Loss 0.5181 Trn_Acc 0.96 Val_Acc 0.86\n",
      "Epoch 35 Batch 300 Trn Loss 0.4973 Val Loss 0.5332 Trn_Acc 0.96 Val_Acc 0.86\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-bc62cada5e7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_targ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_targ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtrn_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2alpha/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    412\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2alpha/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2alpha/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \"\"\"\n\u001b[1;32m    573\u001b[0m     return self._call_flat(\n\u001b[0;32m--> 574\u001b[0;31m         (t for t in nest.flatten((args, kwargs))\n\u001b[0m\u001b[1;32m    575\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m    576\u001b[0m                            resource_variable_ops.ResourceVariable))))\n",
      "\u001b[0;32m~/anaconda3/envs/tf2alpha/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2alpha/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    413\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    414\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 415\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    416\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2alpha/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     59\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "trn_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "steps_per_epoch = len(x_train)//BATCH_SIZE\n",
    "\n",
    "EPOCHS = 30\n",
    "trn_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    val_total_loss = 0\n",
    "    \n",
    "    hidden = model.reset_states()\n",
    "    for batch, ((trn_inp, trn_targ), (val_inp, val_targ)) in enumerate(zip(train_dataset.take(steps_per_epoch), test_dataset.take(steps_per_epoch))):\n",
    "        batch_loss = train_step(trn_inp, trn_targ)\n",
    "        total_loss += batch_loss\n",
    "        trn_loss_list.append(batch_loss)\n",
    "\n",
    "        val_loss = val_step(val_inp, val_targ)\n",
    "        val_total_loss += val_loss\n",
    "        val_loss_list.append(val_loss)\n",
    "\n",
    "        if batch%100==0:\n",
    "              print('Epoch {} Batch {} Trn Loss {:.4f} Val Loss {:.4f} Trn_Acc {:.2f} Val_Acc {:.2f}'.format(epoch + 1,\n",
    "                                                                                                             batch,\n",
    "                                                                                                             batch_loss.numpy(),\n",
    "                                                                                                             val_loss.numpy(),\n",
    "                                                                                                             trn_acc_metric.result().numpy(),\n",
    "                                                                                                             val_acc_metric.result().numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Trn Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Epoch {} Val Loss {:.4f}'.format(epoch + 1,\n",
    "                                      val_total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(trn_loss_list)\n",
    "plt.plot(val_loss_list)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('pre-deploy_models/imdbEager.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer_imdb.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "    \n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "test_sentence = ['Sapura Energy bags 5 new contracts worth RM1.3 billion - Free Malaysia Today', \n",
    "                 'BREAKING: Trump just signed two executive orders that seek to expedite permits for pipelines and other fossil fuel projects by restricting public input and states authority.',\n",
    "                'Number of companies producing oil and gas in Western Canada drops 17.5% since 2014', \n",
    "                'Wheelchair customers stuck out in the pouring rain when the cab was booked in advance is OUTRAGEOUS. This cab is now 55 minutes late and my client has missed a VERY important DRs appt. BRING #Uber TO VANCOUVER if anything just to SCREW with @vancouvertaxi customers. @NEWS1130',\n",
    "                'Uber worst service... Cab booked to take 1.5 year child to doctor and cab did not come more then 1hr driver not answering and not cancelling the ride...',\n",
    "                'That #Uber ad sums up that company!! I’ve no sympathy for anyone male or female who uses em and becomes unstuck! They’re not even fucking cheap that’s a myth! Use your local mini cab firm n black cabs in London! Let’s keep money in England ay n drive this firm out!',\n",
    "                'Uber is awesome!',\n",
    "                'Looking forward to your  presentation at Benzinga next week! Armenians represent! ']\n",
    "\n",
    "test_sentence = tokenizer.texts_to_sequences(test_sentence)\n",
    "test_sentence = sequence.pad_sequences(test_sentence, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0413 16:28:37.887531 139752145893184 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f19f9d6f6d8>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
      "W0413 16:28:37.895192 139752145893184 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f19fc7f7438>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate, Flatten, Embedding\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, Input, LSTM\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM, CuDNNGRU\n",
    "\n",
    "class MyLSTM(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length)\n",
    "        self.dropout1 = SpatialDropout1D(0.3)\n",
    "        self.lstm1 = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True, dropout=0.2))\n",
    "        self.gmp = GlobalMaxPooling1D()\n",
    "        self.dense1 = Dense(100, activation='relu')\n",
    "        self.dropout2 = Dropout(0.2)\n",
    "        self.denseOut = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.gmp(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.denseOut(x)\n",
    "        return x\n",
    "    \n",
    "model = MyLSTM()\n",
    "model.build(input_shape=(vocab_size, max_length))\n",
    "model.load_weights('pre-deploy_models/imdbEager.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('pre-deploy_models/imdbEager.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9991095e-01],\n",
       "       [9.9998152e-01],\n",
       "       [0.0000000e+00],\n",
       "       [1.0728836e-06],\n",
       "       [0.0000000e+00],\n",
       "       [0.0000000e+00],\n",
       "       [1.0000000e+00],\n",
       "       [9.9390018e-01]], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = ['Sapura Energy bags 5 new contracts worth RM1.3 billion - Free Malaysia Today', \n",
    "                 'BREAKING: Trump just signed two executive orders that seek to expedite permits for pipelines and other fossil fuel projects by restricting public input and states authority.',\n",
    "                'Number of companies producing oil and gas in Western Canada drops 17.5% since 2014', \n",
    "                'Wheelchair customers stuck out in the pouring rain when the cab was booked in advance is OUTRAGEOUS. This cab is now 55 minutes late and my client has missed a VERY important DRs appt. BRING #Uber TO VANCOUVER if anything just to SCREW with @vancouvertaxi customers. @NEWS1130',\n",
    "                'Uber worst service... Cab booked to take 1.5 year child to doctor and cab did not come more then 1hr driver not answering and not cancelling the ride...',\n",
    "                'That #Uber ad sums up that company!! I’ve no sympathy for anyone male or female who uses em and becomes unstuck! They’re not even fucking cheap that’s a myth! Use your local mini cab firm n black cabs in London! Let’s keep money in England ay n drive this firm out!',\n",
    "                'Uber is awesome!',\n",
    "                'Looking forward to your  presentation at Benzinga next week! Armenians represent! ']\n",
    "\n",
    "model.predict(test_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
