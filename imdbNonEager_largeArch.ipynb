{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import pickle \n",
    "import os\n",
    "\n",
    "keras = tf.keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
    "from keras.layers import LSTM, CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_directory_data(directory):\n",
    "    data={}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    for file_path in os.listdir(directory):\n",
    "        with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(directory + \"/pos\")\n",
    "    neg_df = load_directory_data(directory + \"/neg\")\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_dataset(\"data/aclImdb/train/\")\n",
    "test_df = load_dataset(\"data/aclImdb/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df['sentence']\n",
    "y_train = train_df['polarity']\n",
    "x_test = test_df['sentence']\n",
    "y_test = test_df['polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = text.Tokenizer()\n",
    "# tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "# with open('tokenizer_imdb.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer_imdb.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(s.split()) for s in (x_train + x_test)])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILES = [\n",
    "    'embeddings/crawl-300d-2M.vec',\n",
    "    'embeddings/glove.840B.300d.txt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix = np.concatenate(\n",
    "#     [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n",
    "#  np.savetxt('embeddings_concat.txt', embedding_matrix, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.loadtxt('embeddings/embeddings_concat.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LSTM_UNITS = 128\n",
    "DENSE_UNITS = LSTM_UNITS * 4\n",
    "\n",
    "def build_model(embedding_matrix):\n",
    "    words = Input(shape=(max_length,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, \n",
    "                           return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, \n",
    "                           return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = add([hidden, Dense(DENSE_UNITS, activation='relu')(hidden)])\n",
    "    hidden = Dropout(0.2)(hidden)\n",
    "    hidden = add([hidden, Dense(DENSE_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='sigmoid')(hidden)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=result)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='/media/eigenstir/1TBSecondary/tbgraphs', histogram_freq=0, write_graph=True, write_images=True)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "lrs = LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 279s 11ms/step - loss: 0.6612 - acc: 0.5896 - val_loss: 0.6015 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.67392, saving model to pre-deploy_models/imdbNonEager_bestModel0.h5\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 288s 12ms/step - loss: 0.5850 - acc: 0.6876 - val_loss: 0.5565 - val_acc: 0.7173\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.71732, saving model to pre-deploy_models/imdbNonEager_bestModel0.h5\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 288s 12ms/step - loss: 0.5468 - acc: 0.7179 - val_loss: 0.5346 - val_acc: 0.7249\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.72488, saving model to pre-deploy_models/imdbNonEager_bestModel0.h5\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 288s 12ms/step - loss: 0.5201 - acc: 0.7402 - val_loss: 0.5326 - val_acc: 0.7285\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.72852, saving model to pre-deploy_models/imdbNonEager_bestModel0.h5\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 289s 12ms/step - loss: 0.6644 - acc: 0.5890 - val_loss: 0.5968 - val_acc: 0.6907\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.69072, saving model to pre-deploy_models/imdbNonEager_bestModel1.h5\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 288s 12ms/step - loss: 0.5853 - acc: 0.6882 - val_loss: 0.5564 - val_acc: 0.7221\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.72212, saving model to pre-deploy_models/imdbNonEager_bestModel1.h5\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 289s 12ms/step - loss: 0.5425 - acc: 0.7227 - val_loss: 0.5337 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.73324, saving model to pre-deploy_models/imdbNonEager_bestModel1.h5\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 290s 12ms/step - loss: 0.5186 - acc: 0.7424 - val_loss: 0.5159 - val_acc: 0.7442\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.74420, saving model to pre-deploy_models/imdbNonEager_bestModel1.h5\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 4\n",
    "NUM_MODELS = 2\n",
    "weights = []\n",
    "\n",
    "for model_idx in range(NUM_MODELS):\n",
    "    model = build_model(embedding_matrix)\n",
    "    for global_epoch in range(EPOCHS):\n",
    "        model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data = (x_test, y_test),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=1,\n",
    "        callbacks=[tbCallBack,\n",
    "                   es, lrs,\n",
    "                  ModelCheckpoint('pre-deploy_models/imdbNonEager_bestModel{}.h5'.format(str(model_idx)), \n",
    "                     monitor='val_acc', \n",
    "                     mode='max', \n",
    "                     verbose=1, \n",
    "                     save_best_only=True)])\n",
    "        model.save_weights('pre-deploy_models/imdbNonEager{}.h5'.format(str(model_idx)))\n",
    "        weights.append(2 ** global_epoch)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='/media/eigenstir/1TBSecondary/tbgraphs', histogram_freq=0, write_graph=True, write_images=True)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7)\n",
    "lrs = LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n",
    "mc = ModelCheckpoint('pre-deploy_models/imdbNonEager_LSTMbestModel.h5', \n",
    "                     monitor='val_acc', mode='max', \n",
    "                     verbose=1, \n",
    "                     save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LSTM_UNITS = 256\n",
    "DENSE_UNITS = LSTM_UNITS * 4\n",
    "\n",
    "def build_model(embedding_matrix):\n",
    "    words = Input(shape=(max_length,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, \n",
    "                           return_sequences=True))(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(DENSE_UNITS, activation='relu')(x)\n",
    "    result = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=result)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/30\n",
      "25000/25000 [==============================] - 276s 11ms/step - loss: 0.4043 - acc: 0.8132 - val_loss: 0.4897 - val_acc: 0.7666\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76660, saving model to pre-deploy_models/imdbNonEager_LSTMbestModel.h5\n",
      "Epoch 2/30\n",
      "25000/25000 [==============================] - 283s 11ms/step - loss: 0.3902 - acc: 0.8197 - val_loss: 0.4901 - val_acc: 0.7680\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76660 to 0.76800, saving model to pre-deploy_models/imdbNonEager_LSTMbestModel.h5\n",
      "Epoch 3/30\n",
      "25000/25000 [==============================] - 288s 12ms/step - loss: 0.3721 - acc: 0.8323 - val_loss: 0.4992 - val_acc: 0.7672\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76800\n",
      "Epoch 4/30\n",
      "25000/25000 [==============================] - 284s 11ms/step - loss: 0.3651 - acc: 0.8358 - val_loss: 0.4948 - val_acc: 0.7656\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76800\n",
      "Epoch 5/30\n",
      "25000/25000 [==============================] - 287s 11ms/step - loss: 0.3502 - acc: 0.8434 - val_loss: 0.4996 - val_acc: 0.7674\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76800\n",
      "Epoch 6/30\n",
      "25000/25000 [==============================] - 287s 11ms/step - loss: 0.3387 - acc: 0.8488 - val_loss: 0.5100 - val_acc: 0.7636\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.76800\n",
      "Epoch 7/30\n",
      "25000/25000 [==============================] - 286s 11ms/step - loss: 0.3297 - acc: 0.8556 - val_loss: 0.5159 - val_acc: 0.7625\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.76800\n",
      "Epoch 8/30\n",
      "25000/25000 [==============================] - 288s 12ms/step - loss: 0.3169 - acc: 0.8620 - val_loss: 0.5403 - val_acc: 0.7517\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.76800\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data = (x_test, y_test),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=30,\n",
    "        callbacks=[tbCallBack, es, lrs, mc])\n",
    "model.save_weights('pre-deploy_models/imdbNonEager_LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model.save_weights('pre-deploy_models/imdbNonEager.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Pulled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_dataset(\"data/aclImdb/train/\")\n",
    "test_df = load_dataset(\"data/aclImdb/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "\n",
    "for filename in os.listdir('data/mycollection'):\n",
    "    df = pd.read_csv('data/mycollection/{}'.format(filename), sep='\\t')\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_frame, x_test_frame, y_train_frame, y_test_frame = train_test_split(frame['1'], frame['target'], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df['sentence']\n",
    "y_train = train_df['polarity']\n",
    "x_test = test_df['sentence']\n",
    "y_test = test_df['polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.concat([x_train,x_train_frame])\n",
    "y_train = pd.concat([y_train,y_train_frame])\n",
    "x_test = pd.concat([x_test,x_test_frame])\n",
    "y_test = pd.concat([y_test, y_test_frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "with open('tokenizer_imdb_merged.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(s.split()) for s in (x_train + x_test)])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LSTM_UNITS = 256\n",
    "DENSE_UNITS = LSTM_UNITS * 4\n",
    "\n",
    "def build_model(embedding_matrix):\n",
    "    words = Input(shape=(max_length,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, \n",
    "                           return_sequences=True))(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(DENSE_UNITS, activation='relu')(x)\n",
    "    result = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=result)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='/media/eigenstir/1TBSecondary/tbgraphs', histogram_freq=0, write_graph=True, write_images=True)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7)\n",
    "lrs = LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n",
    "mc = ModelCheckpoint('pre-deploy_models/imdbNonEager_merged_LSTMbestModel.h5', \n",
    "                     monitor='val_acc', mode='max', \n",
    "                     verbose=1, \n",
    "                     save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25145 samples, validate on 25017 samples\n",
      "Epoch 1/1\n",
      "25145/25145 [==============================] - 491s 20ms/step - loss: 0.6561 - acc: 0.5916 - val_loss: 0.5722 - val_acc: 0.7038\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.70380, saving model to pre-deploy_models/imdbNonEager_merged_LSTMbestModel.h5\n",
      "Train on 25145 samples, validate on 25017 samples\n",
      "Epoch 1/1\n",
      "25145/25145 [==============================] - 503s 20ms/step - loss: 0.5361 - acc: 0.7256 - val_loss: 0.5081 - val_acc: 0.7503\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.70380 to 0.75033, saving model to pre-deploy_models/imdbNonEager_merged_LSTMbestModel.h5\n",
      "Train on 25145 samples, validate on 25017 samples\n",
      "Epoch 1/1\n",
      "25145/25145 [==============================] - 510s 20ms/step - loss: 0.4676 - acc: 0.7759 - val_loss: 0.4946 - val_acc: 0.7614\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.75033 to 0.76136, saving model to pre-deploy_models/imdbNonEager_merged_LSTMbestModel.h5\n",
      "Train on 25145 samples, validate on 25017 samples\n",
      "Epoch 1/1\n",
      "16384/25145 [==================>...........] - ETA: 2:06 - loss: 0.4244 - acc: 0.8053"
     ]
    }
   ],
   "source": [
    "EPOCHS=30\n",
    "model = build_model(embedding_matrix)\n",
    "\n",
    "for global_epoch in range(EPOCHS):\n",
    "    model.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            validation_data = (x_test, y_test),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=1,\n",
    "            callbacks=[tbCallBack, es, lrs, mc])\n",
    "model.save_weights('pre-deploy_models/imdbNonEager_merged_LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_idx in range(NUM_MODELS):\n",
    "#     model = build_model(embedding_matrix, y_aux_train.shape[-1])\n",
    "#     for global_epoch in range(EPOCHS):\n",
    "#         model.fit(\n",
    "#             x_train,\n",
    "#             [y_train, y_aux_train],\n",
    "#             batch_size=BATCH_SIZE,\n",
    "#             epochs=1,\n",
    "#             callbacks=[\n",
    "#                 LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n",
    "#             ]\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
