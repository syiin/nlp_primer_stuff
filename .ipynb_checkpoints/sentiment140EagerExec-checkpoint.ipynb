{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['training.1600000.processed.noemoticon.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data/sentiment140/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('data/sentiment140/training.1600000.processed.noemoticon.csv', encoding = 'ISO-8859-1', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                   5  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data_df.iloc[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_polarity(item):\n",
    "    if item == 4:\n",
    "        item = 1\n",
    "    return item\n",
    "        \n",
    "labels = labels.map(change_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_usernames(item):\n",
    "    if item[0] == '@':\n",
    "        start = item.find(' ')\n",
    "        item = item[start:]\n",
    "    return item\n",
    "        \n",
    "sentences = sentences.map(remove_usernames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "\n",
    "# tokenizer = text.Tokenizer()\n",
    "# tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# saving\n",
    "# with open('tokenizer_sen140.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer_sen140.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(s.split()) for s in (sentences)])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "sentences = tokenizer.texts_to_sequences(sentences)\n",
    "sentences = sequence.pad_sequences(sentences, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data & Construct Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(sentences, labels, \n",
    "                                                    test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280000, 320000, 1280000, 320000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(x_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset shapes: ((128, 64), (128,)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "train_dataset.shuffle(BATCH_SIZE)\n",
    "test_dataset.shuffle(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([128, 64]), TensorShape([128]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_inp_batch, example_targ_batch = next(iter(train_dataset))\n",
    "example_inp_batch.shape, example_targ_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate, Flatten, Embedding\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, Input, LSTM\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM, CuDNNGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0412 19:05:18.036418 140020481242944 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f588d148d30>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
      "W0412 19:05:18.038701 140020481242944 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f588d167630>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 500\n",
    "LSTM_UNITS = 256\n",
    "\n",
    "class MyLSTM(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length)\n",
    "        self.dropout1 = SpatialDropout1D(0.3)\n",
    "        self.lstm1 = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))\n",
    "        self.gmp = GlobalMaxPooling1D()\n",
    "        self.dense1 = Dense(100, activation='tanh')\n",
    "        self.dropout2 = Dropout(0.2)\n",
    "        self.denseOut = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.gmp(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.denseOut(x)\n",
    "        return x\n",
    "    \n",
    "model = MyLSTM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, clipnorm=1.)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    return tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.expand_dims(real, axis=1), \n",
    "                                                              pred, from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(inp)\n",
    "        loss += loss_function(targ, preds)\n",
    "    \n",
    "    trn_acc_metric.update_state(targ, preds)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def val_step(inp, targ):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(inp)\n",
    "        loss += loss_function(targ, preds)\n",
    "    \n",
    "    val_acc_metric.update_state(targ, preds)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Trn Loss 0.7199 Val Loss 0.7224 Trn_Acc 0.51 Val_Acc 0.50\n",
      "Epoch 1 Batch 100 Trn Loss 0.6931 Val Loss 0.6931 Trn_Acc 0.51 Val_Acc 0.50\n",
      "Epoch 1 Batch 200 Trn Loss 0.6932 Val Loss 0.6930 Trn_Acc 0.50 Val_Acc 0.50\n",
      "Epoch 1 Batch 300 Trn Loss 0.6931 Val Loss 0.6931 Trn_Acc 0.50 Val_Acc 0.50\n",
      "Epoch 1 Batch 400 Trn Loss 0.6931 Val Loss 0.6932 Trn_Acc 0.50 Val_Acc 0.50\n",
      "Epoch 1 Batch 500 Trn Loss 0.6932 Val Loss 0.6931 Trn_Acc 0.50 Val_Acc 0.50\n",
      "Epoch 1 Batch 600 Trn Loss 0.6932 Val Loss 0.6931 Trn_Acc 0.50 Val_Acc 0.50\n",
      "Epoch 1 Batch 700 Trn Loss 0.6932 Val Loss 0.6931 Trn_Acc 0.50 Val_Acc 0.50\n",
      "Epoch 1 Batch 800 Trn Loss 0.6931 Val Loss 0.6931 Trn_Acc 0.50 Val_Acc 0.50\n",
      "Epoch 1 Batch 900 Trn Loss 0.6954 Val Loss 0.6939 Trn_Acc 0.50 Val_Acc 0.50\n",
      "Epoch 1 Batch 1000 Trn Loss 0.6898 Val Loss 0.6938 Trn_Acc 0.50 Val_Acc 0.50\n",
      "Epoch 1 Batch 1100 Trn Loss 0.6852 Val Loss 0.6246 Trn_Acc 0.51 Val_Acc 0.51\n",
      "Epoch 1 Batch 1200 Trn Loss 0.6102 Val Loss 0.6461 Trn_Acc 0.53 Val_Acc 0.53\n",
      "Epoch 1 Batch 1300 Trn Loss 0.6448 Val Loss 0.6330 Trn_Acc 0.54 Val_Acc 0.54\n",
      "Epoch 1 Batch 1400 Trn Loss 0.5989 Val Loss 0.6010 Trn_Acc 0.56 Val_Acc 0.56\n",
      "Epoch 1 Batch 1500 Trn Loss 0.5866 Val Loss 0.6281 Trn_Acc 0.57 Val_Acc 0.57\n",
      "Epoch 1 Batch 1600 Trn Loss 0.6187 Val Loss 0.6001 Trn_Acc 0.58 Val_Acc 0.58\n",
      "Epoch 1 Batch 1700 Trn Loss 0.5826 Val Loss 0.5889 Trn_Acc 0.59 Val_Acc 0.59\n",
      "Epoch 1 Batch 1800 Trn Loss 0.6097 Val Loss 0.5747 Trn_Acc 0.60 Val_Acc 0.60\n",
      "Epoch 1 Batch 1900 Trn Loss 0.6472 Val Loss 0.5840 Trn_Acc 0.61 Val_Acc 0.61\n",
      "Epoch 1 Batch 2000 Trn Loss 0.6127 Val Loss 0.6518 Trn_Acc 0.62 Val_Acc 0.62\n",
      "Epoch 1 Batch 2100 Trn Loss 0.5543 Val Loss 0.6150 Trn_Acc 0.63 Val_Acc 0.63\n",
      "Epoch 1 Batch 2200 Trn Loss 0.6539 Val Loss 0.5856 Trn_Acc 0.63 Val_Acc 0.63\n",
      "Epoch 1 Batch 2300 Trn Loss 0.6009 Val Loss 0.6222 Trn_Acc 0.64 Val_Acc 0.64\n",
      "Epoch 1 Batch 2400 Trn Loss 0.5854 Val Loss 0.5670 Trn_Acc 0.64 Val_Acc 0.64\n",
      "Epoch 1 Trn Loss 0.1615\n",
      "Epoch 1 Val Loss 0.1614\n",
      "Time taken for 1 epoch 359.5497124195099 sec\n",
      "\n",
      "Epoch 2 Batch 0 Trn Loss 0.6067 Val Loss 0.6212 Trn_Acc 0.65 Val_Acc 0.65\n",
      "Epoch 2 Batch 100 Trn Loss 0.5772 Val Loss 0.5958 Trn_Acc 0.65 Val_Acc 0.65\n",
      "Epoch 2 Batch 200 Trn Loss 0.5994 Val Loss 0.5681 Trn_Acc 0.66 Val_Acc 0.66\n",
      "Epoch 2 Batch 300 Trn Loss 0.5890 Val Loss 0.5803 Trn_Acc 0.66 Val_Acc 0.66\n",
      "Epoch 2 Batch 400 Trn Loss 0.5907 Val Loss 0.6219 Trn_Acc 0.67 Val_Acc 0.67\n",
      "Epoch 2 Batch 500 Trn Loss 0.6272 Val Loss 0.6298 Trn_Acc 0.67 Val_Acc 0.67\n",
      "Epoch 2 Batch 600 Trn Loss 0.6153 Val Loss 0.5688 Trn_Acc 0.67 Val_Acc 0.67\n",
      "Epoch 2 Batch 700 Trn Loss 0.6417 Val Loss 0.5812 Trn_Acc 0.68 Val_Acc 0.68\n",
      "Epoch 2 Batch 800 Trn Loss 0.6098 Val Loss 0.5901 Trn_Acc 0.68 Val_Acc 0.68\n",
      "Epoch 2 Batch 900 Trn Loss 0.6202 Val Loss 0.5983 Trn_Acc 0.68 Val_Acc 0.68\n",
      "Epoch 2 Batch 1000 Trn Loss 0.5741 Val Loss 0.6075 Trn_Acc 0.69 Val_Acc 0.69\n",
      "Epoch 2 Batch 1100 Trn Loss 0.5857 Val Loss 0.5986 Trn_Acc 0.69 Val_Acc 0.69\n",
      "Epoch 2 Batch 1200 Trn Loss 0.5757 Val Loss 0.6088 Trn_Acc 0.69 Val_Acc 0.69\n",
      "Epoch 2 Batch 1300 Trn Loss 0.6014 Val Loss 0.6262 Trn_Acc 0.70 Val_Acc 0.69\n",
      "Epoch 2 Batch 1400 Trn Loss 0.5764 Val Loss 0.5917 Trn_Acc 0.70 Val_Acc 0.70\n",
      "Epoch 2 Batch 1500 Trn Loss 0.5659 Val Loss 0.6310 Trn_Acc 0.70 Val_Acc 0.70\n",
      "Epoch 2 Batch 1600 Trn Loss 0.6078 Val Loss 0.5939 Trn_Acc 0.70 Val_Acc 0.70\n",
      "Epoch 2 Batch 1700 Trn Loss 0.5735 Val Loss 0.5841 Trn_Acc 0.71 Val_Acc 0.70\n",
      "Epoch 2 Batch 1800 Trn Loss 0.5846 Val Loss 0.5771 Trn_Acc 0.71 Val_Acc 0.70\n",
      "Epoch 2 Batch 1900 Trn Loss 0.6294 Val Loss 0.5813 Trn_Acc 0.71 Val_Acc 0.71\n",
      "Epoch 2 Batch 2000 Trn Loss 0.5942 Val Loss 0.6427 Trn_Acc 0.71 Val_Acc 0.71\n",
      "Epoch 2 Batch 2100 Trn Loss 0.5502 Val Loss 0.6125 Trn_Acc 0.71 Val_Acc 0.71\n",
      "Epoch 2 Batch 2200 Trn Loss 0.6319 Val Loss 0.5830 Trn_Acc 0.72 Val_Acc 0.71\n",
      "Epoch 2 Batch 2300 Trn Loss 0.5869 Val Loss 0.6169 Trn_Acc 0.72 Val_Acc 0.71\n",
      "Epoch 2 Batch 2400 Trn Loss 0.5719 Val Loss 0.5597 Trn_Acc 0.72 Val_Acc 0.71\n",
      "Epoch 2 Trn Loss 0.1485\n",
      "Epoch 2 Val Loss 0.1499\n",
      "Time taken for 1 epoch 349.4170677661896 sec\n",
      "\n",
      "Epoch 3 Batch 0 Trn Loss 0.5927 Val Loss 0.6192 Trn_Acc 0.72 Val_Acc 0.71\n",
      "Epoch 3 Batch 100 Trn Loss 0.5562 Val Loss 0.5909 Trn_Acc 0.72 Val_Acc 0.72\n",
      "Epoch 3 Batch 200 Trn Loss 0.5894 Val Loss 0.5653 Trn_Acc 0.73 Val_Acc 0.72\n",
      "Epoch 3 Batch 300 Trn Loss 0.5666 Val Loss 0.5798 Trn_Acc 0.73 Val_Acc 0.72\n",
      "Epoch 3 Batch 400 Trn Loss 0.5857 Val Loss 0.6302 Trn_Acc 0.73 Val_Acc 0.72\n",
      "Epoch 3 Batch 500 Trn Loss 0.6100 Val Loss 0.6240 Trn_Acc 0.73 Val_Acc 0.72\n",
      "Epoch 3 Batch 600 Trn Loss 0.6140 Val Loss 0.5673 Trn_Acc 0.73 Val_Acc 0.72\n",
      "Epoch 3 Batch 700 Trn Loss 0.6269 Val Loss 0.5778 Trn_Acc 0.73 Val_Acc 0.72\n",
      "Epoch 3 Batch 800 Trn Loss 0.5838 Val Loss 0.5940 Trn_Acc 0.73 Val_Acc 0.72\n",
      "Epoch 3 Batch 900 Trn Loss 0.5980 Val Loss 0.5914 Trn_Acc 0.74 Val_Acc 0.73\n",
      "Epoch 3 Batch 1000 Trn Loss 0.5519 Val Loss 0.6004 Trn_Acc 0.74 Val_Acc 0.73\n",
      "Epoch 3 Batch 1100 Trn Loss 0.5655 Val Loss 0.6104 Trn_Acc 0.74 Val_Acc 0.73\n",
      "Epoch 3 Batch 1200 Trn Loss 0.5749 Val Loss 0.6126 Trn_Acc 0.74 Val_Acc 0.73\n",
      "Epoch 3 Batch 1300 Trn Loss 0.5789 Val Loss 0.6225 Trn_Acc 0.74 Val_Acc 0.73\n",
      "Epoch 3 Batch 1400 Trn Loss 0.5746 Val Loss 0.5768 Trn_Acc 0.74 Val_Acc 0.73\n",
      "Epoch 3 Batch 1500 Trn Loss 0.5652 Val Loss 0.6208 Trn_Acc 0.74 Val_Acc 0.73\n",
      "Epoch 3 Batch 1600 Trn Loss 0.6039 Val Loss 0.5940 Trn_Acc 0.74 Val_Acc 0.73\n",
      "Epoch 3 Batch 1700 Trn Loss 0.5628 Val Loss 0.5825 Trn_Acc 0.75 Val_Acc 0.73\n",
      "Epoch 3 Batch 1800 Trn Loss 0.5838 Val Loss 0.5808 Trn_Acc 0.75 Val_Acc 0.73\n",
      "Epoch 3 Batch 1900 Trn Loss 0.6181 Val Loss 0.5805 Trn_Acc 0.75 Val_Acc 0.73\n",
      "Epoch 3 Batch 2000 Trn Loss 0.5752 Val Loss 0.6506 Trn_Acc 0.75 Val_Acc 0.73\n",
      "Epoch 3 Batch 2100 Trn Loss 0.5389 Val Loss 0.6107 Trn_Acc 0.75 Val_Acc 0.74\n",
      "Epoch 3 Batch 2200 Trn Loss 0.6224 Val Loss 0.5842 Trn_Acc 0.75 Val_Acc 0.74\n",
      "Epoch 3 Batch 2300 Trn Loss 0.5744 Val Loss 0.6111 Trn_Acc 0.75 Val_Acc 0.74\n",
      "Epoch 3 Batch 2400 Trn Loss 0.5571 Val Loss 0.5514 Trn_Acc 0.75 Val_Acc 0.74\n",
      "Epoch 3 Trn Loss 0.1458\n",
      "Epoch 3 Val Loss 0.1496\n",
      "Time taken for 1 epoch 374.9010260105133 sec\n",
      "\n",
      "Epoch 4 Batch 0 Trn Loss 0.5936 Val Loss 0.6052 Trn_Acc 0.75 Val_Acc 0.74\n",
      "Epoch 4 Batch 100 Trn Loss 0.5612 Val Loss 0.5907 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 200 Trn Loss 0.5757 Val Loss 0.5669 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 300 Trn Loss 0.5549 Val Loss 0.5762 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 400 Trn Loss 0.5736 Val Loss 0.6336 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 500 Trn Loss 0.6032 Val Loss 0.6158 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 600 Trn Loss 0.6018 Val Loss 0.5697 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 700 Trn Loss 0.6126 Val Loss 0.5766 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 800 Trn Loss 0.5864 Val Loss 0.5946 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 900 Trn Loss 0.6003 Val Loss 0.5972 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 1000 Trn Loss 0.5591 Val Loss 0.6065 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 1100 Trn Loss 0.5630 Val Loss 0.6075 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 1200 Trn Loss 0.5716 Val Loss 0.6100 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 1300 Trn Loss 0.5803 Val Loss 0.6284 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 4 Batch 1400 Trn Loss 0.5683 Val Loss 0.5780 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 4 Batch 1500 Trn Loss 0.5636 Val Loss 0.6183 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 4 Batch 1600 Trn Loss 0.6016 Val Loss 0.5811 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 4 Batch 1700 Trn Loss 0.5533 Val Loss 0.5772 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 4 Batch 1800 Trn Loss 0.5776 Val Loss 0.5920 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 4 Batch 1900 Trn Loss 0.6155 Val Loss 0.5730 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 4 Batch 2000 Trn Loss 0.5727 Val Loss 0.6379 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 4 Batch 2100 Trn Loss 0.5414 Val Loss 0.6065 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 4 Batch 2200 Trn Loss 0.6183 Val Loss 0.5846 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 4 Batch 2300 Trn Loss 0.5683 Val Loss 0.6046 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 4 Batch 2400 Trn Loss 0.5647 Val Loss 0.5615 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 4 Trn Loss 0.1442\n",
      "Epoch 4 Val Loss 0.1495\n",
      "Time taken for 1 epoch 347.7864043712616 sec\n",
      "\n",
      "Epoch 5 Batch 0 Trn Loss 0.5808 Val Loss 0.6044 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 5 Batch 100 Trn Loss 0.5551 Val Loss 0.5914 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 5 Batch 200 Trn Loss 0.5793 Val Loss 0.5816 Trn_Acc 0.78 Val_Acc 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 300 Trn Loss 0.5520 Val Loss 0.5882 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 400 Trn Loss 0.5690 Val Loss 0.6381 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 500 Trn Loss 0.5982 Val Loss 0.6119 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 600 Trn Loss 0.6055 Val Loss 0.5543 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 700 Trn Loss 0.5986 Val Loss 0.5812 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 800 Trn Loss 0.5934 Val Loss 0.5893 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 900 Trn Loss 0.5965 Val Loss 0.6006 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 1000 Trn Loss 0.5500 Val Loss 0.5924 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 1100 Trn Loss 0.5594 Val Loss 0.5955 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 1200 Trn Loss 0.5682 Val Loss 0.6174 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 1300 Trn Loss 0.5728 Val Loss 0.6301 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 1400 Trn Loss 0.5656 Val Loss 0.5824 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 1500 Trn Loss 0.5658 Val Loss 0.6169 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 1600 Trn Loss 0.5911 Val Loss 0.5919 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 1700 Trn Loss 0.5521 Val Loss 0.5740 Trn_Acc 0.78 Val_Acc 0.76\n",
      "Epoch 5 Batch 1800 Trn Loss 0.5655 Val Loss 0.5904 Trn_Acc 0.78 Val_Acc 0.76\n",
      "Epoch 5 Batch 1900 Trn Loss 0.6059 Val Loss 0.5772 Trn_Acc 0.78 Val_Acc 0.76\n",
      "Epoch 5 Batch 2000 Trn Loss 0.5748 Val Loss 0.6374 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 5 Batch 2100 Trn Loss 0.5360 Val Loss 0.6071 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 5 Batch 2200 Trn Loss 0.6037 Val Loss 0.5910 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 5 Batch 2300 Trn Loss 0.5714 Val Loss 0.6130 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 5 Batch 2400 Trn Loss 0.5596 Val Loss 0.5593 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 5 Trn Loss 0.1431\n",
      "Epoch 5 Val Loss 0.1495\n",
      "Time taken for 1 epoch 325.4093225002289 sec\n",
      "\n",
      "Epoch 6 Batch 0 Trn Loss 0.5740 Val Loss 0.6121 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 100 Trn Loss 0.5465 Val Loss 0.5820 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 200 Trn Loss 0.5712 Val Loss 0.5654 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 300 Trn Loss 0.5454 Val Loss 0.5887 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 400 Trn Loss 0.5686 Val Loss 0.6412 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 500 Trn Loss 0.5815 Val Loss 0.6096 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 600 Trn Loss 0.6002 Val Loss 0.5590 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 700 Trn Loss 0.5973 Val Loss 0.5923 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 800 Trn Loss 0.5724 Val Loss 0.5999 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 900 Trn Loss 0.5975 Val Loss 0.5976 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 1000 Trn Loss 0.5511 Val Loss 0.6016 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 1100 Trn Loss 0.5543 Val Loss 0.6177 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 1200 Trn Loss 0.5753 Val Loss 0.6129 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 1300 Trn Loss 0.5693 Val Loss 0.6254 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 1400 Trn Loss 0.5681 Val Loss 0.5810 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 1500 Trn Loss 0.5613 Val Loss 0.6142 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 1600 Trn Loss 0.5875 Val Loss 0.5722 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 1700 Trn Loss 0.5552 Val Loss 0.5669 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 1800 Trn Loss 0.5607 Val Loss 0.5866 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 6 Batch 1900 Trn Loss 0.5972 Val Loss 0.5756 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 6 Batch 2000 Trn Loss 0.5684 Val Loss 0.6502 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 6 Batch 2100 Trn Loss 0.5343 Val Loss 0.6047 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 6 Batch 2200 Trn Loss 0.6093 Val Loss 0.5867 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 6 Batch 2300 Trn Loss 0.5639 Val Loss 0.6089 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 6 Batch 2400 Trn Loss 0.5617 Val Loss 0.5639 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 6 Trn Loss 0.1424\n",
      "Epoch 6 Val Loss 0.1495\n",
      "Time taken for 1 epoch 328.02992510795593 sec\n",
      "\n",
      "Epoch 7 Batch 0 Trn Loss 0.5697 Val Loss 0.6126 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 100 Trn Loss 0.5392 Val Loss 0.5834 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 200 Trn Loss 0.5738 Val Loss 0.5638 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 300 Trn Loss 0.5402 Val Loss 0.5826 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 400 Trn Loss 0.5700 Val Loss 0.6238 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 500 Trn Loss 0.5889 Val Loss 0.6118 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 600 Trn Loss 0.5923 Val Loss 0.5509 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 700 Trn Loss 0.5876 Val Loss 0.5730 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 800 Trn Loss 0.5738 Val Loss 0.5844 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 900 Trn Loss 0.6014 Val Loss 0.5947 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1000 Trn Loss 0.5488 Val Loss 0.5935 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1100 Trn Loss 0.5503 Val Loss 0.6032 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1200 Trn Loss 0.5671 Val Loss 0.6078 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1300 Trn Loss 0.5675 Val Loss 0.6258 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1400 Trn Loss 0.5653 Val Loss 0.5778 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1500 Trn Loss 0.5595 Val Loss 0.6386 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1600 Trn Loss 0.5828 Val Loss 0.5800 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1700 Trn Loss 0.5452 Val Loss 0.5812 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1800 Trn Loss 0.5598 Val Loss 0.5955 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1900 Trn Loss 0.5910 Val Loss 0.5742 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 2000 Trn Loss 0.5615 Val Loss 0.6413 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 2100 Trn Loss 0.5312 Val Loss 0.6058 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 2200 Trn Loss 0.6026 Val Loss 0.5902 Trn_Acc 0.80 Val_Acc 0.77\n",
      "Epoch 7 Batch 2300 Trn Loss 0.5702 Val Loss 0.6106 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 7 Batch 2400 Trn Loss 0.5558 Val Loss 0.5597 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 7 Trn Loss 0.1417\n",
      "Epoch 7 Val Loss 0.1496\n",
      "Time taken for 1 epoch 325.50069308280945 sec\n",
      "\n",
      "Epoch 8 Batch 0 Trn Loss 0.5649 Val Loss 0.6222 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 100 Trn Loss 0.5431 Val Loss 0.5820 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 200 Trn Loss 0.5799 Val Loss 0.5716 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 300 Trn Loss 0.5389 Val Loss 0.5823 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 400 Trn Loss 0.5667 Val Loss 0.6319 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 500 Trn Loss 0.5862 Val Loss 0.6184 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 600 Trn Loss 0.5892 Val Loss 0.5594 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 700 Trn Loss 0.5894 Val Loss 0.5695 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 800 Trn Loss 0.5787 Val Loss 0.5864 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 900 Trn Loss 0.5946 Val Loss 0.5907 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 1000 Trn Loss 0.5423 Val Loss 0.5970 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 1100 Trn Loss 0.5457 Val Loss 0.6133 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 1200 Trn Loss 0.5548 Val Loss 0.6190 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 1300 Trn Loss 0.5662 Val Loss 0.6227 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 1400 Trn Loss 0.5585 Val Loss 0.5820 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 1500 Trn Loss 0.5533 Val Loss 0.6189 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 1600 Trn Loss 0.5780 Val Loss 0.5838 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 1700 Trn Loss 0.5417 Val Loss 0.5913 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 1800 Trn Loss 0.5582 Val Loss 0.5990 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 1900 Trn Loss 0.5899 Val Loss 0.5745 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 2000 Trn Loss 0.5565 Val Loss 0.6434 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 2100 Trn Loss 0.5364 Val Loss 0.6072 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 2200 Trn Loss 0.5934 Val Loss 0.5853 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 2300 Trn Loss 0.5658 Val Loss 0.6098 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 2400 Trn Loss 0.5494 Val Loss 0.5529 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Trn Loss 0.1411\n",
      "Epoch 8 Val Loss 0.1497\n",
      "Time taken for 1 epoch 329.6902005672455 sec\n",
      "\n",
      "Epoch 9 Batch 0 Trn Loss 0.5716 Val Loss 0.6220 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 100 Trn Loss 0.5324 Val Loss 0.5779 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 200 Trn Loss 0.5745 Val Loss 0.5754 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 300 Trn Loss 0.5448 Val Loss 0.5845 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 400 Trn Loss 0.5683 Val Loss 0.6161 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 500 Trn Loss 0.5799 Val Loss 0.6155 Trn_Acc 0.81 Val_Acc 0.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 600 Trn Loss 0.5957 Val Loss 0.5576 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 700 Trn Loss 0.5804 Val Loss 0.5756 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 800 Trn Loss 0.5658 Val Loss 0.5770 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 900 Trn Loss 0.5899 Val Loss 0.5967 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 1000 Trn Loss 0.5518 Val Loss 0.6073 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 1100 Trn Loss 0.5450 Val Loss 0.6096 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Batch 1200 Trn Loss 0.5513 Val Loss 0.6056 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Batch 1300 Trn Loss 0.5648 Val Loss 0.6210 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Batch 1400 Trn Loss 0.5547 Val Loss 0.5797 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Batch 1500 Trn Loss 0.5557 Val Loss 0.6205 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Batch 1600 Trn Loss 0.5644 Val Loss 0.5878 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Batch 1700 Trn Loss 0.5430 Val Loss 0.5810 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Batch 1800 Trn Loss 0.5598 Val Loss 0.5960 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Batch 1900 Trn Loss 0.5824 Val Loss 0.5906 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Batch 2000 Trn Loss 0.5620 Val Loss 0.6454 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Batch 2100 Trn Loss 0.5317 Val Loss 0.6164 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Batch 2200 Trn Loss 0.6006 Val Loss 0.5929 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Batch 2300 Trn Loss 0.5672 Val Loss 0.6156 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Batch 2400 Trn Loss 0.5434 Val Loss 0.5585 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Trn Loss 0.1407\n",
      "Epoch 9 Val Loss 0.1497\n",
      "Time taken for 1 epoch 328.15516352653503 sec\n",
      "\n",
      "Epoch 10 Batch 0 Trn Loss 0.5660 Val Loss 0.6167 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 100 Trn Loss 0.5387 Val Loss 0.5844 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 200 Trn Loss 0.5665 Val Loss 0.5708 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 300 Trn Loss 0.5395 Val Loss 0.5842 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 400 Trn Loss 0.5636 Val Loss 0.6345 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 500 Trn Loss 0.5779 Val Loss 0.6206 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 600 Trn Loss 0.5896 Val Loss 0.5659 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 700 Trn Loss 0.5787 Val Loss 0.5758 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 800 Trn Loss 0.5677 Val Loss 0.5771 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 900 Trn Loss 0.5925 Val Loss 0.5912 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1000 Trn Loss 0.5433 Val Loss 0.6054 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1100 Trn Loss 0.5482 Val Loss 0.6077 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1200 Trn Loss 0.5520 Val Loss 0.6066 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1300 Trn Loss 0.5684 Val Loss 0.6225 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1400 Trn Loss 0.5589 Val Loss 0.5707 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1500 Trn Loss 0.5466 Val Loss 0.6220 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1600 Trn Loss 0.5736 Val Loss 0.5800 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1700 Trn Loss 0.5374 Val Loss 0.5894 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1800 Trn Loss 0.5537 Val Loss 0.5827 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1900 Trn Loss 0.5797 Val Loss 0.5876 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 2000 Trn Loss 0.5525 Val Loss 0.6455 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 2100 Trn Loss 0.5283 Val Loss 0.6177 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 2200 Trn Loss 0.5876 Val Loss 0.5915 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 2300 Trn Loss 0.5639 Val Loss 0.6002 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 2400 Trn Loss 0.5435 Val Loss 0.5693 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Trn Loss 0.1402\n",
      "Epoch 10 Val Loss 0.1496\n",
      "Time taken for 1 epoch 330.11796021461487 sec\n",
      "\n",
      "Epoch 11 Batch 0 Trn Loss 0.5622 Val Loss 0.6223 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 100 Trn Loss 0.5452 Val Loss 0.5845 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 200 Trn Loss 0.5646 Val Loss 0.5705 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 300 Trn Loss 0.5421 Val Loss 0.5827 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 400 Trn Loss 0.5677 Val Loss 0.6358 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 500 Trn Loss 0.5836 Val Loss 0.6241 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 600 Trn Loss 0.5929 Val Loss 0.5538 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 700 Trn Loss 0.5733 Val Loss 0.5783 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 800 Trn Loss 0.5607 Val Loss 0.5710 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 900 Trn Loss 0.5914 Val Loss 0.5968 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 1000 Trn Loss 0.5392 Val Loss 0.6053 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 1100 Trn Loss 0.5467 Val Loss 0.6094 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 1200 Trn Loss 0.5554 Val Loss 0.6044 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 1300 Trn Loss 0.5622 Val Loss 0.6213 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 1400 Trn Loss 0.5548 Val Loss 0.5732 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 1500 Trn Loss 0.5501 Val Loss 0.6306 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 1600 Trn Loss 0.5711 Val Loss 0.5836 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 1700 Trn Loss 0.5311 Val Loss 0.5744 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 1800 Trn Loss 0.5544 Val Loss 0.5904 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 1900 Trn Loss 0.5837 Val Loss 0.5872 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 2000 Trn Loss 0.5533 Val Loss 0.6444 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 2100 Trn Loss 0.5311 Val Loss 0.6027 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 2200 Trn Loss 0.5959 Val Loss 0.5916 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 2300 Trn Loss 0.5639 Val Loss 0.6058 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 2400 Trn Loss 0.5424 Val Loss 0.5654 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Trn Loss 0.1399\n",
      "Epoch 11 Val Loss 0.1496\n",
      "Time taken for 1 epoch 327.6186261177063 sec\n",
      "\n",
      "Epoch 12 Batch 0 Trn Loss 0.5599 Val Loss 0.6213 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 100 Trn Loss 0.5418 Val Loss 0.5829 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 200 Trn Loss 0.5665 Val Loss 0.5621 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 300 Trn Loss 0.5453 Val Loss 0.5870 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 400 Trn Loss 0.5621 Val Loss 0.6433 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 500 Trn Loss 0.5661 Val Loss 0.6217 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 600 Trn Loss 0.5785 Val Loss 0.5594 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 700 Trn Loss 0.5757 Val Loss 0.5707 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 800 Trn Loss 0.5659 Val Loss 0.5795 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 900 Trn Loss 0.5961 Val Loss 0.6033 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1000 Trn Loss 0.5424 Val Loss 0.6055 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1100 Trn Loss 0.5434 Val Loss 0.5973 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1200 Trn Loss 0.5463 Val Loss 0.5961 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1300 Trn Loss 0.5633 Val Loss 0.6307 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1400 Trn Loss 0.5467 Val Loss 0.5820 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1500 Trn Loss 0.5435 Val Loss 0.6268 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1600 Trn Loss 0.5752 Val Loss 0.5897 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1700 Trn Loss 0.5327 Val Loss 0.5612 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1800 Trn Loss 0.5518 Val Loss 0.5893 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1900 Trn Loss 0.5823 Val Loss 0.5825 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 2000 Trn Loss 0.5587 Val Loss 0.6463 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 2100 Trn Loss 0.5276 Val Loss 0.5963 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 2200 Trn Loss 0.5876 Val Loss 0.5959 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 2300 Trn Loss 0.5591 Val Loss 0.5964 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 2400 Trn Loss 0.5407 Val Loss 0.5602 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Trn Loss 0.1396\n",
      "Epoch 12 Val Loss 0.1496\n",
      "Time taken for 1 epoch 330.0772953033447 sec\n",
      "\n",
      "Epoch 13 Batch 0 Trn Loss 0.5649 Val Loss 0.6186 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 100 Trn Loss 0.5386 Val Loss 0.5714 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 200 Trn Loss 0.5634 Val Loss 0.5691 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 300 Trn Loss 0.5454 Val Loss 0.5894 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 400 Trn Loss 0.5549 Val Loss 0.6276 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 500 Trn Loss 0.5711 Val Loss 0.6251 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 600 Trn Loss 0.5888 Val Loss 0.5631 Trn_Acc 0.83 Val_Acc 0.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Batch 700 Trn Loss 0.5818 Val Loss 0.5841 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 800 Trn Loss 0.5627 Val Loss 0.5793 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 900 Trn Loss 0.5909 Val Loss 0.5959 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 1000 Trn Loss 0.5365 Val Loss 0.6041 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 1100 Trn Loss 0.5435 Val Loss 0.5952 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 1200 Trn Loss 0.5440 Val Loss 0.6084 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 1300 Trn Loss 0.5591 Val Loss 0.6286 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 1400 Trn Loss 0.5502 Val Loss 0.5722 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 1500 Trn Loss 0.5481 Val Loss 0.6131 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 1600 Trn Loss 0.5696 Val Loss 0.5867 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 1700 Trn Loss 0.5269 Val Loss 0.5804 Trn_Acc 0.83 Val_Acc 0.78\n",
      "Epoch 13 Batch 1800 Trn Loss 0.5606 Val Loss 0.5894 Trn_Acc 0.83 Val_Acc 0.78\n",
      "Epoch 13 Batch 1900 Trn Loss 0.5817 Val Loss 0.5764 Trn_Acc 0.83 Val_Acc 0.78\n",
      "Epoch 13 Batch 2000 Trn Loss 0.5506 Val Loss 0.6531 Trn_Acc 0.83 Val_Acc 0.78\n",
      "Epoch 13 Batch 2100 Trn Loss 0.5349 Val Loss 0.6121 Trn_Acc 0.83 Val_Acc 0.78\n",
      "Epoch 13 Batch 2200 Trn Loss 0.5891 Val Loss 0.5908 Trn_Acc 0.83 Val_Acc 0.78\n",
      "Epoch 13 Batch 2300 Trn Loss 0.5560 Val Loss 0.6076 Trn_Acc 0.83 Val_Acc 0.78\n",
      "Epoch 13 Batch 2400 Trn Loss 0.5434 Val Loss 0.5739 Trn_Acc 0.83 Val_Acc 0.78\n",
      "Epoch 13 Trn Loss 0.1393\n",
      "Epoch 13 Val Loss 0.1496\n",
      "Time taken for 1 epoch 327.5292999744415 sec\n",
      "\n",
      "Epoch 14 Batch 0 Trn Loss 0.5608 Val Loss 0.6126 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 100 Trn Loss 0.5450 Val Loss 0.5717 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 200 Trn Loss 0.5631 Val Loss 0.5600 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 300 Trn Loss 0.5366 Val Loss 0.5933 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 400 Trn Loss 0.5656 Val Loss 0.6304 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 500 Trn Loss 0.5703 Val Loss 0.6253 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 600 Trn Loss 0.5805 Val Loss 0.5540 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 700 Trn Loss 0.5816 Val Loss 0.5794 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 800 Trn Loss 0.5634 Val Loss 0.5993 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 900 Trn Loss 0.5885 Val Loss 0.5990 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1000 Trn Loss 0.5322 Val Loss 0.5954 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1100 Trn Loss 0.5426 Val Loss 0.6049 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1200 Trn Loss 0.5517 Val Loss 0.6026 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1300 Trn Loss 0.5594 Val Loss 0.6262 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1400 Trn Loss 0.5452 Val Loss 0.5788 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1500 Trn Loss 0.5434 Val Loss 0.6197 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1600 Trn Loss 0.5719 Val Loss 0.5876 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1700 Trn Loss 0.5288 Val Loss 0.5743 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1800 Trn Loss 0.5515 Val Loss 0.5811 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1900 Trn Loss 0.5794 Val Loss 0.5789 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 2000 Trn Loss 0.5579 Val Loss 0.6395 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 2100 Trn Loss 0.5263 Val Loss 0.6060 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 2200 Trn Loss 0.5857 Val Loss 0.5992 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 2300 Trn Loss 0.5586 Val Loss 0.6068 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 2400 Trn Loss 0.5428 Val Loss 0.5677 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Trn Loss 0.1391\n",
      "Epoch 14 Val Loss 0.1496\n",
      "Time taken for 1 epoch 330.06081080436707 sec\n",
      "\n",
      "Epoch 15 Batch 0 Trn Loss 0.5608 Val Loss 0.6202 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 100 Trn Loss 0.5451 Val Loss 0.5792 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 200 Trn Loss 0.5566 Val Loss 0.5697 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 300 Trn Loss 0.5262 Val Loss 0.5785 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 400 Trn Loss 0.5651 Val Loss 0.6248 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 500 Trn Loss 0.5751 Val Loss 0.6288 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 600 Trn Loss 0.5814 Val Loss 0.5640 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 700 Trn Loss 0.5816 Val Loss 0.5783 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 800 Trn Loss 0.5616 Val Loss 0.5892 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 900 Trn Loss 0.5938 Val Loss 0.5985 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1000 Trn Loss 0.5350 Val Loss 0.5954 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1100 Trn Loss 0.5351 Val Loss 0.5987 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1200 Trn Loss 0.5356 Val Loss 0.5968 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1300 Trn Loss 0.5640 Val Loss 0.6267 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1400 Trn Loss 0.5554 Val Loss 0.5797 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1500 Trn Loss 0.5421 Val Loss 0.6249 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1600 Trn Loss 0.5737 Val Loss 0.5876 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1700 Trn Loss 0.5269 Val Loss 0.5762 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1800 Trn Loss 0.5602 Val Loss 0.5865 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1900 Trn Loss 0.5833 Val Loss 0.5846 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 2000 Trn Loss 0.5601 Val Loss 0.6442 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 2100 Trn Loss 0.5277 Val Loss 0.6085 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 2200 Trn Loss 0.5887 Val Loss 0.5837 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 2300 Trn Loss 0.5600 Val Loss 0.6030 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 2400 Trn Loss 0.5405 Val Loss 0.5634 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Trn Loss 0.1388\n",
      "Epoch 15 Val Loss 0.1497\n",
      "Time taken for 1 epoch 327.3540954589844 sec\n",
      "\n",
      "Epoch 16 Batch 0 Trn Loss 0.5583 Val Loss 0.6198 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 100 Trn Loss 0.5412 Val Loss 0.5791 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 200 Trn Loss 0.5655 Val Loss 0.5601 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 300 Trn Loss 0.5276 Val Loss 0.5886 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 400 Trn Loss 0.5589 Val Loss 0.6234 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 500 Trn Loss 0.5702 Val Loss 0.6255 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 600 Trn Loss 0.5787 Val Loss 0.5589 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 700 Trn Loss 0.5803 Val Loss 0.5758 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 800 Trn Loss 0.5681 Val Loss 0.5842 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 900 Trn Loss 0.5870 Val Loss 0.6016 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1000 Trn Loss 0.5321 Val Loss 0.6038 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1100 Trn Loss 0.5362 Val Loss 0.6039 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1200 Trn Loss 0.5393 Val Loss 0.6063 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1300 Trn Loss 0.5583 Val Loss 0.6294 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1400 Trn Loss 0.5493 Val Loss 0.5767 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1500 Trn Loss 0.5421 Val Loss 0.6241 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1600 Trn Loss 0.5670 Val Loss 0.5934 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1700 Trn Loss 0.5238 Val Loss 0.5791 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1800 Trn Loss 0.5626 Val Loss 0.5839 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1900 Trn Loss 0.5806 Val Loss 0.5797 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 2000 Trn Loss 0.5553 Val Loss 0.6445 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 2100 Trn Loss 0.5239 Val Loss 0.6101 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 2200 Trn Loss 0.5863 Val Loss 0.5937 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 2300 Trn Loss 0.5603 Val Loss 0.6046 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 2400 Trn Loss 0.5497 Val Loss 0.5757 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Trn Loss 0.1386\n",
      "Epoch 16 Val Loss 0.1497\n",
      "Time taken for 1 epoch 330.13904190063477 sec\n",
      "\n",
      "Epoch 17 Batch 0 Trn Loss 0.5570 Val Loss 0.6262 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 100 Trn Loss 0.5409 Val Loss 0.5910 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 200 Trn Loss 0.5539 Val Loss 0.5604 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 300 Trn Loss 0.5313 Val Loss 0.5863 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 400 Trn Loss 0.5602 Val Loss 0.6278 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 500 Trn Loss 0.5621 Val Loss 0.6138 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 600 Trn Loss 0.5764 Val Loss 0.5623 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 700 Trn Loss 0.5767 Val Loss 0.5760 Trn_Acc 0.84 Val_Acc 0.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Batch 800 Trn Loss 0.5652 Val Loss 0.5943 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 900 Trn Loss 0.5893 Val Loss 0.5958 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 1000 Trn Loss 0.5320 Val Loss 0.6106 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 1100 Trn Loss 0.5441 Val Loss 0.5976 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 1200 Trn Loss 0.5358 Val Loss 0.6141 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 1300 Trn Loss 0.5657 Val Loss 0.6224 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 1400 Trn Loss 0.5520 Val Loss 0.5773 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 1500 Trn Loss 0.5401 Val Loss 0.6226 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 17 Batch 1600 Trn Loss 0.5639 Val Loss 0.5803 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 17 Batch 1700 Trn Loss 0.5234 Val Loss 0.5688 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 17 Batch 1800 Trn Loss 0.5659 Val Loss 0.5878 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 17 Batch 1900 Trn Loss 0.5762 Val Loss 0.5799 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 17 Batch 2000 Trn Loss 0.5609 Val Loss 0.6416 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 17 Batch 2100 Trn Loss 0.5236 Val Loss 0.6048 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 17 Batch 2200 Trn Loss 0.5817 Val Loss 0.5932 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 17 Batch 2300 Trn Loss 0.5680 Val Loss 0.5957 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 17 Batch 2400 Trn Loss 0.5435 Val Loss 0.5693 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 17 Trn Loss 0.1383\n",
      "Epoch 17 Val Loss 0.1497\n",
      "Time taken for 1 epoch 328.0861403942108 sec\n",
      "\n",
      "Epoch 18 Batch 0 Trn Loss 0.5600 Val Loss 0.6369 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 100 Trn Loss 0.5350 Val Loss 0.5986 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 200 Trn Loss 0.5566 Val Loss 0.5637 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 300 Trn Loss 0.5283 Val Loss 0.5803 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 400 Trn Loss 0.5581 Val Loss 0.6179 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 500 Trn Loss 0.5629 Val Loss 0.6190 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 600 Trn Loss 0.5781 Val Loss 0.5584 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 700 Trn Loss 0.5806 Val Loss 0.5784 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 800 Trn Loss 0.5575 Val Loss 0.5853 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 900 Trn Loss 0.5835 Val Loss 0.5927 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1000 Trn Loss 0.5334 Val Loss 0.5984 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1100 Trn Loss 0.5350 Val Loss 0.6015 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1200 Trn Loss 0.5342 Val Loss 0.6165 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1300 Trn Loss 0.5529 Val Loss 0.6324 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1400 Trn Loss 0.5414 Val Loss 0.5806 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1500 Trn Loss 0.5387 Val Loss 0.6335 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1600 Trn Loss 0.5676 Val Loss 0.5879 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1700 Trn Loss 0.5242 Val Loss 0.5788 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1800 Trn Loss 0.5530 Val Loss 0.5929 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1900 Trn Loss 0.5814 Val Loss 0.5784 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 2000 Trn Loss 0.5515 Val Loss 0.6418 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 2100 Trn Loss 0.5234 Val Loss 0.6046 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 2200 Trn Loss 0.5800 Val Loss 0.5929 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 2300 Trn Loss 0.5621 Val Loss 0.5983 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 2400 Trn Loss 0.5399 Val Loss 0.5652 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Trn Loss 0.1382\n",
      "Epoch 18 Val Loss 0.1498\n",
      "Time taken for 1 epoch 330.562246799469 sec\n",
      "\n",
      "Epoch 19 Batch 0 Trn Loss 0.5582 Val Loss 0.6274 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 100 Trn Loss 0.5452 Val Loss 0.5894 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 200 Trn Loss 0.5523 Val Loss 0.5637 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 300 Trn Loss 0.5305 Val Loss 0.5792 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 400 Trn Loss 0.5606 Val Loss 0.6157 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 500 Trn Loss 0.5529 Val Loss 0.6258 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 600 Trn Loss 0.5801 Val Loss 0.5654 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 700 Trn Loss 0.5762 Val Loss 0.5767 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 800 Trn Loss 0.5638 Val Loss 0.5878 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 900 Trn Loss 0.5843 Val Loss 0.5969 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1000 Trn Loss 0.5390 Val Loss 0.6125 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1100 Trn Loss 0.5390 Val Loss 0.5984 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1200 Trn Loss 0.5335 Val Loss 0.5993 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1300 Trn Loss 0.5563 Val Loss 0.6314 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1400 Trn Loss 0.5409 Val Loss 0.5780 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1500 Trn Loss 0.5358 Val Loss 0.6177 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1600 Trn Loss 0.5687 Val Loss 0.5883 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1700 Trn Loss 0.5238 Val Loss 0.5807 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1800 Trn Loss 0.5609 Val Loss 0.5854 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1900 Trn Loss 0.5822 Val Loss 0.5842 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 2000 Trn Loss 0.5504 Val Loss 0.6659 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 2100 Trn Loss 0.5243 Val Loss 0.6012 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 2200 Trn Loss 0.5865 Val Loss 0.5902 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 2300 Trn Loss 0.5595 Val Loss 0.6142 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 2400 Trn Loss 0.5418 Val Loss 0.5696 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Trn Loss 0.1380\n",
      "Epoch 19 Val Loss 0.1498\n",
      "Time taken for 1 epoch 328.251718044281 sec\n",
      "\n",
      "Epoch 20 Batch 0 Trn Loss 0.5548 Val Loss 0.6353 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 100 Trn Loss 0.5360 Val Loss 0.5972 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 200 Trn Loss 0.5532 Val Loss 0.5612 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 300 Trn Loss 0.5263 Val Loss 0.5754 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 400 Trn Loss 0.5522 Val Loss 0.6264 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 500 Trn Loss 0.5539 Val Loss 0.6337 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 600 Trn Loss 0.5749 Val Loss 0.5646 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 700 Trn Loss 0.5757 Val Loss 0.5768 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 800 Trn Loss 0.5621 Val Loss 0.5809 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 900 Trn Loss 0.5839 Val Loss 0.6000 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1000 Trn Loss 0.5318 Val Loss 0.6117 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1100 Trn Loss 0.5406 Val Loss 0.5907 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1200 Trn Loss 0.5270 Val Loss 0.6071 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1300 Trn Loss 0.5539 Val Loss 0.6255 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1400 Trn Loss 0.5411 Val Loss 0.5824 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1500 Trn Loss 0.5356 Val Loss 0.6267 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1600 Trn Loss 0.5636 Val Loss 0.5928 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1700 Trn Loss 0.5298 Val Loss 0.5801 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1800 Trn Loss 0.5534 Val Loss 0.5860 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1900 Trn Loss 0.5721 Val Loss 0.5864 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 2000 Trn Loss 0.5477 Val Loss 0.6510 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 2100 Trn Loss 0.5210 Val Loss 0.6061 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 2200 Trn Loss 0.5825 Val Loss 0.5881 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 2300 Trn Loss 0.5554 Val Loss 0.5938 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 2400 Trn Loss 0.5388 Val Loss 0.5716 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Trn Loss 0.1378\n",
      "Epoch 20 Val Loss 0.1498\n",
      "Time taken for 1 epoch 330.5339524745941 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "trn_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "steps_per_epoch = len(x_train)//BATCH_SIZE\n",
    "\n",
    "EPOCHS = 20\n",
    "trn_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    val_total_loss = 0\n",
    "    \n",
    "    hidden = model.reset_states()\n",
    "    for batch, ((trn_inp, trn_targ), (val_inp, val_targ)) in enumerate(zip(train_dataset.take(steps_per_epoch), test_dataset.take(steps_per_epoch))):\n",
    "        batch_loss = train_step(trn_inp, trn_targ)\n",
    "        total_loss += batch_loss\n",
    "        trn_loss_list.append(batch_loss)\n",
    "\n",
    "        val_loss = val_step(val_inp, val_targ)\n",
    "        val_total_loss += val_loss\n",
    "        val_loss_list.append(val_loss)\n",
    "\n",
    "        if batch%100==0:\n",
    "              print('Epoch {} Batch {} Trn Loss {:.4f} Val Loss {:.4f} Trn_Acc {:.2f} Val_Acc {:.2f}'.format(epoch + 1,\n",
    "                                                                                                             batch,\n",
    "                                                                                                             batch_loss.numpy(),\n",
    "                                                                                                             val_loss.numpy(),\n",
    "                                                                                                             trn_acc_metric.result().numpy(),\n",
    "                                                                                                             val_acc_metric.result().numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Trn Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Epoch {} Val Loss {:.4f}'.format(epoch + 1,\n",
    "                                      val_total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(trn_loss_list)\n",
    "plt.plot(val_loss_list)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('pre-deploy_models/sen140Eager.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "\n",
    "\n",
    "with open('tokenizer_sen140.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = 64\n",
    "    \n",
    "test_sentence = 'Sapura Energy bags 5 new contracts worth RM1.3 billion - Free Malaysia Today'\n",
    "test_sentence = tokenizer.texts_to_sequences(test_sentence)\n",
    "test_sentence = sequence.pad_sequences(test_sentence, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0412 21:40:47.196293 140575884826432 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7fd9df2d29e8>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
      "W0412 21:40:47.198403 140575884826432 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7fd9c8ae81d0>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate, Flatten, Embedding\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, Input, LSTM\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM, CuDNNGRU\n",
    "\n",
    "EMBEDDING_DIM = 500\n",
    "LSTM_UNITS = 256\n",
    "\n",
    "class MyLSTM(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length)\n",
    "        self.dropout1 = SpatialDropout1D(0.3)\n",
    "        self.lstm1 = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))\n",
    "        self.gmp = GlobalMaxPooling1D()\n",
    "        self.dense1 = Dense(100, activation='tanh')\n",
    "        self.dropout2 = Dropout(0.2)\n",
    "        self.denseOut = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.gmp(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.denseOut(x)\n",
    "        return x\n",
    "    \n",
    "model = MyLSTM()\n",
    "\n",
    "model.load_weights('pre-deploy_models/sen140Eager.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = ['Sapura Energy bags 5 new contracts worth RM1.3 billion - Free Malaysia Today']\n",
    "test_sentence = tokenizer.texts_to_sequences(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1444, 2641, 243, 77, 10472, 749, 93553, 104, 6617, 361, 5649, 40]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = sequence.pad_sequences(test_sentence, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1444,  2641,   243,    77, 10472,   749, 93553,   104,  6617,\n",
       "          361,  5649,    40,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0]], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5121009]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_sentence).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
