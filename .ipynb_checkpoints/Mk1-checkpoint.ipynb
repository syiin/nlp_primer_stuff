{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILES = [\n",
    "    'embeddings/crawl-300d-2M.vec',\n",
    "    'embeddings/glove.840B.300d.txt'\n",
    "]\n",
    "\n",
    "NUM_MODELS = 2\n",
    "BATCH_SIZE = 128\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 4\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functiosn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
    "\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embedding_matrix, model_name):\n",
    "    words = Input(shape=(max_length,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='tanh')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(11, activation='softmax')(hidden)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=result)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    model.name = model_name\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_directory_data(directory):\n",
    "    data={}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    for file_path in os.listdir(directory):\n",
    "        with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(directory + \"/pos\")\n",
    "    neg_df = load_directory_data(directory + \"/neg\")\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_dataset(\"data/aclImdb/train/\")\n",
    "test_df = load_dataset(\"data/aclImdb/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df[\"sentence\"]\n",
    "y_train = train_df[\"sentiment\"]\n",
    "\n",
    "x_test = test_df[\"sentence\"]\n",
    "y_test = test_df[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(train_df[\"sentiment\"])\n",
    "y_test = to_categorical(test_df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'1', '10', '2', '3', '4', '7', '8', '9'},\n",
       " '9',\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], dtype=float32))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_df['sentiment'].values), train_df[\"sentiment\"][1], y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('You spend most of this two-hour film wondering \"what\\'s the story regarding the lead character?\" <br /><br />Will Smith, as a low-key \"Ben Thomas\" will keep you guessing. The last 20-25 minutes is when you find out, and it\\'s a shocker....but you knew something dramatic was going to be revealed. Until then, Smith, plays it mysterious, almost stalking people. You know he has a good reason for doing it, but it\\'s never really explained, once again, to keep us guessing until the end.<br /><br />All of it, including a on again/off again but touching romance with Rosario Dawkins (\"Emily Posa\") might make some viewers frustrated or wanting to quit this film.....but don\\'t because the final long segment puts all the pieces of this puzzle together.<br /><br />This is a two-hour film and not the typical action-packed macho Will Smith film. In fact, the most shocking aspect might be seeing the drawn, sad face of Smith throughout this story. It almost doesn\\'t even look like him in a number of shots. He looks like he\\'s lost weight and is sick. Smith does a great job portraying a man carrying around a lot of sadness.<br /><br />Like a good movie will often do, this film will leave you thinking long after the ending credits.',\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], dtype=float32))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1], y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_obj = text.Tokenizer()\n",
    "total_reviews = x_train + x_test\n",
    "tokenizer_obj.fit_on_texts(total_reviews)\n",
    "\n",
    "max_length = max([len(s.split()) for s in total_reviews])\n",
    "vocab_size = len(tokenizer_obj.word_index) + 1\n",
    "\n",
    "X_train_tokens = tokenizer_obj.texts_to_sequences(x_train)\n",
    "X_test_tokens = tokenizer_obj.texts_to_sequences(x_test)\n",
    "\n",
    "X_train_pad = sequence.pad_sequences(X_train_tokens, maxlen=max_length, padding=\"post\")\n",
    "X_test_pad = sequence.pad_sequences(X_test_tokens, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix = np.concatenate(\n",
    "#     [build_matrix(tokenizer_obj.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n",
    "\n",
    "# np.savetxt('embedding_concat.txt', embedding_matrix , fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.loadtxt('embedding_concat.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
    "LOG_DIR = '/media/eigenstir/1TBSecondary/tbgraphs'\n",
    "\n",
    "tbCallBack = TensorBoard(log_dir=LOG_DIR, histogram_freq=0, write_graph=True, write_images=True)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "mc = ModelCheckpoint('best_model_mk1.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "ls_sched = LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      " - 203s - loss: 2.0389 - val_loss: 2.0202\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eigenstir/.local/lib/python3.6/site-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "weights = []\n",
    "for model_idx in range(NUM_MODELS):\n",
    "    model = build_model(embedding_matrix, model_name = str(model_idx))\n",
    "    for global_epoch in range(EPOCHS):\n",
    "        model.fit(\n",
    "            X_train_pad,\n",
    "            y_train,\n",
    "            validation_data=(X_test_pad, y_test),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=3,\n",
    "            verbose=2,\n",
    "            callbacks=[tbCallBack, es, ModelCheckpoint('best_model_mk1' + str(model_idx) + '.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)]\n",
    "        )\n",
    "        weights.append(2 ** global_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(embedding_matrix, 'test')\n",
    "model.load_weights('best_model_mk11.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('When at the very start of the film Paleontologist Donald Sutherland arrives at the Argyle family\\'s house and it comes out he is the undeniable alibi for one of the members executed for murdering his mother two years ago your sensation is that you are about to watch a top thriller; an innocent man has been convicted and a killer is still around. But as the film runs along your disappointment increases inevitably.<br /><br />\"Ordeal by Innocence\" is a dull and at times even boring film that doesn\\'t raise at any moment. Nothing interesting happens all along and even the final revealing of the facts lacks surprise and intensity (wether you guessed or not).<br /><br />Donald Sutherland, Cristopher Plummer, Faye Dunaway and Sarah Miles (far from her good performance in \"Ryan\\'s Daughter\") just pass through their roles and not very enthusiastically either.<br /><br />You won\\'t miss much if you skip this one.',\n",
       " '4')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 5\n",
    "test_text = test_df[\"sentence\"][idx]\n",
    "test_df[\"sentence\"][idx], test_df[\"sentiment\"][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = tokenizer_obj.texts_to_sequences(test_text)\n",
    "test_text = sequence.pad_sequences(test_text, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.8707360e-05, 2.6086321e-01, 1.4081886e-02, 2.1433288e-02,\n",
       "       5.5947327e-03, 4.3497581e-05, 6.8869776e-05, 6.2136460e-02,\n",
       "       5.8546316e-02, 1.5213890e-02, 6.3218963e-01], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = np.average(prediction, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(prediction.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing texts: Uber\n",
    "\n",
    "Neg:\n",
    "\n",
    "1. #uber's customer service department is worst than comcast.\n",
    "1. #Uber ...get your act right! # frustrated#waiting for competition\n",
    "1. .@Uber charges more if they think you're willing to pay more. Uber thinks you're willing to pay more when your battery is about to die. And Uber knows when your battery is about to die.\n",
    "\n",
    "Pos:\n",
    "1. Ride-hailing industry expected to grow eightfold to $285 billion by 2030\n",
    "1. Uber is awesome!\n",
    "1. Best service ever with Uber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "imdb = tf.keras.datasets.imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Times Singapore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "date_sentiments = {}\n",
    "\n",
    "for i in range(1,11):\n",
    "    page = urlopen('https://www.businesstimes.com.sg/search/facebook?page='+str(i)).read()\n",
    "    soup = BeautifulSoup(page, features=\"html.parser\")\n",
    "    posts = soup.findAll(\"div\", {\"class\": \"media-body\"})\n",
    "    for post in posts:\n",
    "        time.sleep(1)\n",
    "        url = post.a['href']\n",
    "        date = post.time.text\n",
    "        print(date, url)\n",
    "        try:\n",
    "            link_page = urlopen(url).read()\n",
    "        except:\n",
    "            url = url[:-2]\n",
    "            link_page = urlopen(url).read()\n",
    "        link_soup = BeautifulSoup(link_page)\n",
    "        sentences = link_soup.findAll(\"p\")\n",
    "        passage = \"\"\n",
    "        for sentence in sentences:\n",
    "            passage += sentence.text\n",
    "        sentiment = sia.polarity_scores(passage)['compound']\n",
    "        date_sentiments.setdefault(date, []).append(sentiment)\n",
    "\n",
    "date_sentiment = {}\n",
    "\n",
    "for k,v in date_sentiments.items():\n",
    "    date_sentiment[datetime.strptime(k, '%d %b %Y').date() + timedelta(days=1)] = round(sum(v)/float(len(v)),3)\n",
    "\n",
    "earliest_date = min(date_sentiment.keys())\n",
    "\n",
    "print(date_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtrader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backtrader as bt\n",
    "import backtrader.indicators as btind\n",
    "import datetime\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "class Sentiment(bt.Indicator):\n",
    "    lines = ('sentiment',)\n",
    "    plotinfo = dict(\n",
    "        plotymargin=0.15,\n",
    "        plothlines=[0],\n",
    "        plotyticks=[1.0, 0, -1.0])\n",
    "    \n",
    "    def next(self):\n",
    "        self.date = self.data.datetime\n",
    "        date = bt.num2date(self.date[0]).date()\n",
    "        prev_sentiment = self.sentiment\n",
    "        if date in date_sentiment:\n",
    "            self.sentiment = date_sentiment[date]\n",
    "        self.lines.sentiment[0] = self.sentiment\n",
    "\n",
    "\n",
    "class SentimentStrat(bt.Strategy):\n",
    "    params = (\n",
    "        ('period', 15),\n",
    "        ('printlog', True),\n",
    "    )\n",
    "\n",
    "    def log(self, txt, dt=None, doprint=False):\n",
    "        ''' Logging function for this strategy'''\n",
    "        if self.params.printlog or doprint:\n",
    "            dt = dt or self.datas[0].datetime.date(0)\n",
    "            print('%s, %s' % (dt.isoformat(), txt))\n",
    "\n",
    "    def __init__(self):\n",
    "        # Keep a reference to the \"close\" line in the data[0] dataseries\n",
    "        self.dataclose = self.datas[0].close\n",
    "        # Keep track of pending orders\n",
    "        self.order = None\n",
    "        self.buyprice = None\n",
    "        self.buycomm = None\n",
    "        self.sma = bt.indicators.SimpleMovingAverage(\n",
    "            self.datas[0], period=self.params.period)\n",
    "        self.date = self.data.datetime\n",
    "        self.sentiment = None\n",
    "        Sentiment(self.data)\n",
    "        \n",
    "    def notify_order(self, order):\n",
    "        if order.status in [order.Submitted, order.Accepted]:\n",
    "            # Buy/Sell order submitted/accepted to/by broker - Nothing to do\n",
    "            return\n",
    "        \n",
    "        # Check if an order has been completed\n",
    "        # Attention: broker could reject order if not enough cash\n",
    "        if order.status in [order.Completed]:\n",
    "            if order.isbuy():\n",
    "                self.log(\n",
    "                    'BUY EXECUTED, Price: %.2f, Cost: %.2f, Comm %.2f' %\n",
    "                    (order.executed.price,\n",
    "                     order.executed.value,\n",
    "                     order.executed.comm))\n",
    "                self.buyprice = order.executed.price\n",
    "                self.buycomm = order.executed.comm\n",
    "            else:  # Sell\n",
    "                self.log('SELL EXECUTED, Price: %.2f, Cost: %.2f, Comm %.2f' %\n",
    "                         (order.executed.price,\n",
    "                          order.executed.value,\n",
    "                          order.executed.comm))\n",
    "                \n",
    "            self.bar_executed = len(self)     \n",
    "            \n",
    "        elif order.status in [order.Canceled, order.Margin, order.Rejected]:\n",
    "            self.log('Order Canceled/Margin/Rejected')\n",
    "            \n",
    "        # Write down: no pending order\n",
    "        self.order = None\n",
    "        \n",
    "    def notify_trade(self, trade):\n",
    "        if not trade.isclosed:\n",
    "            return\n",
    "\n",
    "        self.log('OPERATION PROFIT, GROSS %.2f, NET %.2f' %\n",
    "                 (trade.pnl, trade.pnlcomm))\n",
    "    \n",
    "    ### Main Strat ###\n",
    "    def next(self):\n",
    "        # log closing price of the series from the reference\n",
    "        self.log('Close, %.2f' % self.dataclose[0])\n",
    "        \n",
    "        date = bt.num2date(self.date[0]).date()\n",
    "        prev_sentiment = self.sentiment\n",
    "        if date in date_sentiment:\n",
    "            self.sentiment = date_sentiment[date]\n",
    "        \n",
    "        # Check if an order is pending. if yes, we cannot send a 2nd one\n",
    "        if self.order:\n",
    "            return\n",
    "        print(self.sentiment)\n",
    "        # If not in the market and previous sentiment not none\n",
    "        if not self.position and prev_sentiment:\n",
    "            # buy if current close more than sma AND sentiment increased by >= 0.5\n",
    "            if self.dataclose[0] > self.sma[0] and self.sentiment - prev_sentiment >= 0.5:\n",
    "                self.log('BUY CREATE, %.2f' % self.dataclose[0])\n",
    "                self.order = self.buy()\n",
    "                \n",
    "        # Already in the market and previous sentiment not none\n",
    "        elif prev_sentiment:\n",
    "            # sell if current close less than sma AND sentiment decreased by >= 0.5\n",
    "            if self.dataclose[0] < self.sma[0] and self.sentiment - prev_sentiment <= -0.5:\n",
    "                self.log('SELL CREATE, %.2f' % self.dataclose[0])\n",
    "                self.order = self.sell()\n",
    "\n",
    "    def stop(self):\n",
    "        self.log('(MA Period %2d) Ending Value %.2f' %\n",
    "                 (self.params.period, self.broker.getvalue()), doprint=True)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cerebro = bt.Cerebro()\n",
    "    \n",
    "    # Strategy\n",
    "    cerebro.addstrategy(SentimentStrat)\n",
    "\n",
    "    # Data Feed\n",
    "    data = bt.feeds.YahooFinanceData(\n",
    "        dataname = 'FB',\n",
    "        fromdate = earliest_date,\n",
    "        todate = datetime.datetime(2018,11,25),\n",
    "        reverse = False\n",
    "    )\n",
    "    \n",
    "    cerebro.adddata(data)\n",
    "\n",
    "    cerebro.broker.setcash(100000.0)\n",
    "    cerebro.addsizer(bt.sizers.FixedSize, stake=10)\n",
    "    cerebro.broker.setcommission(commission=0.001)\n",
    "    print('Starting Portfolio Value: %.2f' % cerebro.broker.getvalue())\n",
    "    cerebro.run()\n",
    "    print('Final Portfolio Value: %.2f' % cerebro.broker.getvalue())\n",
    "    \n",
    "    cerebro.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "# from keras import backend as K\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import graph_io\n",
    "from tensorflow.python.saved_model import signature_constants\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.set_learning_phase(0)\n",
    "\n",
    "model = build_model(embedding_matrix, model_name = str(model_idx))\n",
    "config = model.get_config()\n",
    "weights = model.get_weights()\n",
    "new_model = Model.from_config(config)\n",
    "new_model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model import utils\n",
    "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
    "from tensorflow.python.saved_model.signature_def_utils_impl import build_signature_def, predict_signature_def\n",
    "from tensorflow.contrib.session_bundle import exporter\n",
    "\n",
    "export_path = 'exported_model'\n",
    "builder = saved_model_builder.SavedModelBuilder(export_path)\n",
    "\n",
    "signature = predict_signature_def(inputs={'text': new_model.input},\n",
    "                                  outputs={'sentiment': new_model.output})\n",
    "\n",
    "with keras.get_session() as sess:\n",
    "    builder.add_meta_graph_and_variables(sess=sess,\n",
    "                                         tags=[tag_constants.SERVING],\n",
    "                                         signature_def_map={'predict': signature})\n",
    "    builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another type of loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-15-c5479d416f9e>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-c5479d416f9e>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.loadtxt('embedding_concat.txt', dtype=int)\n",
    "words = Input(shape=(max_length,))\n",
    "x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "x = SpatialDropout1D(0.3)(x)\n",
    "x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "result = Dense(11, activation='sigmoid')(hidden)\n",
    "    \n",
    "model = Model(inputs=words, outputs=result)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=0, write_graph=True, write_images=True)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "ls_sched = LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_pad, y_train, batch_size=64, epochs=10, \n",
    "          validation_data=(X_test_pad, y_test), callbacks=[tbCallBack, es, mc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
