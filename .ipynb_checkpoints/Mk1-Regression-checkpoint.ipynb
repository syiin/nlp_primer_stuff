{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILES = [\n",
    "    'embeddings/crawl-300d-2M.vec',\n",
    "    'embeddings/glove.840B.300d.txt'\n",
    "]\n",
    "\n",
    "NUM_MODELS = 2\n",
    "BATCH_SIZE = 128\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 4\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functiosn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
    "\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embedding_matrix, model_name):\n",
    "    words = Input(shape=(max_length,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(11, activation='sigmoid')(hidden)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=result)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    model.name = model_name\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_directory_data(directory):\n",
    "    data={}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    for file_path in os.listdir(directory):\n",
    "        with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(directory + \"/pos\")\n",
    "    neg_df = load_directory_data(directory + \"/neg\")\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_dataset(\"data/aclImdb/train/\")\n",
    "test_df = load_dataset(\"data/aclImdb/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df[\"sentence\"]\n",
    "y_train = train_df[\"sentiment\"]\n",
    "\n",
    "x_test = test_df[\"sentence\"]\n",
    "y_test = test_df[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(train_df[\"sentiment\"])\n",
    "y_test = to_categorical(test_df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'1', '10', '2', '3', '4', '7', '8', '9'},\n",
       " '7',\n",
       " array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_df['sentiment'].values), train_df[\"sentiment\"][1], y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Though it had the misfortune to hit the festival circuit here in Austin (SXSW Film) just as we were getting tired of things like Shakespeare in Love, and Elizabeth, this movie deserves an audience. An inside look at the staging of \"The Scottish Play\" as actors call \"Macbeth\" when producing it to avoid the curse, this is a crisp, efficient and stylish treatment of the treachery which befalls the troupe. With a wonderfully evocative score, and looking and sounding far better than its small budget would suggest, this is a quiet gem, not world-class, but totally satisfying.',\n",
       " array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1], y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_obj = text.Tokenizer()\n",
    "total_reviews = x_train + x_test\n",
    "tokenizer_obj.fit_on_texts(total_reviews)\n",
    "\n",
    "max_length = max([len(s.split()) for s in total_reviews])\n",
    "vocab_size = len(tokenizer_obj.word_index) + 1\n",
    "\n",
    "X_train_tokens = tokenizer_obj.texts_to_sequences(x_train)\n",
    "X_test_tokens = tokenizer_obj.texts_to_sequences(x_test)\n",
    "\n",
    "X_train_pad = sequence.pad_sequences(X_train_tokens, maxlen=max_length, padding=\"post\")\n",
    "X_test_pad = sequence.pad_sequences(X_test_tokens, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix = np.concatenate(\n",
    "#     [build_matrix(tokenizer_obj.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n",
    "\n",
    "# np.savetxt('embedding_concat.txt', embedding_matrix , fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.loadtxt('embedding_concat.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
    "LOG_DIR = '/media/eigenstir/1TBSecondary/tbgraphs'\n",
    "\n",
    "tbCallBack = TensorBoard(log_dir=LOG_DIR, histogram_freq=0, write_graph=True, write_images=True)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "mc = ModelCheckpoint('best_model_mk1.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "ls_sched = LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      " - 196s - loss: 0.2764 - acc: 0.9080 - val_loss: 0.2676 - val_acc: 0.9091\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.90909, saving model to best_model_mk10.h5\n",
      "Epoch 2/3\n",
      " - 207s - loss: 0.2661 - acc: 0.9091 - val_loss: 0.2613 - val_acc: 0.9092\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.90909 to 0.90924, saving model to best_model_mk10.h5\n",
      "Epoch 3/3\n",
      " - 210s - loss: 0.2594 - acc: 0.9098 - val_loss: 0.2590 - val_acc: 0.9098\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.90924 to 0.90985, saving model to best_model_mk10.h5\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      " - 211s - loss: 0.2540 - acc: 0.9103 - val_loss: 0.2472 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91124, saving model to best_model_mk10.h5\n",
      "Epoch 2/3\n",
      " - 211s - loss: 0.2491 - acc: 0.9111 - val_loss: 0.2574 - val_acc: 0.9115\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.91124 to 0.91154, saving model to best_model_mk10.h5\n",
      "Epoch 00002: early stopping\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      " - 211s - loss: 0.2455 - acc: 0.9117 - val_loss: 0.2423 - val_acc: 0.9124\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91236, saving model to best_model_mk10.h5\n",
      "Epoch 2/3\n",
      " - 212s - loss: 0.2417 - acc: 0.9121 - val_loss: 0.2401 - val_acc: 0.9131\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.91236 to 0.91308, saving model to best_model_mk10.h5\n",
      "Epoch 3/3\n",
      " - 212s - loss: 0.2394 - acc: 0.9127 - val_loss: 0.2435 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.91308\n",
      "Epoch 00003: early stopping\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      " - 212s - loss: 0.2363 - acc: 0.9133 - val_loss: 0.2411 - val_acc: 0.9128\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91276, saving model to best_model_mk10.h5\n",
      "Epoch 2/3\n",
      " - 213s - loss: 0.2328 - acc: 0.9143 - val_loss: 0.2375 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.91276 to 0.91341, saving model to best_model_mk10.h5\n",
      "Epoch 3/3\n",
      " - 212s - loss: 0.2299 - acc: 0.9150 - val_loss: 0.2394 - val_acc: 0.9130\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.91341\n",
      "Epoch 00003: early stopping\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      " - 219s - loss: 0.2265 - acc: 0.9156 - val_loss: 0.2382 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91363, saving model to best_model_mk10.h5\n",
      "Epoch 2/3\n",
      " - 211s - loss: 0.2237 - acc: 0.9165 - val_loss: 0.2378 - val_acc: 0.9138\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.91363 to 0.91383, saving model to best_model_mk10.h5\n",
      "Epoch 3/3\n",
      " - 212s - loss: 0.2209 - acc: 0.9168 - val_loss: 0.2360 - val_acc: 0.9144\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.91383 to 0.91441, saving model to best_model_mk10.h5\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      " - 212s - loss: 0.2173 - acc: 0.9183 - val_loss: 0.2362 - val_acc: 0.9145\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91446, saving model to best_model_mk10.h5\n",
      "Epoch 2/3\n",
      " - 211s - loss: 0.2137 - acc: 0.9187 - val_loss: 0.2419 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.91446\n",
      "Epoch 00002: early stopping\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      " - 214s - loss: 0.2111 - acc: 0.9192 - val_loss: 0.2382 - val_acc: 0.9140\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91399, saving model to best_model_mk10.h5\n",
      "Epoch 2/3\n",
      " - 212s - loss: 0.2077 - acc: 0.9206 - val_loss: 0.2423 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.91399\n",
      "Epoch 00002: early stopping\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      " - 213s - loss: 0.2052 - acc: 0.9213 - val_loss: 0.2405 - val_acc: 0.9133\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91329, saving model to best_model_mk10.h5\n",
      "Epoch 2/3\n",
      " - 213s - loss: 0.2001 - acc: 0.9230 - val_loss: 0.2453 - val_acc: 0.9131\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.91329\n",
      "Epoch 00002: early stopping\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      " - 215s - loss: 0.2774 - acc: 0.9076 - val_loss: 0.2674 - val_acc: 0.9091\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.90909, saving model to best_model_mk11.h5\n",
      "Epoch 2/3\n",
      " - 216s - loss: 0.2666 - acc: 0.9091 - val_loss: 0.2626 - val_acc: 0.9091\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.90909\n",
      "Epoch 3/3\n",
      " - 213s - loss: 0.2601 - acc: 0.9098 - val_loss: 0.2537 - val_acc: 0.9103\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.90909 to 0.91035, saving model to best_model_mk11.h5\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      " - 213s - loss: 0.2547 - acc: 0.9102 - val_loss: 0.2539 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91089, saving model to best_model_mk11.h5\n",
      "Epoch 2/3\n",
      " - 215s - loss: 0.2491 - acc: 0.9110 - val_loss: 0.2451 - val_acc: 0.9117\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.91089 to 0.91170, saving model to best_model_mk11.h5\n",
      "Epoch 3/3\n",
      " - 209s - loss: 0.2455 - acc: 0.9114 - val_loss: 0.2455 - val_acc: 0.9115\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.91170\n",
      "Epoch 00003: early stopping\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      " - 211s - loss: 0.2422 - acc: 0.9120 - val_loss: 0.2395 - val_acc: 0.9127\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91272, saving model to best_model_mk11.h5\n",
      "Epoch 2/3\n",
      " - 212s - loss: 0.2388 - acc: 0.9125 - val_loss: 0.2392 - val_acc: 0.9128\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.91272 to 0.91281, saving model to best_model_mk11.h5\n",
      "Epoch 3/3\n",
      " - 212s - loss: 0.2368 - acc: 0.9129 - val_loss: 0.2427 - val_acc: 0.9123\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.91281\n",
      "Epoch 00003: early stopping\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      " - 212s - loss: 0.2336 - acc: 0.9138 - val_loss: 0.2382 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91345, saving model to best_model_mk11.h5\n",
      "Epoch 2/3\n",
      " - 211s - loss: 0.2303 - acc: 0.9143 - val_loss: 0.2376 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.91345 to 0.91358, saving model to best_model_mk11.h5\n",
      "Epoch 3/3\n",
      " - 209s - loss: 0.2284 - acc: 0.9149 - val_loss: 0.2366 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.91358 to 0.91372, saving model to best_model_mk11.h5\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      " - 204s - loss: 0.2239 - acc: 0.9159 - val_loss: 0.2390 - val_acc: 0.9142\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91420, saving model to best_model_mk11.h5\n",
      "Epoch 2/3\n",
      " - 200s - loss: 0.2223 - acc: 0.9164 - val_loss: 0.2390 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.91420\n",
      "Epoch 00002: early stopping\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      " - 208s - loss: 0.2214 - acc: 0.9166 - val_loss: 0.2403 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91355, saving model to best_model_mk11.h5\n",
      "Epoch 2/3\n",
      " - 204s - loss: 0.2164 - acc: 0.9179 - val_loss: 0.2385 - val_acc: 0.9138\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.91355 to 0.91383, saving model to best_model_mk11.h5\n",
      "Epoch 3/3\n",
      " - 201s - loss: 0.2129 - acc: 0.9189 - val_loss: 0.2438 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.91383\n",
      "Epoch 00003: early stopping\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      " - 202s - loss: 0.2093 - acc: 0.9201 - val_loss: 0.2410 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91352, saving model to best_model_mk11.h5\n",
      "Epoch 2/3\n",
      " - 199s - loss: 0.2073 - acc: 0.9202 - val_loss: 0.2414 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.91352 to 0.91357, saving model to best_model_mk11.h5\n",
      "Epoch 00002: early stopping\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      " - 209s - loss: 0.2041 - acc: 0.9215 - val_loss: 0.2443 - val_acc: 0.9122\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91216, saving model to best_model_mk11.h5\n",
      "Epoch 2/3\n",
      " - 210s - loss: 0.2007 - acc: 0.9222 - val_loss: 0.2473 - val_acc: 0.9120\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.91216\n",
      "Epoch 00002: early stopping\n"
     ]
    }
   ],
   "source": [
    "weights = []\n",
    "for model_idx in range(NUM_MODELS):\n",
    "    model = build_model(embedding_matrix, model_name = str(model_idx))\n",
    "    for global_epoch in range(EPOCHS):\n",
    "        model.fit(\n",
    "            X_train_pad,\n",
    "            y_train,\n",
    "            validation_data=(X_test_pad, y_test),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=3,\n",
    "            verbose=2,\n",
    "            callbacks=[tbCallBack, es, ModelCheckpoint('best_model_mk1' + str(model_idx) + '.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)]\n",
    "        )\n",
    "        weights.append(2 ** global_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Testing Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing texts: Uber\n",
    "\n",
    "Neg:\n",
    "\n",
    "1. #uber's customer service department is worst than comcast.\n",
    "1. #Uber ...get your act right! # frustrated#waiting for competition\n",
    "1. .@Uber charges more if they think you're willing to pay more. Uber thinks you're willing to pay more when your battery is about to die. And Uber knows when your battery is about to die.\n",
    "\n",
    "Pos:\n",
    "1. Ride-hailing industry expected to grow eightfold to $285 billion by 2030\n",
    "1. Uber is awesome!\n",
    "1. Best service ever with Uber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5022</td>\n",
       "      <td>5022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4999</td>\n",
       "      <td>4999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2302</td>\n",
       "      <td>2302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2541</td>\n",
       "      <td>2541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2635</td>\n",
       "      <td>2635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2307</td>\n",
       "      <td>2307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2850</td>\n",
       "      <td>2850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2344</td>\n",
       "      <td>2344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sentence  polarity\n",
       "sentiment                    \n",
       "1              5022      5022\n",
       "10             4999      4999\n",
       "2              2302      2302\n",
       "3              2541      2541\n",
       "4              2635      2635\n",
       "7              2307      2307\n",
       "8              2850      2850\n",
       "9              2344      2344"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.groupby('sentiment').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"I live in Mexico City, so I have to suffer throug the trailers for every piece of trash that comes out from all these stupid Mexican filmmakers. You want to admire a Mexican guy for making great films? Take a look at something by Guillermo del Toro (specially The Devil's Backbone), or maybe Alfonso Cuar√≥n (though I really don't like his films, but I respect them).<br /><br />Mexican filmmakers often produce some of the most terrible utter trash ever (Por la Libre, El Segundo Aire, American Visa), but this is one of the lowest points in Mexican films ever. If you respect your brain, please avoid this piece of **** at all costs. It would be more intelligent to watch some video of a wedding or to watch Britney's reality show. That's got more IQ than everyone in this 'film'.\",\n",
       " '1')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"sentence\"][1], test_df[\"sentiment\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tokenizer_obj.texts_to_sequences(test_df[\"sentence\"][1])\n",
    "test_data = sequence.pad_sequences(test_data, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2574, 781)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data[3]), len(test_df[\"sentence\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "781"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.4865126e-04, 2.0165390e-01, 8.5235134e-02, 6.8558104e-02,\n",
       "       6.4780466e-02, 1.4810260e-04, 1.6418842e-04, 8.5876852e-02,\n",
       "       1.5305586e-01, 1.0984342e-01, 3.5275525e-01], dtype=float32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[780]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(prediction[780].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "imdb = tf.keras.datasets.imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Times Singapore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "date_sentiments = {}\n",
    "\n",
    "for i in range(1,11):\n",
    "    page = urlopen('https://www.businesstimes.com.sg/search/facebook?page='+str(i)).read()\n",
    "    soup = BeautifulSoup(page, features=\"html.parser\")\n",
    "    posts = soup.findAll(\"div\", {\"class\": \"media-body\"})\n",
    "    for post in posts:\n",
    "        time.sleep(1)\n",
    "        url = post.a['href']\n",
    "        date = post.time.text\n",
    "        print(date, url)\n",
    "        try:\n",
    "            link_page = urlopen(url).read()\n",
    "        except:\n",
    "            url = url[:-2]\n",
    "            link_page = urlopen(url).read()\n",
    "        link_soup = BeautifulSoup(link_page)\n",
    "        sentences = link_soup.findAll(\"p\")\n",
    "        passage = \"\"\n",
    "        for sentence in sentences:\n",
    "            passage += sentence.text\n",
    "        sentiment = sia.polarity_scores(passage)['compound']\n",
    "        date_sentiments.setdefault(date, []).append(sentiment)\n",
    "\n",
    "date_sentiment = {}\n",
    "\n",
    "for k,v in date_sentiments.items():\n",
    "    date_sentiment[datetime.strptime(k, '%d %b %Y').date() + timedelta(days=1)] = round(sum(v)/float(len(v)),3)\n",
    "\n",
    "earliest_date = min(date_sentiment.keys())\n",
    "\n",
    "print(date_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtrader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backtrader as bt\n",
    "import backtrader.indicators as btind\n",
    "import datetime\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "class Sentiment(bt.Indicator):\n",
    "    lines = ('sentiment',)\n",
    "    plotinfo = dict(\n",
    "        plotymargin=0.15,\n",
    "        plothlines=[0],\n",
    "        plotyticks=[1.0, 0, -1.0])\n",
    "    \n",
    "    def next(self):\n",
    "        self.date = self.data.datetime\n",
    "        date = bt.num2date(self.date[0]).date()\n",
    "        prev_sentiment = self.sentiment\n",
    "        if date in date_sentiment:\n",
    "            self.sentiment = date_sentiment[date]\n",
    "        self.lines.sentiment[0] = self.sentiment\n",
    "\n",
    "\n",
    "class SentimentStrat(bt.Strategy):\n",
    "    params = (\n",
    "        ('period', 15),\n",
    "        ('printlog', True),\n",
    "    )\n",
    "\n",
    "    def log(self, txt, dt=None, doprint=False):\n",
    "        ''' Logging function for this strategy'''\n",
    "        if self.params.printlog or doprint:\n",
    "            dt = dt or self.datas[0].datetime.date(0)\n",
    "            print('%s, %s' % (dt.isoformat(), txt))\n",
    "\n",
    "    def __init__(self):\n",
    "        # Keep a reference to the \"close\" line in the data[0] dataseries\n",
    "        self.dataclose = self.datas[0].close\n",
    "        # Keep track of pending orders\n",
    "        self.order = None\n",
    "        self.buyprice = None\n",
    "        self.buycomm = None\n",
    "        self.sma = bt.indicators.SimpleMovingAverage(\n",
    "            self.datas[0], period=self.params.period)\n",
    "        self.date = self.data.datetime\n",
    "        self.sentiment = None\n",
    "        Sentiment(self.data)\n",
    "        \n",
    "    def notify_order(self, order):\n",
    "        if order.status in [order.Submitted, order.Accepted]:\n",
    "            # Buy/Sell order submitted/accepted to/by broker - Nothing to do\n",
    "            return\n",
    "        \n",
    "        # Check if an order has been completed\n",
    "        # Attention: broker could reject order if not enough cash\n",
    "        if order.status in [order.Completed]:\n",
    "            if order.isbuy():\n",
    "                self.log(\n",
    "                    'BUY EXECUTED, Price: %.2f, Cost: %.2f, Comm %.2f' %\n",
    "                    (order.executed.price,\n",
    "                     order.executed.value,\n",
    "                     order.executed.comm))\n",
    "                self.buyprice = order.executed.price\n",
    "                self.buycomm = order.executed.comm\n",
    "            else:  # Sell\n",
    "                self.log('SELL EXECUTED, Price: %.2f, Cost: %.2f, Comm %.2f' %\n",
    "                         (order.executed.price,\n",
    "                          order.executed.value,\n",
    "                          order.executed.comm))\n",
    "                \n",
    "            self.bar_executed = len(self)     \n",
    "            \n",
    "        elif order.status in [order.Canceled, order.Margin, order.Rejected]:\n",
    "            self.log('Order Canceled/Margin/Rejected')\n",
    "            \n",
    "        # Write down: no pending order\n",
    "        self.order = None\n",
    "        \n",
    "    def notify_trade(self, trade):\n",
    "        if not trade.isclosed:\n",
    "            return\n",
    "\n",
    "        self.log('OPERATION PROFIT, GROSS %.2f, NET %.2f' %\n",
    "                 (trade.pnl, trade.pnlcomm))\n",
    "    \n",
    "    ### Main Strat ###\n",
    "    def next(self):\n",
    "        # log closing price of the series from the reference\n",
    "        self.log('Close, %.2f' % self.dataclose[0])\n",
    "        \n",
    "        date = bt.num2date(self.date[0]).date()\n",
    "        prev_sentiment = self.sentiment\n",
    "        if date in date_sentiment:\n",
    "            self.sentiment = date_sentiment[date]\n",
    "        \n",
    "        # Check if an order is pending. if yes, we cannot send a 2nd one\n",
    "        if self.order:\n",
    "            return\n",
    "        print(self.sentiment)\n",
    "        # If not in the market and previous sentiment not none\n",
    "        if not self.position and prev_sentiment:\n",
    "            # buy if current close more than sma AND sentiment increased by >= 0.5\n",
    "            if self.dataclose[0] > self.sma[0] and self.sentiment - prev_sentiment >= 0.5:\n",
    "                self.log('BUY CREATE, %.2f' % self.dataclose[0])\n",
    "                self.order = self.buy()\n",
    "                \n",
    "        # Already in the market and previous sentiment not none\n",
    "        elif prev_sentiment:\n",
    "            # sell if current close less than sma AND sentiment decreased by >= 0.5\n",
    "            if self.dataclose[0] < self.sma[0] and self.sentiment - prev_sentiment <= -0.5:\n",
    "                self.log('SELL CREATE, %.2f' % self.dataclose[0])\n",
    "                self.order = self.sell()\n",
    "\n",
    "    def stop(self):\n",
    "        self.log('(MA Period %2d) Ending Value %.2f' %\n",
    "                 (self.params.period, self.broker.getvalue()), doprint=True)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cerebro = bt.Cerebro()\n",
    "    \n",
    "    # Strategy\n",
    "    cerebro.addstrategy(SentimentStrat)\n",
    "\n",
    "    # Data Feed\n",
    "    data = bt.feeds.YahooFinanceData(\n",
    "        dataname = 'FB',\n",
    "        fromdate = earliest_date,\n",
    "        todate = datetime.datetime(2018,11,25),\n",
    "        reverse = False\n",
    "    )\n",
    "    \n",
    "    cerebro.adddata(data)\n",
    "\n",
    "    cerebro.broker.setcash(100000.0)\n",
    "    cerebro.addsizer(bt.sizers.FixedSize, stake=10)\n",
    "    cerebro.broker.setcommission(commission=0.001)\n",
    "    print('Starting Portfolio Value: %.2f' % cerebro.broker.getvalue())\n",
    "    cerebro.run()\n",
    "    print('Final Portfolio Value: %.2f' % cerebro.broker.getvalue())\n",
    "    \n",
    "    cerebro.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "# from keras import backend as K\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import graph_io\n",
    "from tensorflow.python.saved_model import signature_constants\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.set_learning_phase(0)\n",
    "\n",
    "model = build_model(embedding_matrix, model_name = str(model_idx))\n",
    "config = model.get_config()\n",
    "weights = model.get_weights()\n",
    "new_model = Model.from_config(config)\n",
    "new_model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model import utils\n",
    "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
    "from tensorflow.python.saved_model.signature_def_utils_impl import build_signature_def, predict_signature_def\n",
    "from tensorflow.contrib.session_bundle import exporter\n",
    "\n",
    "export_path = 'exported_model'\n",
    "builder = saved_model_builder.SavedModelBuilder(export_path)\n",
    "\n",
    "signature = predict_signature_def(inputs={'text': new_model.input},\n",
    "                                  outputs={'sentiment': new_model.output})\n",
    "\n",
    "with keras.get_session() as sess:\n",
    "    builder.add_meta_graph_and_variables(sess=sess,\n",
    "                                         tags=[tag_constants.SERVING],\n",
    "                                         signature_def_map={'predict': signature})\n",
    "    builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another type of loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-15-c5479d416f9e>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-c5479d416f9e>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.loadtxt('embedding_concat.txt', dtype=int)\n",
    "words = Input(shape=(max_length,))\n",
    "x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "x = SpatialDropout1D(0.3)(x)\n",
    "x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "result = Dense(11, activation='sigmoid')(hidden)\n",
    "    \n",
    "model = Model(inputs=words, outputs=result)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=0, write_graph=True, write_images=True)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "ls_sched = LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_pad, y_train, batch_size=64, epochs=10, \n",
    "          validation_data=(X_test_pad, y_test), callbacks=[tbCallBack, es, mc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
