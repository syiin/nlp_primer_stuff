{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['training.1600000.processed.noemoticon.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data/sentiment140/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('data/sentiment140/training.1600000.processed.noemoticon.csv', encoding = 'ISO-8859-1', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                   5  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data_df.iloc[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_polarity(item):\n",
    "    if item == 4:\n",
    "        item = 1\n",
    "    return item\n",
    "        \n",
    "labels = labels.map(change_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_usernames(item):\n",
    "    if item[0] == '@':\n",
    "        start = item.find(' ')\n",
    "        item = item[start:]\n",
    "    return item\n",
    "        \n",
    "sentences = sentences.map(remove_usernames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "\n",
    "# tokenizer = text.Tokenizer()\n",
    "# tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# saving\n",
    "# with open('tokenizer_sen140.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer_sen140.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(s.split()) for s in (sentences)])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "sentences = tokenizer.texts_to_sequences(sentences)\n",
    "sentences = sequence.pad_sequences(sentences, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data & Construct Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(sentences, labels, \n",
    "                                                    test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280000, 320000, 1280000, 320000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(x_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset shapes: ((128, 64), (128,)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "train_dataset.shuffle(BATCH_SIZE)\n",
    "test_dataset.shuffle(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([128, 64]), TensorShape([128]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_inp_batch, example_targ_batch = next(iter(train_dataset))\n",
    "example_inp_batch.shape, example_targ_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate, Flatten, Embedding\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, Input, LSTM\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM, CuDNNGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0412 22:12:25.416198 139788378425152 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f2282b14cc0>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
      "W0412 22:12:25.418886 139788378425152 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f2282ab0550>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 500\n",
    "LSTM_UNITS = 256\n",
    "\n",
    "class MyLSTM(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length)\n",
    "        self.dropout1 = SpatialDropout1D(0.3)\n",
    "        self.lstm1 = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))\n",
    "        self.gmp = GlobalMaxPooling1D()\n",
    "        self.dense1 = Dense(100, activation='tanh')\n",
    "        self.dropout2 = Dropout(0.2)\n",
    "        self.denseOut = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.gmp(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.denseOut(x)\n",
    "        return x\n",
    "    \n",
    "model = MyLSTM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, clipnorm=1.)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    return tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.expand_dims(real, axis=1), \n",
    "                                                              pred, from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(inp)\n",
    "        loss += loss_function(targ, preds)\n",
    "    \n",
    "    trn_acc_metric.update_state(targ, preds)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def val_step(inp, targ):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(inp)\n",
    "        loss += loss_function(targ, preds)\n",
    "    \n",
    "    val_acc_metric.update_state(targ, preds)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Trn Loss 0.7206 Val Loss 0.7233 Trn_Acc 0.52 Val_Acc 0.50\n",
      "Epoch 1 Batch 100 Trn Loss 0.6931 Val Loss 0.6930 Trn_Acc 0.51 Val_Acc 0.50\n",
      "Epoch 1 Batch 200 Trn Loss 0.6932 Val Loss 0.6930 Trn_Acc 0.50 Val_Acc 0.50\n",
      "Epoch 1 Batch 300 Trn Loss 0.6931 Val Loss 0.6931 Trn_Acc 0.50 Val_Acc 0.50\n",
      "Epoch 1 Batch 400 Trn Loss 0.6931 Val Loss 0.6932 Trn_Acc 0.50 Val_Acc 0.50\n",
      "Epoch 1 Batch 500 Trn Loss 0.6932 Val Loss 0.6931 Trn_Acc 0.50 Val_Acc 0.50\n",
      "Epoch 1 Batch 600 Trn Loss 0.6933 Val Loss 0.6931 Trn_Acc 0.50 Val_Acc 0.50\n",
      "Epoch 1 Batch 700 Trn Loss 0.6932 Val Loss 0.6930 Trn_Acc 0.50 Val_Acc 0.50\n",
      "Epoch 1 Batch 800 Trn Loss 0.6931 Val Loss 0.6929 Trn_Acc 0.50 Val_Acc 0.50\n",
      "Epoch 1 Batch 900 Trn Loss 0.7012 Val Loss 0.6956 Trn_Acc 0.50 Val_Acc 0.50\n",
      "Epoch 1 Batch 1000 Trn Loss 0.6941 Val Loss 0.6794 Trn_Acc 0.51 Val_Acc 0.51\n",
      "Epoch 1 Batch 1100 Trn Loss 0.7544 Val Loss 0.6272 Trn_Acc 0.52 Val_Acc 0.52\n",
      "Epoch 1 Batch 1200 Trn Loss 0.6430 Val Loss 0.6504 Trn_Acc 0.53 Val_Acc 0.53\n",
      "Epoch 1 Batch 1300 Trn Loss 0.6585 Val Loss 0.6727 Trn_Acc 0.54 Val_Acc 0.54\n",
      "Epoch 1 Batch 1400 Trn Loss 0.6589 Val Loss 0.6094 Trn_Acc 0.55 Val_Acc 0.55\n",
      "Epoch 1 Batch 1500 Trn Loss 0.5996 Val Loss 0.6347 Trn_Acc 0.56 Val_Acc 0.56\n",
      "Epoch 1 Batch 1600 Trn Loss 0.6191 Val Loss 0.6210 Trn_Acc 0.57 Val_Acc 0.57\n",
      "Epoch 1 Batch 1700 Trn Loss 0.6187 Val Loss 0.5957 Trn_Acc 0.58 Val_Acc 0.58\n",
      "Epoch 1 Batch 1800 Trn Loss 0.6163 Val Loss 0.6089 Trn_Acc 0.59 Val_Acc 0.59\n",
      "Epoch 1 Batch 1900 Trn Loss 0.6237 Val Loss 0.5931 Trn_Acc 0.60 Val_Acc 0.59\n",
      "Epoch 1 Batch 2000 Trn Loss 0.6149 Val Loss 0.6632 Trn_Acc 0.60 Val_Acc 0.60\n",
      "Epoch 1 Batch 2100 Trn Loss 0.5762 Val Loss 0.6158 Trn_Acc 0.61 Val_Acc 0.61\n",
      "Epoch 1 Batch 2200 Trn Loss 0.6555 Val Loss 0.6063 Trn_Acc 0.61 Val_Acc 0.61\n",
      "Epoch 1 Batch 2300 Trn Loss 0.6063 Val Loss 0.6255 Trn_Acc 0.62 Val_Acc 0.62\n",
      "Epoch 1 Batch 2400 Trn Loss 0.5922 Val Loss 0.5899 Trn_Acc 0.63 Val_Acc 0.62\n",
      "Epoch 1 Trn Loss 0.1635\n",
      "Epoch 1 Val Loss 0.1635\n",
      "Time taken for 1 epoch 346.85281324386597 sec\n",
      "\n",
      "Epoch 2 Batch 0 Trn Loss 0.6194 Val Loss 0.6249 Trn_Acc 0.63 Val_Acc 0.63\n",
      "Epoch 2 Batch 100 Trn Loss 0.5921 Val Loss 0.5914 Trn_Acc 0.64 Val_Acc 0.63\n",
      "Epoch 2 Batch 200 Trn Loss 0.6219 Val Loss 0.5764 Trn_Acc 0.64 Val_Acc 0.64\n",
      "Epoch 2 Batch 300 Trn Loss 0.5931 Val Loss 0.5806 Trn_Acc 0.64 Val_Acc 0.64\n",
      "Epoch 2 Batch 400 Trn Loss 0.5790 Val Loss 0.6362 Trn_Acc 0.65 Val_Acc 0.65\n",
      "Epoch 2 Batch 500 Trn Loss 0.6390 Val Loss 0.6250 Trn_Acc 0.65 Val_Acc 0.65\n",
      "Epoch 2 Batch 600 Trn Loss 0.6292 Val Loss 0.5733 Trn_Acc 0.66 Val_Acc 0.66\n",
      "Epoch 2 Batch 700 Trn Loss 0.6298 Val Loss 0.5876 Trn_Acc 0.66 Val_Acc 0.66\n",
      "Epoch 2 Batch 800 Trn Loss 0.6160 Val Loss 0.5810 Trn_Acc 0.66 Val_Acc 0.66\n",
      "Epoch 2 Batch 900 Trn Loss 0.6345 Val Loss 0.5889 Trn_Acc 0.67 Val_Acc 0.67\n",
      "Epoch 2 Batch 1000 Trn Loss 0.5880 Val Loss 0.6259 Trn_Acc 0.67 Val_Acc 0.67\n",
      "Epoch 2 Batch 1100 Trn Loss 0.5942 Val Loss 0.6037 Trn_Acc 0.67 Val_Acc 0.67\n",
      "Epoch 2 Batch 1200 Trn Loss 0.5817 Val Loss 0.6111 Trn_Acc 0.68 Val_Acc 0.68\n",
      "Epoch 2 Batch 1300 Trn Loss 0.6080 Val Loss 0.6234 Trn_Acc 0.68 Val_Acc 0.68\n",
      "Epoch 2 Batch 1400 Trn Loss 0.5844 Val Loss 0.5866 Trn_Acc 0.68 Val_Acc 0.68\n",
      "Epoch 2 Batch 1500 Trn Loss 0.5830 Val Loss 0.6283 Trn_Acc 0.69 Val_Acc 0.68\n",
      "Epoch 2 Batch 1600 Trn Loss 0.6018 Val Loss 0.5932 Trn_Acc 0.69 Val_Acc 0.69\n",
      "Epoch 2 Batch 1700 Trn Loss 0.5673 Val Loss 0.5779 Trn_Acc 0.69 Val_Acc 0.69\n",
      "Epoch 2 Batch 1800 Trn Loss 0.5867 Val Loss 0.5951 Trn_Acc 0.69 Val_Acc 0.69\n",
      "Epoch 2 Batch 1900 Trn Loss 0.6121 Val Loss 0.5730 Trn_Acc 0.70 Val_Acc 0.69\n",
      "Epoch 2 Batch 2000 Trn Loss 0.5969 Val Loss 0.6478 Trn_Acc 0.70 Val_Acc 0.69\n",
      "Epoch 2 Batch 2100 Trn Loss 0.5490 Val Loss 0.5998 Trn_Acc 0.70 Val_Acc 0.70\n",
      "Epoch 2 Batch 2200 Trn Loss 0.6433 Val Loss 0.5849 Trn_Acc 0.70 Val_Acc 0.70\n",
      "Epoch 2 Batch 2300 Trn Loss 0.5828 Val Loss 0.6195 Trn_Acc 0.70 Val_Acc 0.70\n",
      "Epoch 2 Batch 2400 Trn Loss 0.5739 Val Loss 0.5638 Trn_Acc 0.71 Val_Acc 0.70\n",
      "Epoch 2 Trn Loss 0.1495\n",
      "Epoch 2 Val Loss 0.1501\n",
      "Time taken for 1 epoch 341.9635167121887 sec\n",
      "\n",
      "Epoch 3 Batch 0 Trn Loss 0.5881 Val Loss 0.6128 Trn_Acc 0.71 Val_Acc 0.70\n",
      "Epoch 3 Batch 100 Trn Loss 0.5590 Val Loss 0.5889 Trn_Acc 0.71 Val_Acc 0.70\n",
      "Epoch 3 Batch 200 Trn Loss 0.5939 Val Loss 0.5795 Trn_Acc 0.71 Val_Acc 0.71\n",
      "Epoch 3 Batch 300 Trn Loss 0.5720 Val Loss 0.5865 Trn_Acc 0.71 Val_Acc 0.71\n",
      "Epoch 3 Batch 400 Trn Loss 0.5673 Val Loss 0.6256 Trn_Acc 0.72 Val_Acc 0.71\n",
      "Epoch 3 Batch 500 Trn Loss 0.6240 Val Loss 0.6270 Trn_Acc 0.72 Val_Acc 0.71\n",
      "Epoch 3 Batch 600 Trn Loss 0.6172 Val Loss 0.5680 Trn_Acc 0.72 Val_Acc 0.71\n",
      "Epoch 3 Batch 700 Trn Loss 0.6269 Val Loss 0.5780 Trn_Acc 0.72 Val_Acc 0.71\n",
      "Epoch 3 Batch 800 Trn Loss 0.5982 Val Loss 0.5915 Trn_Acc 0.72 Val_Acc 0.71\n",
      "Epoch 3 Batch 900 Trn Loss 0.6179 Val Loss 0.5959 Trn_Acc 0.72 Val_Acc 0.72\n",
      "Epoch 3 Batch 1000 Trn Loss 0.5578 Val Loss 0.6079 Trn_Acc 0.73 Val_Acc 0.72\n",
      "Epoch 3 Batch 1100 Trn Loss 0.5696 Val Loss 0.6028 Trn_Acc 0.73 Val_Acc 0.72\n",
      "Epoch 3 Batch 1200 Trn Loss 0.5693 Val Loss 0.6103 Trn_Acc 0.73 Val_Acc 0.72\n",
      "Epoch 3 Batch 1300 Trn Loss 0.5873 Val Loss 0.6202 Trn_Acc 0.73 Val_Acc 0.72\n",
      "Epoch 3 Batch 1400 Trn Loss 0.5798 Val Loss 0.5909 Trn_Acc 0.73 Val_Acc 0.72\n",
      "Epoch 3 Batch 1500 Trn Loss 0.5717 Val Loss 0.6239 Trn_Acc 0.73 Val_Acc 0.72\n",
      "Epoch 3 Batch 1600 Trn Loss 0.5849 Val Loss 0.5871 Trn_Acc 0.73 Val_Acc 0.72\n",
      "Epoch 3 Batch 1700 Trn Loss 0.5562 Val Loss 0.5804 Trn_Acc 0.74 Val_Acc 0.72\n",
      "Epoch 3 Batch 1800 Trn Loss 0.5790 Val Loss 0.5944 Trn_Acc 0.74 Val_Acc 0.73\n",
      "Epoch 3 Batch 1900 Trn Loss 0.6035 Val Loss 0.5728 Trn_Acc 0.74 Val_Acc 0.73\n",
      "Epoch 3 Batch 2000 Trn Loss 0.5907 Val Loss 0.6438 Trn_Acc 0.74 Val_Acc 0.73\n",
      "Epoch 3 Batch 2100 Trn Loss 0.5543 Val Loss 0.6035 Trn_Acc 0.74 Val_Acc 0.73\n",
      "Epoch 3 Batch 2200 Trn Loss 0.6234 Val Loss 0.5885 Trn_Acc 0.74 Val_Acc 0.73\n",
      "Epoch 3 Batch 2300 Trn Loss 0.5817 Val Loss 0.6238 Trn_Acc 0.74 Val_Acc 0.73\n",
      "Epoch 3 Batch 2400 Trn Loss 0.5619 Val Loss 0.5651 Trn_Acc 0.74 Val_Acc 0.73\n",
      "Epoch 3 Trn Loss 0.1458\n",
      "Epoch 3 Val Loss 0.1493\n",
      "Time taken for 1 epoch 348.18035101890564 sec\n",
      "\n",
      "Epoch 4 Batch 0 Trn Loss 0.5777 Val Loss 0.6194 Trn_Acc 0.75 Val_Acc 0.73\n",
      "Epoch 4 Batch 100 Trn Loss 0.5551 Val Loss 0.5958 Trn_Acc 0.75 Val_Acc 0.73\n",
      "Epoch 4 Batch 200 Trn Loss 0.5874 Val Loss 0.5805 Trn_Acc 0.75 Val_Acc 0.73\n",
      "Epoch 4 Batch 300 Trn Loss 0.5550 Val Loss 0.5781 Trn_Acc 0.75 Val_Acc 0.73\n",
      "Epoch 4 Batch 400 Trn Loss 0.5595 Val Loss 0.6451 Trn_Acc 0.75 Val_Acc 0.73\n",
      "Epoch 4 Batch 500 Trn Loss 0.6088 Val Loss 0.6215 Trn_Acc 0.75 Val_Acc 0.73\n",
      "Epoch 4 Batch 600 Trn Loss 0.6099 Val Loss 0.5633 Trn_Acc 0.75 Val_Acc 0.74\n",
      "Epoch 4 Batch 700 Trn Loss 0.6152 Val Loss 0.5861 Trn_Acc 0.75 Val_Acc 0.74\n",
      "Epoch 4 Batch 800 Trn Loss 0.5910 Val Loss 0.5917 Trn_Acc 0.75 Val_Acc 0.74\n",
      "Epoch 4 Batch 900 Trn Loss 0.6140 Val Loss 0.6062 Trn_Acc 0.75 Val_Acc 0.74\n",
      "Epoch 4 Batch 1000 Trn Loss 0.5558 Val Loss 0.6119 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 1100 Trn Loss 0.5695 Val Loss 0.5987 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 1200 Trn Loss 0.5670 Val Loss 0.6085 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 1300 Trn Loss 0.5742 Val Loss 0.6247 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 1400 Trn Loss 0.5704 Val Loss 0.5926 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 1500 Trn Loss 0.5640 Val Loss 0.6168 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 1600 Trn Loss 0.5761 Val Loss 0.5755 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 1700 Trn Loss 0.5603 Val Loss 0.5763 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 1800 Trn Loss 0.5745 Val Loss 0.5981 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 1900 Trn Loss 0.5893 Val Loss 0.5775 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 2000 Trn Loss 0.5836 Val Loss 0.6480 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 2100 Trn Loss 0.5391 Val Loss 0.6070 Trn_Acc 0.76 Val_Acc 0.74\n",
      "Epoch 4 Batch 2200 Trn Loss 0.6232 Val Loss 0.5935 Trn_Acc 0.77 Val_Acc 0.74\n",
      "Epoch 4 Batch 2300 Trn Loss 0.5786 Val Loss 0.6185 Trn_Acc 0.77 Val_Acc 0.74\n",
      "Epoch 4 Batch 2400 Trn Loss 0.5477 Val Loss 0.5680 Trn_Acc 0.77 Val_Acc 0.74\n",
      "Epoch 4 Trn Loss 0.1440\n",
      "Epoch 4 Val Loss 0.1494\n",
      "Time taken for 1 epoch 337.47496819496155 sec\n",
      "\n",
      "Epoch 5 Batch 0 Trn Loss 0.5776 Val Loss 0.6177 Trn_Acc 0.77 Val_Acc 0.74\n",
      "Epoch 5 Batch 100 Trn Loss 0.5542 Val Loss 0.5926 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 5 Batch 200 Trn Loss 0.5812 Val Loss 0.5893 Trn_Acc 0.77 Val_Acc 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 300 Trn Loss 0.5461 Val Loss 0.5734 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 5 Batch 400 Trn Loss 0.5630 Val Loss 0.6554 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 5 Batch 500 Trn Loss 0.5978 Val Loss 0.6300 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 5 Batch 600 Trn Loss 0.6023 Val Loss 0.5692 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 5 Batch 700 Trn Loss 0.6105 Val Loss 0.5825 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 5 Batch 800 Trn Loss 0.5938 Val Loss 0.5872 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 5 Batch 900 Trn Loss 0.5970 Val Loss 0.6022 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 5 Batch 1000 Trn Loss 0.5489 Val Loss 0.5973 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 5 Batch 1100 Trn Loss 0.5702 Val Loss 0.5972 Trn_Acc 0.77 Val_Acc 0.75\n",
      "Epoch 5 Batch 1200 Trn Loss 0.5679 Val Loss 0.6113 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 1300 Trn Loss 0.5695 Val Loss 0.6151 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 1400 Trn Loss 0.5719 Val Loss 0.5847 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 1500 Trn Loss 0.5610 Val Loss 0.6163 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 1600 Trn Loss 0.5739 Val Loss 0.5772 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 1700 Trn Loss 0.5591 Val Loss 0.5754 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 1800 Trn Loss 0.5779 Val Loss 0.5962 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 1900 Trn Loss 0.5862 Val Loss 0.5723 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 2000 Trn Loss 0.5774 Val Loss 0.6454 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 2100 Trn Loss 0.5325 Val Loss 0.6088 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 2200 Trn Loss 0.6168 Val Loss 0.5909 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 2300 Trn Loss 0.5731 Val Loss 0.6118 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Batch 2400 Trn Loss 0.5490 Val Loss 0.5733 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 5 Trn Loss 0.1429\n",
      "Epoch 5 Val Loss 0.1494\n",
      "Time taken for 1 epoch 333.61196970939636 sec\n",
      "\n",
      "Epoch 6 Batch 0 Trn Loss 0.5739 Val Loss 0.6100 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 6 Batch 100 Trn Loss 0.5503 Val Loss 0.5906 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 6 Batch 200 Trn Loss 0.5739 Val Loss 0.5814 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 6 Batch 300 Trn Loss 0.5429 Val Loss 0.5747 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 6 Batch 400 Trn Loss 0.5524 Val Loss 0.6507 Trn_Acc 0.78 Val_Acc 0.75\n",
      "Epoch 6 Batch 500 Trn Loss 0.6011 Val Loss 0.6296 Trn_Acc 0.79 Val_Acc 0.75\n",
      "Epoch 6 Batch 600 Trn Loss 0.6008 Val Loss 0.5745 Trn_Acc 0.79 Val_Acc 0.75\n",
      "Epoch 6 Batch 700 Trn Loss 0.6096 Val Loss 0.5759 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 800 Trn Loss 0.5776 Val Loss 0.5967 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 900 Trn Loss 0.5963 Val Loss 0.6004 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 1000 Trn Loss 0.5405 Val Loss 0.6110 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 1100 Trn Loss 0.5655 Val Loss 0.6035 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 1200 Trn Loss 0.5616 Val Loss 0.6107 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 1300 Trn Loss 0.5691 Val Loss 0.6327 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 1400 Trn Loss 0.5719 Val Loss 0.5869 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 1500 Trn Loss 0.5508 Val Loss 0.6240 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 1600 Trn Loss 0.5747 Val Loss 0.5819 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 1700 Trn Loss 0.5566 Val Loss 0.5728 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 1800 Trn Loss 0.5814 Val Loss 0.5914 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 1900 Trn Loss 0.5828 Val Loss 0.5754 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 2000 Trn Loss 0.5805 Val Loss 0.6516 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 2100 Trn Loss 0.5343 Val Loss 0.6092 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 2200 Trn Loss 0.6116 Val Loss 0.5945 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 2300 Trn Loss 0.5664 Val Loss 0.6046 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Batch 2400 Trn Loss 0.5627 Val Loss 0.5811 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 6 Trn Loss 0.1421\n",
      "Epoch 6 Val Loss 0.1495\n",
      "Time taken for 1 epoch 341.4453308582306 sec\n",
      "\n",
      "Epoch 7 Batch 0 Trn Loss 0.5665 Val Loss 0.6125 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 7 Batch 100 Trn Loss 0.5533 Val Loss 0.5961 Trn_Acc 0.79 Val_Acc 0.76\n",
      "Epoch 7 Batch 200 Trn Loss 0.5816 Val Loss 0.5750 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 300 Trn Loss 0.5395 Val Loss 0.5855 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 400 Trn Loss 0.5573 Val Loss 0.6387 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 500 Trn Loss 0.5959 Val Loss 0.6200 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 600 Trn Loss 0.5994 Val Loss 0.5723 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 700 Trn Loss 0.6031 Val Loss 0.5750 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 800 Trn Loss 0.5698 Val Loss 0.5821 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 900 Trn Loss 0.5991 Val Loss 0.6050 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1000 Trn Loss 0.5366 Val Loss 0.6115 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1100 Trn Loss 0.5698 Val Loss 0.6120 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1200 Trn Loss 0.5590 Val Loss 0.6075 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1300 Trn Loss 0.5673 Val Loss 0.6155 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1400 Trn Loss 0.5625 Val Loss 0.5801 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1500 Trn Loss 0.5576 Val Loss 0.6337 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1600 Trn Loss 0.5785 Val Loss 0.5762 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1700 Trn Loss 0.5498 Val Loss 0.5706 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1800 Trn Loss 0.5776 Val Loss 0.5911 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 1900 Trn Loss 0.5837 Val Loss 0.5774 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 2000 Trn Loss 0.5705 Val Loss 0.6490 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 2100 Trn Loss 0.5360 Val Loss 0.6016 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 2200 Trn Loss 0.6113 Val Loss 0.5831 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 2300 Trn Loss 0.5651 Val Loss 0.6013 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Batch 2400 Trn Loss 0.5523 Val Loss 0.5756 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 7 Trn Loss 0.1416\n",
      "Epoch 7 Val Loss 0.1494\n",
      "Time taken for 1 epoch 340.62176537513733 sec\n",
      "\n",
      "Epoch 8 Batch 0 Trn Loss 0.5652 Val Loss 0.6199 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 8 Batch 100 Trn Loss 0.5385 Val Loss 0.5871 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 8 Batch 200 Trn Loss 0.5819 Val Loss 0.5662 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 8 Batch 300 Trn Loss 0.5344 Val Loss 0.5757 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 8 Batch 400 Trn Loss 0.5521 Val Loss 0.6516 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 8 Batch 500 Trn Loss 0.5914 Val Loss 0.6175 Trn_Acc 0.80 Val_Acc 0.76\n",
      "Epoch 8 Batch 600 Trn Loss 0.5881 Val Loss 0.5686 Trn_Acc 0.81 Val_Acc 0.76\n",
      "Epoch 8 Batch 700 Trn Loss 0.5991 Val Loss 0.5741 Trn_Acc 0.81 Val_Acc 0.76\n",
      "Epoch 8 Batch 800 Trn Loss 0.5717 Val Loss 0.5856 Trn_Acc 0.81 Val_Acc 0.76\n",
      "Epoch 8 Batch 900 Trn Loss 0.6036 Val Loss 0.5976 Trn_Acc 0.81 Val_Acc 0.76\n",
      "Epoch 8 Batch 1000 Trn Loss 0.5376 Val Loss 0.6046 Trn_Acc 0.81 Val_Acc 0.76\n",
      "Epoch 8 Batch 1100 Trn Loss 0.5634 Val Loss 0.6155 Trn_Acc 0.81 Val_Acc 0.76\n",
      "Epoch 8 Batch 1200 Trn Loss 0.5589 Val Loss 0.6119 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 1300 Trn Loss 0.5664 Val Loss 0.6068 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 1400 Trn Loss 0.5538 Val Loss 0.5832 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 1500 Trn Loss 0.5499 Val Loss 0.6186 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 1600 Trn Loss 0.5754 Val Loss 0.5808 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 1700 Trn Loss 0.5486 Val Loss 0.5781 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 1800 Trn Loss 0.5729 Val Loss 0.5904 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 1900 Trn Loss 0.5861 Val Loss 0.5793 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 2000 Trn Loss 0.5657 Val Loss 0.6466 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 2100 Trn Loss 0.5370 Val Loss 0.6040 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 2200 Trn Loss 0.6068 Val Loss 0.5880 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 2300 Trn Loss 0.5654 Val Loss 0.6075 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Batch 2400 Trn Loss 0.5455 Val Loss 0.5737 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 8 Trn Loss 0.1411\n",
      "Epoch 8 Val Loss 0.1494\n",
      "Time taken for 1 epoch 346.42240047454834 sec\n",
      "\n",
      "Epoch 9 Batch 0 Trn Loss 0.5662 Val Loss 0.6105 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 100 Trn Loss 0.5422 Val Loss 0.5872 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 200 Trn Loss 0.5681 Val Loss 0.5697 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 300 Trn Loss 0.5369 Val Loss 0.5813 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 400 Trn Loss 0.5538 Val Loss 0.6406 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 500 Trn Loss 0.5830 Val Loss 0.6286 Trn_Acc 0.81 Val_Acc 0.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 600 Trn Loss 0.5875 Val Loss 0.5740 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 700 Trn Loss 0.6048 Val Loss 0.5748 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 800 Trn Loss 0.5764 Val Loss 0.5852 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 900 Trn Loss 0.5974 Val Loss 0.6101 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 1000 Trn Loss 0.5399 Val Loss 0.6164 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 1100 Trn Loss 0.5636 Val Loss 0.6054 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 1200 Trn Loss 0.5591 Val Loss 0.6088 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 1300 Trn Loss 0.5654 Val Loss 0.6206 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 1400 Trn Loss 0.5502 Val Loss 0.5779 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 1500 Trn Loss 0.5521 Val Loss 0.6281 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 1600 Trn Loss 0.5732 Val Loss 0.5759 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 1700 Trn Loss 0.5526 Val Loss 0.5742 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 1800 Trn Loss 0.5722 Val Loss 0.5917 Trn_Acc 0.81 Val_Acc 0.77\n",
      "Epoch 9 Batch 1900 Trn Loss 0.5855 Val Loss 0.5699 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Batch 2000 Trn Loss 0.5647 Val Loss 0.6336 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Batch 2100 Trn Loss 0.5305 Val Loss 0.6000 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Batch 2200 Trn Loss 0.6096 Val Loss 0.5881 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Batch 2300 Trn Loss 0.5697 Val Loss 0.6017 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Batch 2400 Trn Loss 0.5524 Val Loss 0.5732 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 9 Trn Loss 0.1406\n",
      "Epoch 9 Val Loss 0.1494\n",
      "Time taken for 1 epoch 346.3723814487457 sec\n",
      "\n",
      "Epoch 10 Batch 0 Trn Loss 0.5739 Val Loss 0.6129 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 100 Trn Loss 0.5479 Val Loss 0.5884 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 200 Trn Loss 0.5712 Val Loss 0.5620 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 300 Trn Loss 0.5316 Val Loss 0.5768 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 400 Trn Loss 0.5511 Val Loss 0.6438 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 500 Trn Loss 0.5798 Val Loss 0.6201 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 600 Trn Loss 0.5934 Val Loss 0.5741 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 700 Trn Loss 0.5964 Val Loss 0.5673 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 800 Trn Loss 0.5773 Val Loss 0.5862 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 900 Trn Loss 0.5831 Val Loss 0.5988 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1000 Trn Loss 0.5376 Val Loss 0.6127 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1100 Trn Loss 0.5593 Val Loss 0.6237 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1200 Trn Loss 0.5576 Val Loss 0.6007 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1300 Trn Loss 0.5568 Val Loss 0.6114 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1400 Trn Loss 0.5499 Val Loss 0.5777 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1500 Trn Loss 0.5510 Val Loss 0.6266 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1600 Trn Loss 0.5724 Val Loss 0.5771 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1700 Trn Loss 0.5429 Val Loss 0.5738 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1800 Trn Loss 0.5661 Val Loss 0.5870 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 1900 Trn Loss 0.5815 Val Loss 0.5655 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 2000 Trn Loss 0.5554 Val Loss 0.6441 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 2100 Trn Loss 0.5334 Val Loss 0.5944 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 2200 Trn Loss 0.6049 Val Loss 0.5926 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 2300 Trn Loss 0.5582 Val Loss 0.5999 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Batch 2400 Trn Loss 0.5458 Val Loss 0.5693 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 10 Trn Loss 0.1402\n",
      "Epoch 10 Val Loss 0.1495\n",
      "Time taken for 1 epoch 348.35803294181824 sec\n",
      "\n",
      "Epoch 11 Batch 0 Trn Loss 0.5661 Val Loss 0.6186 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 100 Trn Loss 0.5470 Val Loss 0.5972 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 200 Trn Loss 0.5706 Val Loss 0.5571 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 300 Trn Loss 0.5335 Val Loss 0.5810 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 400 Trn Loss 0.5464 Val Loss 0.6306 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 500 Trn Loss 0.5820 Val Loss 0.6188 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 600 Trn Loss 0.5889 Val Loss 0.5707 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 700 Trn Loss 0.5952 Val Loss 0.5765 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 800 Trn Loss 0.5692 Val Loss 0.5848 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 900 Trn Loss 0.5834 Val Loss 0.6039 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 1000 Trn Loss 0.5366 Val Loss 0.6156 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 1100 Trn Loss 0.5536 Val Loss 0.6161 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 1200 Trn Loss 0.5593 Val Loss 0.6107 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 1300 Trn Loss 0.5566 Val Loss 0.6205 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 1400 Trn Loss 0.5608 Val Loss 0.5672 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 1500 Trn Loss 0.5559 Val Loss 0.6200 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 1600 Trn Loss 0.5803 Val Loss 0.5723 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 1700 Trn Loss 0.5480 Val Loss 0.5712 Trn_Acc 0.82 Val_Acc 0.77\n",
      "Epoch 11 Batch 1800 Trn Loss 0.5655 Val Loss 0.5960 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 1900 Trn Loss 0.5919 Val Loss 0.5648 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 2000 Trn Loss 0.5568 Val Loss 0.6541 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 2100 Trn Loss 0.5314 Val Loss 0.6016 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 2200 Trn Loss 0.6055 Val Loss 0.5897 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 2300 Trn Loss 0.5588 Val Loss 0.6026 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Batch 2400 Trn Loss 0.5415 Val Loss 0.5769 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 11 Trn Loss 0.1399\n",
      "Epoch 11 Val Loss 0.1495\n",
      "Time taken for 1 epoch 347.68494153022766 sec\n",
      "\n",
      "Epoch 12 Batch 0 Trn Loss 0.5661 Val Loss 0.6220 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 100 Trn Loss 0.5332 Val Loss 0.5837 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 200 Trn Loss 0.5688 Val Loss 0.5621 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 300 Trn Loss 0.5403 Val Loss 0.5764 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 400 Trn Loss 0.5484 Val Loss 0.6346 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 500 Trn Loss 0.5879 Val Loss 0.6293 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 600 Trn Loss 0.5920 Val Loss 0.5774 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 700 Trn Loss 0.5919 Val Loss 0.5726 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 800 Trn Loss 0.5637 Val Loss 0.6017 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 900 Trn Loss 0.5903 Val Loss 0.6114 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1000 Trn Loss 0.5441 Val Loss 0.6071 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1100 Trn Loss 0.5579 Val Loss 0.6098 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1200 Trn Loss 0.5488 Val Loss 0.5963 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1300 Trn Loss 0.5737 Val Loss 0.6235 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1400 Trn Loss 0.5424 Val Loss 0.5814 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1500 Trn Loss 0.5528 Val Loss 0.6174 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1600 Trn Loss 0.5782 Val Loss 0.5773 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1700 Trn Loss 0.5399 Val Loss 0.5711 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1800 Trn Loss 0.5633 Val Loss 0.5937 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 1900 Trn Loss 0.5825 Val Loss 0.5603 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 2000 Trn Loss 0.5577 Val Loss 0.6436 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 2100 Trn Loss 0.5304 Val Loss 0.6063 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 2200 Trn Loss 0.5936 Val Loss 0.5854 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 2300 Trn Loss 0.5596 Val Loss 0.5922 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Batch 2400 Trn Loss 0.5447 Val Loss 0.5718 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 12 Trn Loss 0.1395\n",
      "Epoch 12 Val Loss 0.1495\n",
      "Time taken for 1 epoch 343.16918110847473 sec\n",
      "\n",
      "Epoch 13 Batch 0 Trn Loss 0.5639 Val Loss 0.6101 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 100 Trn Loss 0.5387 Val Loss 0.5914 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 200 Trn Loss 0.5669 Val Loss 0.5629 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 300 Trn Loss 0.5374 Val Loss 0.5721 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 400 Trn Loss 0.5436 Val Loss 0.6440 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 500 Trn Loss 0.5814 Val Loss 0.6226 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 600 Trn Loss 0.5931 Val Loss 0.5732 Trn_Acc 0.83 Val_Acc 0.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Batch 700 Trn Loss 0.5935 Val Loss 0.5744 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 800 Trn Loss 0.5654 Val Loss 0.6016 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 900 Trn Loss 0.5897 Val Loss 0.6013 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 1000 Trn Loss 0.5353 Val Loss 0.6025 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 1100 Trn Loss 0.5491 Val Loss 0.6169 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 1200 Trn Loss 0.5492 Val Loss 0.5991 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 1300 Trn Loss 0.5616 Val Loss 0.6242 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 1400 Trn Loss 0.5400 Val Loss 0.5847 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 1500 Trn Loss 0.5503 Val Loss 0.6143 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 1600 Trn Loss 0.5750 Val Loss 0.5710 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 1700 Trn Loss 0.5368 Val Loss 0.5676 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 1800 Trn Loss 0.5594 Val Loss 0.5961 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 1900 Trn Loss 0.5793 Val Loss 0.5648 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 2000 Trn Loss 0.5544 Val Loss 0.6560 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 2100 Trn Loss 0.5341 Val Loss 0.6080 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 2200 Trn Loss 0.5884 Val Loss 0.5876 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 2300 Trn Loss 0.5580 Val Loss 0.6039 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Batch 2400 Trn Loss 0.5376 Val Loss 0.5695 Trn_Acc 0.83 Val_Acc 0.77\n",
      "Epoch 13 Trn Loss 0.1393\n",
      "Epoch 13 Val Loss 0.1496\n",
      "Time taken for 1 epoch 341.459424495697 sec\n",
      "\n",
      "Epoch 14 Batch 0 Trn Loss 0.5631 Val Loss 0.6198 Trn_Acc 0.83 Val_Acc 0.78\n",
      "Epoch 14 Batch 100 Trn Loss 0.5330 Val Loss 0.5856 Trn_Acc 0.83 Val_Acc 0.78\n",
      "Epoch 14 Batch 200 Trn Loss 0.5737 Val Loss 0.5576 Trn_Acc 0.83 Val_Acc 0.78\n",
      "Epoch 14 Batch 300 Trn Loss 0.5351 Val Loss 0.5746 Trn_Acc 0.83 Val_Acc 0.78\n",
      "Epoch 14 Batch 400 Trn Loss 0.5447 Val Loss 0.6239 Trn_Acc 0.83 Val_Acc 0.78\n",
      "Epoch 14 Batch 500 Trn Loss 0.5751 Val Loss 0.6205 Trn_Acc 0.83 Val_Acc 0.78\n",
      "Epoch 14 Batch 600 Trn Loss 0.5929 Val Loss 0.5735 Trn_Acc 0.83 Val_Acc 0.78\n",
      "Epoch 14 Batch 700 Trn Loss 0.5898 Val Loss 0.5745 Trn_Acc 0.83 Val_Acc 0.78\n",
      "Epoch 14 Batch 800 Trn Loss 0.5694 Val Loss 0.5982 Trn_Acc 0.83 Val_Acc 0.78\n",
      "Epoch 14 Batch 900 Trn Loss 0.5881 Val Loss 0.6103 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1000 Trn Loss 0.5378 Val Loss 0.6347 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1100 Trn Loss 0.5552 Val Loss 0.6060 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1200 Trn Loss 0.5498 Val Loss 0.6052 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1300 Trn Loss 0.5668 Val Loss 0.6204 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1400 Trn Loss 0.5534 Val Loss 0.5764 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1500 Trn Loss 0.5380 Val Loss 0.6229 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1600 Trn Loss 0.5673 Val Loss 0.5739 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1700 Trn Loss 0.5360 Val Loss 0.5749 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1800 Trn Loss 0.5545 Val Loss 0.5904 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 1900 Trn Loss 0.5825 Val Loss 0.5674 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 2000 Trn Loss 0.5560 Val Loss 0.6499 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 2100 Trn Loss 0.5362 Val Loss 0.5960 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 2200 Trn Loss 0.5926 Val Loss 0.5913 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 2300 Trn Loss 0.5537 Val Loss 0.5972 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Batch 2400 Trn Loss 0.5353 Val Loss 0.5702 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 14 Trn Loss 0.1390\n",
      "Epoch 14 Val Loss 0.1495\n",
      "Time taken for 1 epoch 339.1433651447296 sec\n",
      "\n",
      "Epoch 15 Batch 0 Trn Loss 0.5656 Val Loss 0.6159 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 100 Trn Loss 0.5311 Val Loss 0.5840 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 200 Trn Loss 0.5687 Val Loss 0.5667 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 300 Trn Loss 0.5333 Val Loss 0.5832 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 400 Trn Loss 0.5429 Val Loss 0.6356 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 500 Trn Loss 0.5705 Val Loss 0.6258 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 600 Trn Loss 0.5936 Val Loss 0.5719 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 700 Trn Loss 0.5786 Val Loss 0.5644 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 800 Trn Loss 0.5618 Val Loss 0.6039 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 900 Trn Loss 0.5940 Val Loss 0.6113 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1000 Trn Loss 0.5404 Val Loss 0.6261 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1100 Trn Loss 0.5387 Val Loss 0.6123 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1200 Trn Loss 0.5519 Val Loss 0.6059 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1300 Trn Loss 0.5592 Val Loss 0.6315 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1400 Trn Loss 0.5418 Val Loss 0.5851 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1500 Trn Loss 0.5450 Val Loss 0.6231 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1600 Trn Loss 0.5666 Val Loss 0.5848 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1700 Trn Loss 0.5320 Val Loss 0.5746 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1800 Trn Loss 0.5555 Val Loss 0.5912 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 1900 Trn Loss 0.5773 Val Loss 0.5702 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 2000 Trn Loss 0.5544 Val Loss 0.6416 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 2100 Trn Loss 0.5365 Val Loss 0.5977 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 2200 Trn Loss 0.6012 Val Loss 0.5922 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 2300 Trn Loss 0.5501 Val Loss 0.5984 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Batch 2400 Trn Loss 0.5375 Val Loss 0.5684 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 15 Trn Loss 0.1388\n",
      "Epoch 15 Val Loss 0.1496\n",
      "Time taken for 1 epoch 336.15191292762756 sec\n",
      "\n",
      "Epoch 16 Batch 0 Trn Loss 0.5589 Val Loss 0.6145 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 100 Trn Loss 0.5234 Val Loss 0.5842 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 200 Trn Loss 0.5701 Val Loss 0.5702 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 300 Trn Loss 0.5365 Val Loss 0.5699 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 400 Trn Loss 0.5475 Val Loss 0.6369 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 500 Trn Loss 0.5744 Val Loss 0.6110 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 600 Trn Loss 0.5938 Val Loss 0.5646 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 700 Trn Loss 0.5728 Val Loss 0.5711 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 800 Trn Loss 0.5623 Val Loss 0.5953 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 900 Trn Loss 0.5862 Val Loss 0.6071 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1000 Trn Loss 0.5366 Val Loss 0.6246 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1100 Trn Loss 0.5536 Val Loss 0.6075 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1200 Trn Loss 0.5485 Val Loss 0.6107 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1300 Trn Loss 0.5642 Val Loss 0.6153 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1400 Trn Loss 0.5391 Val Loss 0.5836 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1500 Trn Loss 0.5446 Val Loss 0.6188 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1600 Trn Loss 0.5601 Val Loss 0.5762 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1700 Trn Loss 0.5260 Val Loss 0.5680 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1800 Trn Loss 0.5550 Val Loss 0.5881 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 1900 Trn Loss 0.5866 Val Loss 0.5637 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 2000 Trn Loss 0.5599 Val Loss 0.6426 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 2100 Trn Loss 0.5353 Val Loss 0.6015 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 2200 Trn Loss 0.5925 Val Loss 0.5834 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 2300 Trn Loss 0.5608 Val Loss 0.5980 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Batch 2400 Trn Loss 0.5427 Val Loss 0.5697 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 16 Trn Loss 0.1386\n",
      "Epoch 16 Val Loss 0.1496\n",
      "Time taken for 1 epoch 341.8705554008484 sec\n",
      "\n",
      "Epoch 17 Batch 0 Trn Loss 0.5553 Val Loss 0.6154 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 100 Trn Loss 0.5302 Val Loss 0.5799 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 200 Trn Loss 0.5645 Val Loss 0.5649 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 300 Trn Loss 0.5355 Val Loss 0.5869 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 400 Trn Loss 0.5434 Val Loss 0.6325 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 500 Trn Loss 0.5707 Val Loss 0.6206 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 600 Trn Loss 0.5909 Val Loss 0.5655 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 700 Trn Loss 0.5782 Val Loss 0.5681 Trn_Acc 0.84 Val_Acc 0.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Batch 800 Trn Loss 0.5598 Val Loss 0.5969 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 900 Trn Loss 0.5852 Val Loss 0.5955 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 1000 Trn Loss 0.5404 Val Loss 0.6199 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 1100 Trn Loss 0.5422 Val Loss 0.6179 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 1200 Trn Loss 0.5507 Val Loss 0.6103 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 1300 Trn Loss 0.5693 Val Loss 0.6226 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 1400 Trn Loss 0.5448 Val Loss 0.5935 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 1500 Trn Loss 0.5373 Val Loss 0.6140 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 1600 Trn Loss 0.5661 Val Loss 0.5833 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 1700 Trn Loss 0.5301 Val Loss 0.5803 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 1800 Trn Loss 0.5562 Val Loss 0.5811 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 1900 Trn Loss 0.5771 Val Loss 0.5690 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 2000 Trn Loss 0.5511 Val Loss 0.6427 Trn_Acc 0.84 Val_Acc 0.78\n",
      "Epoch 17 Batch 2100 Trn Loss 0.5319 Val Loss 0.5905 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 17 Batch 2200 Trn Loss 0.5977 Val Loss 0.5887 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 17 Batch 2300 Trn Loss 0.5618 Val Loss 0.5960 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 17 Batch 2400 Trn Loss 0.5492 Val Loss 0.5661 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 17 Trn Loss 0.1383\n",
      "Epoch 17 Val Loss 0.1496\n",
      "Time taken for 1 epoch 335.38857769966125 sec\n",
      "\n",
      "Epoch 18 Batch 0 Trn Loss 0.5535 Val Loss 0.6167 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 100 Trn Loss 0.5401 Val Loss 0.5871 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 200 Trn Loss 0.5683 Val Loss 0.5619 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 300 Trn Loss 0.5352 Val Loss 0.5872 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 400 Trn Loss 0.5485 Val Loss 0.6261 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 500 Trn Loss 0.5759 Val Loss 0.6137 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 600 Trn Loss 0.5972 Val Loss 0.5591 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 700 Trn Loss 0.5770 Val Loss 0.5568 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 800 Trn Loss 0.5602 Val Loss 0.5884 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 900 Trn Loss 0.5834 Val Loss 0.6010 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1000 Trn Loss 0.5350 Val Loss 0.6199 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1100 Trn Loss 0.5473 Val Loss 0.6029 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1200 Trn Loss 0.5487 Val Loss 0.6031 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1300 Trn Loss 0.5655 Val Loss 0.6275 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1400 Trn Loss 0.5399 Val Loss 0.5824 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1500 Trn Loss 0.5358 Val Loss 0.6141 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1600 Trn Loss 0.5633 Val Loss 0.5872 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1700 Trn Loss 0.5344 Val Loss 0.5908 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1800 Trn Loss 0.5521 Val Loss 0.5887 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 1900 Trn Loss 0.5801 Val Loss 0.5631 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 2000 Trn Loss 0.5585 Val Loss 0.6427 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 2100 Trn Loss 0.5331 Val Loss 0.6056 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 2200 Trn Loss 0.5910 Val Loss 0.5906 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 2300 Trn Loss 0.5565 Val Loss 0.5995 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Batch 2400 Trn Loss 0.5400 Val Loss 0.5641 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 18 Trn Loss 0.1381\n",
      "Epoch 18 Val Loss 0.1497\n",
      "Time taken for 1 epoch 353.9774582386017 sec\n",
      "\n",
      "Epoch 19 Batch 0 Trn Loss 0.5571 Val Loss 0.6179 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 100 Trn Loss 0.5312 Val Loss 0.5950 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 200 Trn Loss 0.5690 Val Loss 0.5680 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 300 Trn Loss 0.5329 Val Loss 0.5814 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 400 Trn Loss 0.5451 Val Loss 0.6307 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 500 Trn Loss 0.5713 Val Loss 0.6153 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 600 Trn Loss 0.5920 Val Loss 0.5626 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 700 Trn Loss 0.5762 Val Loss 0.5644 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 800 Trn Loss 0.5638 Val Loss 0.5869 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 900 Trn Loss 0.5841 Val Loss 0.6073 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1000 Trn Loss 0.5398 Val Loss 0.6240 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1100 Trn Loss 0.5429 Val Loss 0.6003 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1200 Trn Loss 0.5485 Val Loss 0.6099 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1300 Trn Loss 0.5613 Val Loss 0.6323 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1400 Trn Loss 0.5494 Val Loss 0.5889 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1500 Trn Loss 0.5354 Val Loss 0.6143 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1600 Trn Loss 0.5621 Val Loss 0.5859 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1700 Trn Loss 0.5303 Val Loss 0.5720 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1800 Trn Loss 0.5529 Val Loss 0.5834 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 1900 Trn Loss 0.5782 Val Loss 0.5722 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 2000 Trn Loss 0.5474 Val Loss 0.6388 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 2100 Trn Loss 0.5331 Val Loss 0.6010 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 2200 Trn Loss 0.5901 Val Loss 0.5846 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 2300 Trn Loss 0.5553 Val Loss 0.6044 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Batch 2400 Trn Loss 0.5308 Val Loss 0.5589 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 19 Trn Loss 0.1379\n",
      "Epoch 19 Val Loss 0.1496\n",
      "Time taken for 1 epoch 375.4183473587036 sec\n",
      "\n",
      "Epoch 20 Batch 0 Trn Loss 0.5539 Val Loss 0.6107 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 100 Trn Loss 0.5275 Val Loss 0.5936 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 200 Trn Loss 0.5563 Val Loss 0.5674 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 300 Trn Loss 0.5336 Val Loss 0.5811 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 400 Trn Loss 0.5435 Val Loss 0.6291 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 500 Trn Loss 0.5713 Val Loss 0.6213 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 600 Trn Loss 0.5959 Val Loss 0.5555 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 700 Trn Loss 0.5796 Val Loss 0.5598 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 800 Trn Loss 0.5626 Val Loss 0.5842 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 900 Trn Loss 0.5830 Val Loss 0.6099 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1000 Trn Loss 0.5289 Val Loss 0.6173 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1100 Trn Loss 0.5445 Val Loss 0.6059 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1200 Trn Loss 0.5512 Val Loss 0.6147 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1300 Trn Loss 0.5585 Val Loss 0.6390 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1400 Trn Loss 0.5414 Val Loss 0.5917 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1500 Trn Loss 0.5277 Val Loss 0.6177 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1600 Trn Loss 0.5685 Val Loss 0.5843 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1700 Trn Loss 0.5321 Val Loss 0.5802 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1800 Trn Loss 0.5516 Val Loss 0.5910 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 1900 Trn Loss 0.5784 Val Loss 0.5734 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 2000 Trn Loss 0.5503 Val Loss 0.6380 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 2100 Trn Loss 0.5327 Val Loss 0.5914 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 2200 Trn Loss 0.5891 Val Loss 0.5846 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 2300 Trn Loss 0.5547 Val Loss 0.6068 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Batch 2400 Trn Loss 0.5231 Val Loss 0.5620 Trn_Acc 0.85 Val_Acc 0.78\n",
      "Epoch 20 Trn Loss 0.1377\n",
      "Epoch 20 Val Loss 0.1497\n",
      "Time taken for 1 epoch 379.53370547294617 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "trn_acc_metric = tf.keras.metrics.BinaryAccuracy(threshold=0.5)\n",
    "val_acc_metric = tf.keras.metrics.BinaryAccuracy(threshold=0.5)\n",
    "\n",
    "steps_per_epoch = len(x_train)//BATCH_SIZE\n",
    "\n",
    "EPOCHS = 20\n",
    "trn_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    val_total_loss = 0\n",
    "    \n",
    "    hidden = model.reset_states()\n",
    "    for batch, ((trn_inp, trn_targ), (val_inp, val_targ)) in enumerate(zip(train_dataset.take(steps_per_epoch), test_dataset.take(steps_per_epoch))):\n",
    "        batch_loss = train_step(trn_inp, trn_targ)\n",
    "        total_loss += batch_loss\n",
    "        trn_loss_list.append(batch_loss)\n",
    "\n",
    "        val_loss = val_step(val_inp, val_targ)\n",
    "        val_total_loss += val_loss\n",
    "        val_loss_list.append(val_loss)\n",
    "\n",
    "        if batch%100==0:\n",
    "              print('Epoch {} Batch {} Trn Loss {:.4f} Val Loss {:.4f} Trn_Acc {:.2f} Val_Acc {:.2f}'.format(epoch + 1,\n",
    "                                                                                                             batch,\n",
    "                                                                                                             batch_loss.numpy(),\n",
    "                                                                                                             val_loss.numpy(),\n",
    "                                                                                                             trn_acc_metric.result().numpy(),\n",
    "                                                                                                             val_acc_metric.result().numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Trn Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Epoch {} Val Loss {:.4f}'.format(epoch + 1,\n",
    "                                      val_total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(trn_loss_list)\n",
    "plt.plot(val_loss_list)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('pre-deploy_models/sen140Eager.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "\n",
    "\n",
    "with open('tokenizer_sen140.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = 64\n",
    "    \n",
    "test_sentence = 'Sapura Energy bags 5 new contracts worth RM1.3 billion - Free Malaysia Today'\n",
    "test_sentence = tokenizer.texts_to_sequences(test_sentence)\n",
    "test_sentence = sequence.pad_sequences(test_sentence, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0413 00:08:46.850948 139788378425152 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f21f04ba128>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
      "W0413 00:08:46.852722 139788378425152 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f21f04ba400>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate, Flatten, Embedding\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, Input, LSTM\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM, CuDNNGRU\n",
    "\n",
    "EMBEDDING_DIM = 500\n",
    "LSTM_UNITS = 256\n",
    "\n",
    "class MyLSTM(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length)\n",
    "        self.dropout1 = SpatialDropout1D(0.3)\n",
    "        self.lstm1 = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))\n",
    "        self.gmp = GlobalMaxPooling1D()\n",
    "        self.dense1 = Dense(100, activation='tanh')\n",
    "        self.dropout2 = Dropout(0.2)\n",
    "        self.denseOut = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.gmp(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.denseOut(x)\n",
    "        return x\n",
    "    \n",
    "model = MyLSTM()\n",
    "\n",
    "model.load_weights('pre-deploy_models/sen140Eager.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = ['Sapura Energy bags 5 new contracts worth RM1.3 billion - Free Malaysia Today', \n",
    "                 'BREAKING: Trump just signed two executive orders that seek to expedite permits for pipelines and other fossil fuel projects by restricting public input and states authority.',\n",
    "                'Number of companies producing oil and gas in Western Canada drops 17.5% since 2014']\n",
    "\n",
    "# test_sentence = ['Wheelchair customers stuck out in the pouring rain when the cab was booked in advance is OUTRAGEOUS. This cab is now 55 minutes late and my client has missed a VERY important DRs appt. BRING #Uber TO VANCOUVER if anything just to SCREW with @vancouvertaxi customers. @NEWS1130',\n",
    "#                 'Uber worst service... Cab booked to take 1.5 year child to doctor and cab did not come more then 1hr driver not answering and not cancelling the ride...',\n",
    "#                 'That #Uber ad sums up that company!! I’ve no sympathy for anyone male or female who uses em and becomes unstuck! They’re not even fucking cheap that’s a myth! Use your local mini cab firm n black cabs in London! Let’s keep money in England ay n drive this firm out!']\n",
    "\n",
    "# example_inp_batch, example_targ_batch = next(iter(test_dataset))\n",
    "# example_inp_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = tokenizer.texts_to_sequences(test_sentence)\n",
    "test_sentence = sequence.pad_sequences(test_sentence, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50469756],\n",
       "       [0.5047392 ],\n",
       "       [0.5026513 ]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
